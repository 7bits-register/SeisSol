// Copyright (c) 2015, Intel Corporation
// 
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are met:
// 
//     * Redistributions of source code must retain the above copyright notice,
//       this list of conditions and the following disclaimer.
//     * Redistributions in binary form must reproduce the above copyright
//       notice, this list of conditions and the following disclaimer in the
//       documentation and/or other materials provided with the distribution.
//     * Neither the name of the copyright holder nor the names of its contributors
//       may be used to endorse or promote products derived from this software
//       without specific prior written permission.
// 
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
// 
// @file
// This file is part of SeisSol.
// 
// @author Alexander Breuer (breuer AT mytum.de, http://www5.in.tum.de/wiki/index.php/Dipl.-Math._Alexander_Breuer)
// @author Alexander Heinecke (alexander.heinecke AT mytum.de, http://www5.in.tum.de/wiki/index.php/Alexander_Heinecke,_M.Sc.,_M.Sc._with_honors)
// 
// @date 2015-11-22 19:13:49.696361
// 
// @section LICENSE
// Copyright (c) 2012-2015, SeisSol Group
// All rights reserved.
// 
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are met:
// 
// 1. Redistributions of source code must retain the above copyright notice,
//    this list of conditions and the following disclaimer.
// 
// 2. Redistributions in binary form must reproduce the above copyright notice,
//    this list of conditions and the following disclaimer in the documentation
//    and/or other materials provided with the distribution.
// 
// 3. Neither the name of the copyright holder nor the names of its
//    contributors may be used to endorse or promote products derived from this
//    software without specific prior written permission.
// 
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
// POSSIBILITY OF SUCH DAMAGE.
// 
// @section DESCRIPTION
// Remark: This file was generated.
#ifndef SPARSESWSMCPP
#define SPARSESWSMCPP

#if defined( __SSE3__) || defined(__MIC__)
#include <immintrin.h>
#endif

#include <cstddef>
#ifndef NDEBUG
extern long long libxsmm_num_total_flops;
#endif

void ssparse_kXiDivMT_m1_n9_k4_ldAna2_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_kEtaDivMT_m1_n9_k4_ldAna2_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*4)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*4)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a2_0 = _mm_load_ss(&A[1]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*4)+0], c2_0);
#else
    C[(l_n*4)+0] += A[1] * B[(l_n*4)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 36;
#endif
}

void ssparse_kZetaDivMT_m1_n9_k4_ldAna2_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*4)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*4)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a2_0 = _mm_load_ss(&A[1]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*4)+0], c2_0);
#else
    C[(l_n*4)+0] += A[1] * B[(l_n*4)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*4)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*4)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a3_0 = _mm_load_ss(&A[2]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*4)+0], c3_0);
#else
    C[(l_n*4)+0] += A[2] * B[(l_n*4)+3];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 54;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna2_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna3_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna3_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_kXiDivMT_m10_n9_k20_ldAna4_ldB20_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*20)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+0], c1_0);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*20)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*20)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+1], c4_0);
#else
    C[(l_n*12)+1] += A[1] * B[(l_n*20)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*20)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*12)+0] += A[2] * B[(l_n*20)+5];
    C[(l_n*12)+2] += A[3] * B[(l_n*20)+5];
    C[(l_n*12)+3] += A[4] * B[(l_n*20)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*20)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+3], c7_1);
#else
    C[(l_n*12)+0] += A[5] * B[(l_n*20)+7];
    C[(l_n*12)+3] += A[6] * B[(l_n*20)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*20)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*12)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*12)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*12)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*20)+10];
    C[(l_n*12)+2] += A[8] * B[(l_n*20)+10];
    C[(l_n*12)+3] += A[9] * B[(l_n*20)+10];
    C[(l_n*12)+4] += A[10] * B[(l_n*20)+10];
    C[(l_n*12)+6] += A[11] * B[(l_n*20)+10];
    C[(l_n*12)+8] += A[12] * B[(l_n*20)+10];
    C[(l_n*12)+9] += A[13] * B[(l_n*20)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*20)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*12)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*12)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*12)+7], c11_2);
#else
    C[(l_n*12)+1] += A[14] * B[(l_n*20)+11];
    C[(l_n*12)+5] += A[15] * B[(l_n*20)+11];
    C[(l_n*12)+7] += A[16] * B[(l_n*20)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*20)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*12)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*12)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*20)+12];
    C[(l_n*12)+2] += A[18] * B[(l_n*20)+12];
    C[(l_n*12)+3] += A[19] * B[(l_n*20)+12];
    C[(l_n*12)+6] += A[20] * B[(l_n*20)+12];
    C[(l_n*12)+8] += A[21] * B[(l_n*20)+12];
    C[(l_n*12)+9] += A[22] * B[(l_n*20)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*20)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*12)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*12)+7], c14_1);
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*20)+14];
    C[(l_n*12)+7] += A[24] * B[(l_n*20)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*20)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*12)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*12)+0] += A[25] * B[(l_n*20)+15];
    C[(l_n*12)+2] += A[26] * B[(l_n*20)+15];
    C[(l_n*12)+3] += A[27] * B[(l_n*20)+15];
    C[(l_n*12)+8] += A[28] * B[(l_n*20)+15];
    C[(l_n*12)+9] += A[29] * B[(l_n*20)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*20)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*12)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*12)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*12)+9], c17_2);
#else
    C[(l_n*12)+0] += A[30] * B[(l_n*20)+17];
    C[(l_n*12)+3] += A[31] * B[(l_n*20)+17];
    C[(l_n*12)+9] += A[32] * B[(l_n*20)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 594;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna4_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_kXiDivMT_m4_n9_k10_ldAna4_ldB12_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 4; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*4)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*4)+1], c4_0);
#else
    C[(l_n*4)+1] += A[1] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*4)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*4)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*4)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*4)+0] += A[2] * B[(l_n*12)+5];
    C[(l_n*4)+2] += A[3] * B[(l_n*12)+5];
    C[(l_n*4)+3] += A[4] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*4)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*4)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*4)+3], c7_1);
#else
    C[(l_n*4)+0] += A[5] * B[(l_n*12)+7];
    C[(l_n*4)+3] += A[6] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 126;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna4_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_kXiDivMT_m1_n9_k4_ldAna4_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna4_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_kXiDivMT_m20_n9_k35_ldAna5_ldB36_ldC20_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 20; l_m++) {
      C[(l_n*20)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*20)+0], c1_0);
#else
    C[(l_n*20)+0] += A[0] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*20)+1], c4_0);
#else
    C[(l_n*20)+1] += A[1] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*20)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*20)+0] += A[2] * B[(l_n*36)+5];
    C[(l_n*20)+2] += A[3] * B[(l_n*36)+5];
    C[(l_n*20)+3] += A[4] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*20)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*20)+3], c7_1);
#else
    C[(l_n*20)+0] += A[5] * B[(l_n*36)+7];
    C[(l_n*20)+3] += A[6] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*20)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*20)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*20)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*20)+0] += A[7] * B[(l_n*36)+10];
    C[(l_n*20)+2] += A[8] * B[(l_n*36)+10];
    C[(l_n*20)+3] += A[9] * B[(l_n*36)+10];
    C[(l_n*20)+4] += A[10] * B[(l_n*36)+10];
    C[(l_n*20)+6] += A[11] * B[(l_n*36)+10];
    C[(l_n*20)+8] += A[12] * B[(l_n*36)+10];
    C[(l_n*20)+9] += A[13] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*20)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*20)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*20)+7], c11_2);
#else
    C[(l_n*20)+1] += A[14] * B[(l_n*36)+11];
    C[(l_n*20)+5] += A[15] * B[(l_n*36)+11];
    C[(l_n*20)+7] += A[16] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*20)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*20)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*20)+0] += A[17] * B[(l_n*36)+12];
    C[(l_n*20)+2] += A[18] * B[(l_n*36)+12];
    C[(l_n*20)+3] += A[19] * B[(l_n*36)+12];
    C[(l_n*20)+6] += A[20] * B[(l_n*36)+12];
    C[(l_n*20)+8] += A[21] * B[(l_n*36)+12];
    C[(l_n*20)+9] += A[22] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*20)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*20)+7], c14_1);
#else
    C[(l_n*20)+1] += A[23] * B[(l_n*36)+14];
    C[(l_n*20)+7] += A[24] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*20)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*20)+0] += A[25] * B[(l_n*36)+15];
    C[(l_n*20)+2] += A[26] * B[(l_n*36)+15];
    C[(l_n*20)+3] += A[27] * B[(l_n*36)+15];
    C[(l_n*20)+8] += A[28] * B[(l_n*36)+15];
    C[(l_n*20)+9] += A[29] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*20)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*20)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*20)+9], c17_2);
#else
    C[(l_n*20)+0] += A[30] * B[(l_n*36)+17];
    C[(l_n*20)+3] += A[31] * B[(l_n*36)+17];
    C[(l_n*20)+9] += A[32] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*20)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*20)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*20)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*20)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*20)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*20)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*20)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*20)+17], c20_6);
#else
    C[(l_n*20)+1] += A[33] * B[(l_n*36)+20];
    C[(l_n*20)+5] += A[34] * B[(l_n*36)+20];
    C[(l_n*20)+7] += A[35] * B[(l_n*36)+20];
    C[(l_n*20)+10] += A[36] * B[(l_n*36)+20];
    C[(l_n*20)+12] += A[37] * B[(l_n*36)+20];
    C[(l_n*20)+15] += A[38] * B[(l_n*36)+20];
    C[(l_n*20)+17] += A[39] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*20)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*20)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*20)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*20)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*20)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*20)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*20)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*20)+0] += A[40] * B[(l_n*36)+21];
    C[(l_n*20)+2] += A[41] * B[(l_n*36)+21];
    C[(l_n*20)+3] += A[42] * B[(l_n*36)+21];
    C[(l_n*20)+4] += A[43] * B[(l_n*36)+21];
    C[(l_n*20)+6] += A[44] * B[(l_n*36)+21];
    C[(l_n*20)+8] += A[45] * B[(l_n*36)+21];
    C[(l_n*20)+9] += A[46] * B[(l_n*36)+21];
    C[(l_n*20)+11] += A[47] * B[(l_n*36)+21];
    C[(l_n*20)+13] += A[48] * B[(l_n*36)+21];
    C[(l_n*20)+14] += A[49] * B[(l_n*36)+21];
    C[(l_n*20)+16] += A[50] * B[(l_n*36)+21];
    C[(l_n*20)+18] += A[51] * B[(l_n*36)+21];
    C[(l_n*20)+19] += A[52] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*20)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*20)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*20)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*20)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*20)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*20)+17], c22_5);
#else
    C[(l_n*20)+1] += A[53] * B[(l_n*36)+22];
    C[(l_n*20)+5] += A[54] * B[(l_n*36)+22];
    C[(l_n*20)+7] += A[55] * B[(l_n*36)+22];
    C[(l_n*20)+12] += A[56] * B[(l_n*36)+22];
    C[(l_n*20)+15] += A[57] * B[(l_n*36)+22];
    C[(l_n*20)+17] += A[58] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*20)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*20)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*20)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*20)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*20)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*20)+0] += A[59] * B[(l_n*36)+23];
    C[(l_n*20)+2] += A[60] * B[(l_n*36)+23];
    C[(l_n*20)+3] += A[61] * B[(l_n*36)+23];
    C[(l_n*20)+6] += A[62] * B[(l_n*36)+23];
    C[(l_n*20)+8] += A[63] * B[(l_n*36)+23];
    C[(l_n*20)+9] += A[64] * B[(l_n*36)+23];
    C[(l_n*20)+13] += A[65] * B[(l_n*36)+23];
    C[(l_n*20)+16] += A[66] * B[(l_n*36)+23];
    C[(l_n*20)+18] += A[67] * B[(l_n*36)+23];
    C[(l_n*20)+19] += A[68] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*20)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*20)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*20)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*20)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*20)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*20)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*20)+0] += A[69] * B[(l_n*36)+25];
    C[(l_n*20)+2] += A[70] * B[(l_n*36)+25];
    C[(l_n*20)+3] += A[71] * B[(l_n*36)+25];
    C[(l_n*20)+4] += A[72] * B[(l_n*36)+25];
    C[(l_n*20)+6] += A[73] * B[(l_n*36)+25];
    C[(l_n*20)+8] += A[74] * B[(l_n*36)+25];
    C[(l_n*20)+9] += A[75] * B[(l_n*36)+25];
    C[(l_n*20)+14] += A[76] * B[(l_n*36)+25];
    C[(l_n*20)+16] += A[77] * B[(l_n*36)+25];
    C[(l_n*20)+18] += A[78] * B[(l_n*36)+25];
    C[(l_n*20)+19] += A[79] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*20)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*20)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*20)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*20)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*20)+17], c26_4);
#else
    C[(l_n*20)+1] += A[80] * B[(l_n*36)+26];
    C[(l_n*20)+5] += A[81] * B[(l_n*36)+26];
    C[(l_n*20)+7] += A[82] * B[(l_n*36)+26];
    C[(l_n*20)+15] += A[83] * B[(l_n*36)+26];
    C[(l_n*20)+17] += A[84] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*20)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*20)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*20)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*20)+0] += A[85] * B[(l_n*36)+27];
    C[(l_n*20)+2] += A[86] * B[(l_n*36)+27];
    C[(l_n*20)+3] += A[87] * B[(l_n*36)+27];
    C[(l_n*20)+6] += A[88] * B[(l_n*36)+27];
    C[(l_n*20)+8] += A[89] * B[(l_n*36)+27];
    C[(l_n*20)+9] += A[90] * B[(l_n*36)+27];
    C[(l_n*20)+16] += A[91] * B[(l_n*36)+27];
    C[(l_n*20)+18] += A[92] * B[(l_n*36)+27];
    C[(l_n*20)+19] += A[93] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*20)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*20)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*20)+17], c29_2);
#else
    C[(l_n*20)+1] += A[94] * B[(l_n*36)+29];
    C[(l_n*20)+7] += A[95] * B[(l_n*36)+29];
    C[(l_n*20)+17] += A[96] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*20)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*20)+0] += A[97] * B[(l_n*36)+30];
    C[(l_n*20)+2] += A[98] * B[(l_n*36)+30];
    C[(l_n*20)+3] += A[99] * B[(l_n*36)+30];
    C[(l_n*20)+8] += A[100] * B[(l_n*36)+30];
    C[(l_n*20)+9] += A[101] * B[(l_n*36)+30];
    C[(l_n*20)+18] += A[102] * B[(l_n*36)+30];
    C[(l_n*20)+19] += A[103] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*20)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*20)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*20)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*20)+19], c32_3);
#else
    C[(l_n*20)+0] += A[104] * B[(l_n*36)+32];
    C[(l_n*20)+3] += A[105] * B[(l_n*36)+32];
    C[(l_n*20)+9] += A[106] * B[(l_n*36)+32];
    C[(l_n*20)+19] += A[107] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1944;
#endif
}

void ssparse_starMatrix_m20_n9_k9_ldA20_ldBna5_ldC20_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 20; l_m++) {
    C[0+l_m] += A[120+l_m] * B[0];
    C[0+l_m] += A[140+l_m] * B[1];
    C[0+l_m] += A[160+l_m] * B[2];
    C[20+l_m] += A[120+l_m] * B[3];
    C[20+l_m] += A[140+l_m] * B[4];
    C[20+l_m] += A[160+l_m] * B[5];
    C[40+l_m] += A[120+l_m] * B[6];
    C[40+l_m] += A[140+l_m] * B[7];
    C[40+l_m] += A[160+l_m] * B[8];
    C[60+l_m] += A[120+l_m] * B[9];
    C[60+l_m] += A[140+l_m] * B[10];
    C[80+l_m] += A[140+l_m] * B[11];
    C[80+l_m] += A[160+l_m] * B[12];
    C[100+l_m] += A[120+l_m] * B[13];
    C[100+l_m] += A[160+l_m] * B[14];
    C[120+l_m] += A[0+l_m] * B[15];
    C[120+l_m] += A[60+l_m] * B[16];
    C[120+l_m] += A[100+l_m] * B[17];
    C[140+l_m] += A[20+l_m] * B[18];
    C[140+l_m] += A[60+l_m] * B[19];
    C[140+l_m] += A[80+l_m] * B[20];
    C[160+l_m] += A[40+l_m] * B[21];
    C[160+l_m] += A[80+l_m] * B[22];
    C[160+l_m] += A[100+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 960;
#endif
}

void ssparse_kXiDivMT_m10_n9_k20_ldAna5_ldB20_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*20)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+0], c1_0);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*20)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*20)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+1], c4_0);
#else
    C[(l_n*12)+1] += A[1] * B[(l_n*20)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*20)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*12)+0] += A[2] * B[(l_n*20)+5];
    C[(l_n*12)+2] += A[3] * B[(l_n*20)+5];
    C[(l_n*12)+3] += A[4] * B[(l_n*20)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*20)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+3], c7_1);
#else
    C[(l_n*12)+0] += A[5] * B[(l_n*20)+7];
    C[(l_n*12)+3] += A[6] * B[(l_n*20)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*20)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*12)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*12)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*12)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*20)+10];
    C[(l_n*12)+2] += A[8] * B[(l_n*20)+10];
    C[(l_n*12)+3] += A[9] * B[(l_n*20)+10];
    C[(l_n*12)+4] += A[10] * B[(l_n*20)+10];
    C[(l_n*12)+6] += A[11] * B[(l_n*20)+10];
    C[(l_n*12)+8] += A[12] * B[(l_n*20)+10];
    C[(l_n*12)+9] += A[13] * B[(l_n*20)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*20)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*12)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*12)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*12)+7], c11_2);
#else
    C[(l_n*12)+1] += A[14] * B[(l_n*20)+11];
    C[(l_n*12)+5] += A[15] * B[(l_n*20)+11];
    C[(l_n*12)+7] += A[16] * B[(l_n*20)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*20)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*12)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*12)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*20)+12];
    C[(l_n*12)+2] += A[18] * B[(l_n*20)+12];
    C[(l_n*12)+3] += A[19] * B[(l_n*20)+12];
    C[(l_n*12)+6] += A[20] * B[(l_n*20)+12];
    C[(l_n*12)+8] += A[21] * B[(l_n*20)+12];
    C[(l_n*12)+9] += A[22] * B[(l_n*20)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*20)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*12)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*12)+7], c14_1);
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*20)+14];
    C[(l_n*12)+7] += A[24] * B[(l_n*20)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*20)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*12)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*12)+0] += A[25] * B[(l_n*20)+15];
    C[(l_n*12)+2] += A[26] * B[(l_n*20)+15];
    C[(l_n*12)+3] += A[27] * B[(l_n*20)+15];
    C[(l_n*12)+8] += A[28] * B[(l_n*20)+15];
    C[(l_n*12)+9] += A[29] * B[(l_n*20)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*20)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*12)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*12)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*12)+9], c17_2);
#else
    C[(l_n*12)+0] += A[30] * B[(l_n*20)+17];
    C[(l_n*12)+3] += A[31] * B[(l_n*20)+17];
    C[(l_n*12)+9] += A[32] * B[(l_n*20)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 594;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna5_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_kXiDivMT_m4_n9_k10_ldAna5_ldB12_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 4; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*4)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*4)+1], c4_0);
#else
    C[(l_n*4)+1] += A[1] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*4)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*4)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*4)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*4)+0] += A[2] * B[(l_n*12)+5];
    C[(l_n*4)+2] += A[3] * B[(l_n*12)+5];
    C[(l_n*4)+3] += A[4] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*4)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*4)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*4)+3], c7_1);
#else
    C[(l_n*4)+0] += A[5] * B[(l_n*12)+7];
    C[(l_n*4)+3] += A[6] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 126;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna5_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_kXiDivMT_m1_n9_k4_ldAna5_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna5_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_kXiDivMT_m35_n9_k56_ldAna6_ldB56_ldC36_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 35; l_m++) {
      C[(l_n*36)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*36)+0], c1_0);
#else
    C[(l_n*36)+0] += A[0] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*36)+1], c4_0);
#else
    C[(l_n*36)+1] += A[1] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*36)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*36)+0] += A[2] * B[(l_n*56)+5];
    C[(l_n*36)+2] += A[3] * B[(l_n*56)+5];
    C[(l_n*36)+3] += A[4] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*36)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*36)+3], c7_1);
#else
    C[(l_n*36)+0] += A[5] * B[(l_n*56)+7];
    C[(l_n*36)+3] += A[6] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*36)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*36)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*36)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*36)+0] += A[7] * B[(l_n*56)+10];
    C[(l_n*36)+2] += A[8] * B[(l_n*56)+10];
    C[(l_n*36)+3] += A[9] * B[(l_n*56)+10];
    C[(l_n*36)+4] += A[10] * B[(l_n*56)+10];
    C[(l_n*36)+6] += A[11] * B[(l_n*56)+10];
    C[(l_n*36)+8] += A[12] * B[(l_n*56)+10];
    C[(l_n*36)+9] += A[13] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*36)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*36)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*36)+7], c11_2);
#else
    C[(l_n*36)+1] += A[14] * B[(l_n*56)+11];
    C[(l_n*36)+5] += A[15] * B[(l_n*56)+11];
    C[(l_n*36)+7] += A[16] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*36)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*36)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*36)+0] += A[17] * B[(l_n*56)+12];
    C[(l_n*36)+2] += A[18] * B[(l_n*56)+12];
    C[(l_n*36)+3] += A[19] * B[(l_n*56)+12];
    C[(l_n*36)+6] += A[20] * B[(l_n*56)+12];
    C[(l_n*36)+8] += A[21] * B[(l_n*56)+12];
    C[(l_n*36)+9] += A[22] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*36)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*36)+7], c14_1);
#else
    C[(l_n*36)+1] += A[23] * B[(l_n*56)+14];
    C[(l_n*36)+7] += A[24] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*36)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*36)+0] += A[25] * B[(l_n*56)+15];
    C[(l_n*36)+2] += A[26] * B[(l_n*56)+15];
    C[(l_n*36)+3] += A[27] * B[(l_n*56)+15];
    C[(l_n*36)+8] += A[28] * B[(l_n*56)+15];
    C[(l_n*36)+9] += A[29] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*36)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*36)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*36)+9], c17_2);
#else
    C[(l_n*36)+0] += A[30] * B[(l_n*56)+17];
    C[(l_n*36)+3] += A[31] * B[(l_n*56)+17];
    C[(l_n*36)+9] += A[32] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*36)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*36)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*36)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*36)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*36)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*36)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*36)+17], c20_6);
#else
    C[(l_n*36)+1] += A[33] * B[(l_n*56)+20];
    C[(l_n*36)+5] += A[34] * B[(l_n*56)+20];
    C[(l_n*36)+7] += A[35] * B[(l_n*56)+20];
    C[(l_n*36)+10] += A[36] * B[(l_n*56)+20];
    C[(l_n*36)+12] += A[37] * B[(l_n*56)+20];
    C[(l_n*36)+15] += A[38] * B[(l_n*56)+20];
    C[(l_n*36)+17] += A[39] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*36)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*36)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*36)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*36)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*36)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*36)+0] += A[40] * B[(l_n*56)+21];
    C[(l_n*36)+2] += A[41] * B[(l_n*56)+21];
    C[(l_n*36)+3] += A[42] * B[(l_n*56)+21];
    C[(l_n*36)+4] += A[43] * B[(l_n*56)+21];
    C[(l_n*36)+6] += A[44] * B[(l_n*56)+21];
    C[(l_n*36)+8] += A[45] * B[(l_n*56)+21];
    C[(l_n*36)+9] += A[46] * B[(l_n*56)+21];
    C[(l_n*36)+11] += A[47] * B[(l_n*56)+21];
    C[(l_n*36)+13] += A[48] * B[(l_n*56)+21];
    C[(l_n*36)+14] += A[49] * B[(l_n*56)+21];
    C[(l_n*36)+16] += A[50] * B[(l_n*56)+21];
    C[(l_n*36)+18] += A[51] * B[(l_n*56)+21];
    C[(l_n*36)+19] += A[52] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*36)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*36)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*36)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*36)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*36)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*36)+17], c22_5);
#else
    C[(l_n*36)+1] += A[53] * B[(l_n*56)+22];
    C[(l_n*36)+5] += A[54] * B[(l_n*56)+22];
    C[(l_n*36)+7] += A[55] * B[(l_n*56)+22];
    C[(l_n*36)+12] += A[56] * B[(l_n*56)+22];
    C[(l_n*36)+15] += A[57] * B[(l_n*56)+22];
    C[(l_n*36)+17] += A[58] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*36)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*36)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*36)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*36)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*36)+0] += A[59] * B[(l_n*56)+23];
    C[(l_n*36)+2] += A[60] * B[(l_n*56)+23];
    C[(l_n*36)+3] += A[61] * B[(l_n*56)+23];
    C[(l_n*36)+6] += A[62] * B[(l_n*56)+23];
    C[(l_n*36)+8] += A[63] * B[(l_n*56)+23];
    C[(l_n*36)+9] += A[64] * B[(l_n*56)+23];
    C[(l_n*36)+13] += A[65] * B[(l_n*56)+23];
    C[(l_n*36)+16] += A[66] * B[(l_n*56)+23];
    C[(l_n*36)+18] += A[67] * B[(l_n*56)+23];
    C[(l_n*36)+19] += A[68] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*36)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*36)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*36)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*36)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*36)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*36)+0] += A[69] * B[(l_n*56)+25];
    C[(l_n*36)+2] += A[70] * B[(l_n*56)+25];
    C[(l_n*36)+3] += A[71] * B[(l_n*56)+25];
    C[(l_n*36)+4] += A[72] * B[(l_n*56)+25];
    C[(l_n*36)+6] += A[73] * B[(l_n*56)+25];
    C[(l_n*36)+8] += A[74] * B[(l_n*56)+25];
    C[(l_n*36)+9] += A[75] * B[(l_n*56)+25];
    C[(l_n*36)+14] += A[76] * B[(l_n*56)+25];
    C[(l_n*36)+16] += A[77] * B[(l_n*56)+25];
    C[(l_n*36)+18] += A[78] * B[(l_n*56)+25];
    C[(l_n*36)+19] += A[79] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*36)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*36)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*36)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*36)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*36)+17], c26_4);
#else
    C[(l_n*36)+1] += A[80] * B[(l_n*56)+26];
    C[(l_n*36)+5] += A[81] * B[(l_n*56)+26];
    C[(l_n*36)+7] += A[82] * B[(l_n*56)+26];
    C[(l_n*36)+15] += A[83] * B[(l_n*56)+26];
    C[(l_n*36)+17] += A[84] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*36)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*36)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*36)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*36)+0] += A[85] * B[(l_n*56)+27];
    C[(l_n*36)+2] += A[86] * B[(l_n*56)+27];
    C[(l_n*36)+3] += A[87] * B[(l_n*56)+27];
    C[(l_n*36)+6] += A[88] * B[(l_n*56)+27];
    C[(l_n*36)+8] += A[89] * B[(l_n*56)+27];
    C[(l_n*36)+9] += A[90] * B[(l_n*56)+27];
    C[(l_n*36)+16] += A[91] * B[(l_n*56)+27];
    C[(l_n*36)+18] += A[92] * B[(l_n*56)+27];
    C[(l_n*36)+19] += A[93] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*36)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*36)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*36)+17], c29_2);
#else
    C[(l_n*36)+1] += A[94] * B[(l_n*56)+29];
    C[(l_n*36)+7] += A[95] * B[(l_n*56)+29];
    C[(l_n*36)+17] += A[96] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*36)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*36)+0] += A[97] * B[(l_n*56)+30];
    C[(l_n*36)+2] += A[98] * B[(l_n*56)+30];
    C[(l_n*36)+3] += A[99] * B[(l_n*56)+30];
    C[(l_n*36)+8] += A[100] * B[(l_n*56)+30];
    C[(l_n*36)+9] += A[101] * B[(l_n*56)+30];
    C[(l_n*36)+18] += A[102] * B[(l_n*56)+30];
    C[(l_n*36)+19] += A[103] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*36)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*36)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*36)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*36)+19], c32_3);
#else
    C[(l_n*36)+0] += A[104] * B[(l_n*56)+32];
    C[(l_n*36)+3] += A[105] * B[(l_n*56)+32];
    C[(l_n*36)+9] += A[106] * B[(l_n*56)+32];
    C[(l_n*36)+19] += A[107] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a35_0 = _mm_load_ss(&A[108]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*36)+0], c35_0);
    __m128 c35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[109]));
    c35_1 = _mm_add_ps(c35_1, _mm_mul_ps(a35_1, b35));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c35_1));
    __m128 c35_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a35_3 = _mm_load_ss(&A[111]);
    c35_3 = _mm_add_ss(c35_3, _mm_mul_ss(a35_3, b35));
    _mm_store_ss(&C[(l_n*36)+4], c35_3);
    __m128 c35_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a35_4 = _mm_load_ss(&A[112]);
    c35_4 = _mm_add_ss(c35_4, _mm_mul_ss(a35_4, b35));
    _mm_store_ss(&C[(l_n*36)+6], c35_4);
    __m128 c35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[113]));
    c35_5 = _mm_add_ps(c35_5, _mm_mul_ps(a35_5, b35));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c35_5));
    __m128 c35_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a35_7 = _mm_load_ss(&A[115]);
    c35_7 = _mm_add_ss(c35_7, _mm_mul_ss(a35_7, b35));
    _mm_store_ss(&C[(l_n*36)+11], c35_7);
    __m128 c35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[116]));
    c35_8 = _mm_add_ps(c35_8, _mm_mul_ps(a35_8, b35));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c35_8));
    __m128 c35_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a35_10 = _mm_load_ss(&A[118]);
    c35_10 = _mm_add_ss(c35_10, _mm_mul_ss(a35_10, b35));
    _mm_store_ss(&C[(l_n*36)+16], c35_10);
    __m128 c35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[119]));
    c35_11 = _mm_add_ps(c35_11, _mm_mul_ps(a35_11, b35));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c35_11));
    __m128 c35_13 = _mm_load_ss(&C[(l_n*36)+20]);
    __m128 a35_13 = _mm_load_ss(&A[121]);
    c35_13 = _mm_add_ss(c35_13, _mm_mul_ss(a35_13, b35));
    _mm_store_ss(&C[(l_n*36)+20], c35_13);
    __m128 c35_14 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a35_14 = _mm_load_ss(&A[122]);
    c35_14 = _mm_add_ss(c35_14, _mm_mul_ss(a35_14, b35));
    _mm_store_ss(&C[(l_n*36)+22], c35_14);
    __m128 c35_15 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a35_15 = _mm_load_ss(&A[123]);
    c35_15 = _mm_add_ss(c35_15, _mm_mul_ss(a35_15, b35));
    _mm_store_ss(&C[(l_n*36)+24], c35_15);
    __m128 c35_16 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a35_16 = _mm_load_ss(&A[124]);
    c35_16 = _mm_add_ss(c35_16, _mm_mul_ss(a35_16, b35));
    _mm_store_ss(&C[(l_n*36)+26], c35_16);
    __m128 c35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[125]));
    c35_17 = _mm_add_ps(c35_17, _mm_mul_ps(a35_17, b35));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c35_17));
    __m128 c35_19 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a35_19 = _mm_load_ss(&A[127]);
    c35_19 = _mm_add_ss(c35_19, _mm_mul_ss(a35_19, b35));
    _mm_store_ss(&C[(l_n*36)+31], c35_19);
    __m128 c35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[128]));
    c35_20 = _mm_add_ps(c35_20, _mm_mul_ps(a35_20, b35));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c35_20));
#else
    C[(l_n*36)+0] += A[108] * B[(l_n*56)+35];
    C[(l_n*36)+2] += A[109] * B[(l_n*56)+35];
    C[(l_n*36)+3] += A[110] * B[(l_n*56)+35];
    C[(l_n*36)+4] += A[111] * B[(l_n*56)+35];
    C[(l_n*36)+6] += A[112] * B[(l_n*56)+35];
    C[(l_n*36)+8] += A[113] * B[(l_n*56)+35];
    C[(l_n*36)+9] += A[114] * B[(l_n*56)+35];
    C[(l_n*36)+11] += A[115] * B[(l_n*56)+35];
    C[(l_n*36)+13] += A[116] * B[(l_n*56)+35];
    C[(l_n*36)+14] += A[117] * B[(l_n*56)+35];
    C[(l_n*36)+16] += A[118] * B[(l_n*56)+35];
    C[(l_n*36)+18] += A[119] * B[(l_n*56)+35];
    C[(l_n*36)+19] += A[120] * B[(l_n*56)+35];
    C[(l_n*36)+20] += A[121] * B[(l_n*56)+35];
    C[(l_n*36)+22] += A[122] * B[(l_n*56)+35];
    C[(l_n*36)+24] += A[123] * B[(l_n*56)+35];
    C[(l_n*36)+26] += A[124] * B[(l_n*56)+35];
    C[(l_n*36)+28] += A[125] * B[(l_n*56)+35];
    C[(l_n*36)+29] += A[126] * B[(l_n*56)+35];
    C[(l_n*36)+31] += A[127] * B[(l_n*56)+35];
    C[(l_n*36)+33] += A[128] * B[(l_n*56)+35];
    C[(l_n*36)+34] += A[129] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a36_0 = _mm_load_ss(&A[130]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*36)+1], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a36_1 = _mm_load_ss(&A[131]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*36)+5], c36_1);
    __m128 c36_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a36_2 = _mm_load_ss(&A[132]);
    c36_2 = _mm_add_ss(c36_2, _mm_mul_ss(a36_2, b36));
    _mm_store_ss(&C[(l_n*36)+7], c36_2);
    __m128 c36_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a36_3 = _mm_load_ss(&A[133]);
    c36_3 = _mm_add_ss(c36_3, _mm_mul_ss(a36_3, b36));
    _mm_store_ss(&C[(l_n*36)+10], c36_3);
    __m128 c36_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a36_4 = _mm_load_ss(&A[134]);
    c36_4 = _mm_add_ss(c36_4, _mm_mul_ss(a36_4, b36));
    _mm_store_ss(&C[(l_n*36)+12], c36_4);
    __m128 c36_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a36_5 = _mm_load_ss(&A[135]);
    c36_5 = _mm_add_ss(c36_5, _mm_mul_ss(a36_5, b36));
    _mm_store_ss(&C[(l_n*36)+15], c36_5);
    __m128 c36_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a36_6 = _mm_load_ss(&A[136]);
    c36_6 = _mm_add_ss(c36_6, _mm_mul_ss(a36_6, b36));
    _mm_store_ss(&C[(l_n*36)+17], c36_6);
    __m128 c36_7 = _mm_load_ss(&C[(l_n*36)+21]);
    __m128 a36_7 = _mm_load_ss(&A[137]);
    c36_7 = _mm_add_ss(c36_7, _mm_mul_ss(a36_7, b36));
    _mm_store_ss(&C[(l_n*36)+21], c36_7);
    __m128 c36_8 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a36_8 = _mm_load_ss(&A[138]);
    c36_8 = _mm_add_ss(c36_8, _mm_mul_ss(a36_8, b36));
    _mm_store_ss(&C[(l_n*36)+23], c36_8);
    __m128 c36_9 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a36_9 = _mm_load_ss(&A[139]);
    c36_9 = _mm_add_ss(c36_9, _mm_mul_ss(a36_9, b36));
    _mm_store_ss(&C[(l_n*36)+25], c36_9);
    __m128 c36_10 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a36_10 = _mm_load_ss(&A[140]);
    c36_10 = _mm_add_ss(c36_10, _mm_mul_ss(a36_10, b36));
    _mm_store_ss(&C[(l_n*36)+27], c36_10);
    __m128 c36_11 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a36_11 = _mm_load_ss(&A[141]);
    c36_11 = _mm_add_ss(c36_11, _mm_mul_ss(a36_11, b36));
    _mm_store_ss(&C[(l_n*36)+30], c36_11);
    __m128 c36_12 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a36_12 = _mm_load_ss(&A[142]);
    c36_12 = _mm_add_ss(c36_12, _mm_mul_ss(a36_12, b36));
    _mm_store_ss(&C[(l_n*36)+32], c36_12);
#else
    C[(l_n*36)+1] += A[130] * B[(l_n*56)+36];
    C[(l_n*36)+5] += A[131] * B[(l_n*56)+36];
    C[(l_n*36)+7] += A[132] * B[(l_n*56)+36];
    C[(l_n*36)+10] += A[133] * B[(l_n*56)+36];
    C[(l_n*36)+12] += A[134] * B[(l_n*56)+36];
    C[(l_n*36)+15] += A[135] * B[(l_n*56)+36];
    C[(l_n*36)+17] += A[136] * B[(l_n*56)+36];
    C[(l_n*36)+21] += A[137] * B[(l_n*56)+36];
    C[(l_n*36)+23] += A[138] * B[(l_n*56)+36];
    C[(l_n*36)+25] += A[139] * B[(l_n*56)+36];
    C[(l_n*36)+27] += A[140] * B[(l_n*56)+36];
    C[(l_n*36)+30] += A[141] * B[(l_n*56)+36];
    C[(l_n*36)+32] += A[142] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a37_0 = _mm_load_ss(&A[143]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*36)+0], c37_0);
    __m128 c37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[144]));
    c37_1 = _mm_add_ps(c37_1, _mm_mul_ps(a37_1, b37));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c37_1));
    __m128 c37_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a37_3 = _mm_load_ss(&A[146]);
    c37_3 = _mm_add_ss(c37_3, _mm_mul_ss(a37_3, b37));
    _mm_store_ss(&C[(l_n*36)+4], c37_3);
    __m128 c37_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a37_4 = _mm_load_ss(&A[147]);
    c37_4 = _mm_add_ss(c37_4, _mm_mul_ss(a37_4, b37));
    _mm_store_ss(&C[(l_n*36)+6], c37_4);
    __m128 c37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[148]));
    c37_5 = _mm_add_ps(c37_5, _mm_mul_ps(a37_5, b37));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c37_5));
    __m128 c37_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a37_7 = _mm_load_ss(&A[150]);
    c37_7 = _mm_add_ss(c37_7, _mm_mul_ss(a37_7, b37));
    _mm_store_ss(&C[(l_n*36)+11], c37_7);
    __m128 c37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[151]));
    c37_8 = _mm_add_ps(c37_8, _mm_mul_ps(a37_8, b37));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c37_8));
    __m128 c37_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a37_10 = _mm_load_ss(&A[153]);
    c37_10 = _mm_add_ss(c37_10, _mm_mul_ss(a37_10, b37));
    _mm_store_ss(&C[(l_n*36)+16], c37_10);
    __m128 c37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[154]));
    c37_11 = _mm_add_ps(c37_11, _mm_mul_ps(a37_11, b37));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c37_11));
    __m128 c37_13 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a37_13 = _mm_load_ss(&A[156]);
    c37_13 = _mm_add_ss(c37_13, _mm_mul_ss(a37_13, b37));
    _mm_store_ss(&C[(l_n*36)+22], c37_13);
    __m128 c37_14 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a37_14 = _mm_load_ss(&A[157]);
    c37_14 = _mm_add_ss(c37_14, _mm_mul_ss(a37_14, b37));
    _mm_store_ss(&C[(l_n*36)+24], c37_14);
    __m128 c37_15 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a37_15 = _mm_load_ss(&A[158]);
    c37_15 = _mm_add_ss(c37_15, _mm_mul_ss(a37_15, b37));
    _mm_store_ss(&C[(l_n*36)+26], c37_15);
    __m128 c37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[159]));
    c37_16 = _mm_add_ps(c37_16, _mm_mul_ps(a37_16, b37));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c37_16));
    __m128 c37_18 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a37_18 = _mm_load_ss(&A[161]);
    c37_18 = _mm_add_ss(c37_18, _mm_mul_ss(a37_18, b37));
    _mm_store_ss(&C[(l_n*36)+31], c37_18);
    __m128 c37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[162]));
    c37_19 = _mm_add_ps(c37_19, _mm_mul_ps(a37_19, b37));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c37_19));
#else
    C[(l_n*36)+0] += A[143] * B[(l_n*56)+37];
    C[(l_n*36)+2] += A[144] * B[(l_n*56)+37];
    C[(l_n*36)+3] += A[145] * B[(l_n*56)+37];
    C[(l_n*36)+4] += A[146] * B[(l_n*56)+37];
    C[(l_n*36)+6] += A[147] * B[(l_n*56)+37];
    C[(l_n*36)+8] += A[148] * B[(l_n*56)+37];
    C[(l_n*36)+9] += A[149] * B[(l_n*56)+37];
    C[(l_n*36)+11] += A[150] * B[(l_n*56)+37];
    C[(l_n*36)+13] += A[151] * B[(l_n*56)+37];
    C[(l_n*36)+14] += A[152] * B[(l_n*56)+37];
    C[(l_n*36)+16] += A[153] * B[(l_n*56)+37];
    C[(l_n*36)+18] += A[154] * B[(l_n*56)+37];
    C[(l_n*36)+19] += A[155] * B[(l_n*56)+37];
    C[(l_n*36)+22] += A[156] * B[(l_n*56)+37];
    C[(l_n*36)+24] += A[157] * B[(l_n*56)+37];
    C[(l_n*36)+26] += A[158] * B[(l_n*56)+37];
    C[(l_n*36)+28] += A[159] * B[(l_n*56)+37];
    C[(l_n*36)+29] += A[160] * B[(l_n*56)+37];
    C[(l_n*36)+31] += A[161] * B[(l_n*56)+37];
    C[(l_n*36)+33] += A[162] * B[(l_n*56)+37];
    C[(l_n*36)+34] += A[163] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a38_0 = _mm_load_ss(&A[164]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*36)+1], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a38_1 = _mm_load_ss(&A[165]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*36)+5], c38_1);
    __m128 c38_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a38_2 = _mm_load_ss(&A[166]);
    c38_2 = _mm_add_ss(c38_2, _mm_mul_ss(a38_2, b38));
    _mm_store_ss(&C[(l_n*36)+7], c38_2);
    __m128 c38_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a38_3 = _mm_load_ss(&A[167]);
    c38_3 = _mm_add_ss(c38_3, _mm_mul_ss(a38_3, b38));
    _mm_store_ss(&C[(l_n*36)+12], c38_3);
    __m128 c38_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a38_4 = _mm_load_ss(&A[168]);
    c38_4 = _mm_add_ss(c38_4, _mm_mul_ss(a38_4, b38));
    _mm_store_ss(&C[(l_n*36)+15], c38_4);
    __m128 c38_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a38_5 = _mm_load_ss(&A[169]);
    c38_5 = _mm_add_ss(c38_5, _mm_mul_ss(a38_5, b38));
    _mm_store_ss(&C[(l_n*36)+17], c38_5);
    __m128 c38_6 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a38_6 = _mm_load_ss(&A[170]);
    c38_6 = _mm_add_ss(c38_6, _mm_mul_ss(a38_6, b38));
    _mm_store_ss(&C[(l_n*36)+23], c38_6);
    __m128 c38_7 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a38_7 = _mm_load_ss(&A[171]);
    c38_7 = _mm_add_ss(c38_7, _mm_mul_ss(a38_7, b38));
    _mm_store_ss(&C[(l_n*36)+27], c38_7);
    __m128 c38_8 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a38_8 = _mm_load_ss(&A[172]);
    c38_8 = _mm_add_ss(c38_8, _mm_mul_ss(a38_8, b38));
    _mm_store_ss(&C[(l_n*36)+30], c38_8);
    __m128 c38_9 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a38_9 = _mm_load_ss(&A[173]);
    c38_9 = _mm_add_ss(c38_9, _mm_mul_ss(a38_9, b38));
    _mm_store_ss(&C[(l_n*36)+32], c38_9);
#else
    C[(l_n*36)+1] += A[164] * B[(l_n*56)+38];
    C[(l_n*36)+5] += A[165] * B[(l_n*56)+38];
    C[(l_n*36)+7] += A[166] * B[(l_n*56)+38];
    C[(l_n*36)+12] += A[167] * B[(l_n*56)+38];
    C[(l_n*36)+15] += A[168] * B[(l_n*56)+38];
    C[(l_n*36)+17] += A[169] * B[(l_n*56)+38];
    C[(l_n*36)+23] += A[170] * B[(l_n*56)+38];
    C[(l_n*36)+27] += A[171] * B[(l_n*56)+38];
    C[(l_n*36)+30] += A[172] * B[(l_n*56)+38];
    C[(l_n*36)+32] += A[173] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a39_0 = _mm_load_ss(&A[174]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*36)+0], c39_0);
    __m128 c39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[175]));
    c39_1 = _mm_add_ps(c39_1, _mm_mul_ps(a39_1, b39));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c39_1));
    __m128 c39_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a39_3 = _mm_load_ss(&A[177]);
    c39_3 = _mm_add_ss(c39_3, _mm_mul_ss(a39_3, b39));
    _mm_store_ss(&C[(l_n*36)+6], c39_3);
    __m128 c39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[178]));
    c39_4 = _mm_add_ps(c39_4, _mm_mul_ps(a39_4, b39));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c39_4));
    __m128 c39_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a39_6 = _mm_load_ss(&A[180]);
    c39_6 = _mm_add_ss(c39_6, _mm_mul_ss(a39_6, b39));
    _mm_store_ss(&C[(l_n*36)+13], c39_6);
    __m128 c39_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a39_7 = _mm_load_ss(&A[181]);
    c39_7 = _mm_add_ss(c39_7, _mm_mul_ss(a39_7, b39));
    _mm_store_ss(&C[(l_n*36)+16], c39_7);
    __m128 c39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[182]));
    c39_8 = _mm_add_ps(c39_8, _mm_mul_ps(a39_8, b39));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c39_8));
    __m128 c39_10 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a39_10 = _mm_load_ss(&A[184]);
    c39_10 = _mm_add_ss(c39_10, _mm_mul_ss(a39_10, b39));
    _mm_store_ss(&C[(l_n*36)+24], c39_10);
    __m128 c39_11 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a39_11 = _mm_load_ss(&A[185]);
    c39_11 = _mm_add_ss(c39_11, _mm_mul_ss(a39_11, b39));
    _mm_store_ss(&C[(l_n*36)+28], c39_11);
    __m128 c39_12 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a39_12 = _mm_load_ss(&A[186]);
    c39_12 = _mm_add_ss(c39_12, _mm_mul_ss(a39_12, b39));
    _mm_store_ss(&C[(l_n*36)+31], c39_12);
    __m128 c39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[187]));
    c39_13 = _mm_add_ps(c39_13, _mm_mul_ps(a39_13, b39));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c39_13));
#else
    C[(l_n*36)+0] += A[174] * B[(l_n*56)+39];
    C[(l_n*36)+2] += A[175] * B[(l_n*56)+39];
    C[(l_n*36)+3] += A[176] * B[(l_n*56)+39];
    C[(l_n*36)+6] += A[177] * B[(l_n*56)+39];
    C[(l_n*36)+8] += A[178] * B[(l_n*56)+39];
    C[(l_n*36)+9] += A[179] * B[(l_n*56)+39];
    C[(l_n*36)+13] += A[180] * B[(l_n*56)+39];
    C[(l_n*36)+16] += A[181] * B[(l_n*56)+39];
    C[(l_n*36)+18] += A[182] * B[(l_n*56)+39];
    C[(l_n*36)+19] += A[183] * B[(l_n*56)+39];
    C[(l_n*36)+24] += A[184] * B[(l_n*56)+39];
    C[(l_n*36)+28] += A[185] * B[(l_n*56)+39];
    C[(l_n*36)+31] += A[186] * B[(l_n*56)+39];
    C[(l_n*36)+33] += A[187] * B[(l_n*56)+39];
    C[(l_n*36)+34] += A[188] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a41_0 = _mm_load_ss(&A[189]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*36)+1], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a41_1 = _mm_load_ss(&A[190]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*36)+5], c41_1);
    __m128 c41_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a41_2 = _mm_load_ss(&A[191]);
    c41_2 = _mm_add_ss(c41_2, _mm_mul_ss(a41_2, b41));
    _mm_store_ss(&C[(l_n*36)+7], c41_2);
    __m128 c41_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a41_3 = _mm_load_ss(&A[192]);
    c41_3 = _mm_add_ss(c41_3, _mm_mul_ss(a41_3, b41));
    _mm_store_ss(&C[(l_n*36)+10], c41_3);
    __m128 c41_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a41_4 = _mm_load_ss(&A[193]);
    c41_4 = _mm_add_ss(c41_4, _mm_mul_ss(a41_4, b41));
    _mm_store_ss(&C[(l_n*36)+12], c41_4);
    __m128 c41_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a41_5 = _mm_load_ss(&A[194]);
    c41_5 = _mm_add_ss(c41_5, _mm_mul_ss(a41_5, b41));
    _mm_store_ss(&C[(l_n*36)+15], c41_5);
    __m128 c41_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a41_6 = _mm_load_ss(&A[195]);
    c41_6 = _mm_add_ss(c41_6, _mm_mul_ss(a41_6, b41));
    _mm_store_ss(&C[(l_n*36)+17], c41_6);
    __m128 c41_7 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a41_7 = _mm_load_ss(&A[196]);
    c41_7 = _mm_add_ss(c41_7, _mm_mul_ss(a41_7, b41));
    _mm_store_ss(&C[(l_n*36)+25], c41_7);
    __m128 c41_8 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a41_8 = _mm_load_ss(&A[197]);
    c41_8 = _mm_add_ss(c41_8, _mm_mul_ss(a41_8, b41));
    _mm_store_ss(&C[(l_n*36)+27], c41_8);
    __m128 c41_9 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a41_9 = _mm_load_ss(&A[198]);
    c41_9 = _mm_add_ss(c41_9, _mm_mul_ss(a41_9, b41));
    _mm_store_ss(&C[(l_n*36)+30], c41_9);
    __m128 c41_10 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a41_10 = _mm_load_ss(&A[199]);
    c41_10 = _mm_add_ss(c41_10, _mm_mul_ss(a41_10, b41));
    _mm_store_ss(&C[(l_n*36)+32], c41_10);
#else
    C[(l_n*36)+1] += A[189] * B[(l_n*56)+41];
    C[(l_n*36)+5] += A[190] * B[(l_n*56)+41];
    C[(l_n*36)+7] += A[191] * B[(l_n*56)+41];
    C[(l_n*36)+10] += A[192] * B[(l_n*56)+41];
    C[(l_n*36)+12] += A[193] * B[(l_n*56)+41];
    C[(l_n*36)+15] += A[194] * B[(l_n*56)+41];
    C[(l_n*36)+17] += A[195] * B[(l_n*56)+41];
    C[(l_n*36)+25] += A[196] * B[(l_n*56)+41];
    C[(l_n*36)+27] += A[197] * B[(l_n*56)+41];
    C[(l_n*36)+30] += A[198] * B[(l_n*56)+41];
    C[(l_n*36)+32] += A[199] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a42_0 = _mm_load_ss(&A[200]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*36)+0], c42_0);
    __m128 c42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[201]));
    c42_1 = _mm_add_ps(c42_1, _mm_mul_ps(a42_1, b42));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c42_1));
    __m128 c42_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a42_3 = _mm_load_ss(&A[203]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*36)+4], c42_3);
    __m128 c42_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a42_4 = _mm_load_ss(&A[204]);
    c42_4 = _mm_add_ss(c42_4, _mm_mul_ss(a42_4, b42));
    _mm_store_ss(&C[(l_n*36)+6], c42_4);
    __m128 c42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[205]));
    c42_5 = _mm_add_ps(c42_5, _mm_mul_ps(a42_5, b42));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c42_5));
    __m128 c42_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a42_7 = _mm_load_ss(&A[207]);
    c42_7 = _mm_add_ss(c42_7, _mm_mul_ss(a42_7, b42));
    _mm_store_ss(&C[(l_n*36)+11], c42_7);
    __m128 c42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[208]));
    c42_8 = _mm_add_ps(c42_8, _mm_mul_ps(a42_8, b42));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c42_8));
    __m128 c42_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a42_10 = _mm_load_ss(&A[210]);
    c42_10 = _mm_add_ss(c42_10, _mm_mul_ss(a42_10, b42));
    _mm_store_ss(&C[(l_n*36)+16], c42_10);
    __m128 c42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[211]));
    c42_11 = _mm_add_ps(c42_11, _mm_mul_ps(a42_11, b42));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c42_11));
    __m128 c42_13 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a42_13 = _mm_load_ss(&A[213]);
    c42_13 = _mm_add_ss(c42_13, _mm_mul_ss(a42_13, b42));
    _mm_store_ss(&C[(l_n*36)+26], c42_13);
    __m128 c42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&A[214]));
    c42_14 = _mm_add_ps(c42_14, _mm_mul_ps(a42_14, b42));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c42_14));
    __m128 c42_16 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a42_16 = _mm_load_ss(&A[216]);
    c42_16 = _mm_add_ss(c42_16, _mm_mul_ss(a42_16, b42));
    _mm_store_ss(&C[(l_n*36)+31], c42_16);
    __m128 c42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[217]));
    c42_17 = _mm_add_ps(c42_17, _mm_mul_ps(a42_17, b42));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c42_17));
#else
    C[(l_n*36)+0] += A[200] * B[(l_n*56)+42];
    C[(l_n*36)+2] += A[201] * B[(l_n*56)+42];
    C[(l_n*36)+3] += A[202] * B[(l_n*56)+42];
    C[(l_n*36)+4] += A[203] * B[(l_n*56)+42];
    C[(l_n*36)+6] += A[204] * B[(l_n*56)+42];
    C[(l_n*36)+8] += A[205] * B[(l_n*56)+42];
    C[(l_n*36)+9] += A[206] * B[(l_n*56)+42];
    C[(l_n*36)+11] += A[207] * B[(l_n*56)+42];
    C[(l_n*36)+13] += A[208] * B[(l_n*56)+42];
    C[(l_n*36)+14] += A[209] * B[(l_n*56)+42];
    C[(l_n*36)+16] += A[210] * B[(l_n*56)+42];
    C[(l_n*36)+18] += A[211] * B[(l_n*56)+42];
    C[(l_n*36)+19] += A[212] * B[(l_n*56)+42];
    C[(l_n*36)+26] += A[213] * B[(l_n*56)+42];
    C[(l_n*36)+28] += A[214] * B[(l_n*56)+42];
    C[(l_n*36)+29] += A[215] * B[(l_n*56)+42];
    C[(l_n*36)+31] += A[216] * B[(l_n*56)+42];
    C[(l_n*36)+33] += A[217] * B[(l_n*56)+42];
    C[(l_n*36)+34] += A[218] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a43_0 = _mm_load_ss(&A[219]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*36)+1], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a43_1 = _mm_load_ss(&A[220]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*36)+5], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a43_2 = _mm_load_ss(&A[221]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*36)+7], c43_2);
    __m128 c43_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a43_3 = _mm_load_ss(&A[222]);
    c43_3 = _mm_add_ss(c43_3, _mm_mul_ss(a43_3, b43));
    _mm_store_ss(&C[(l_n*36)+12], c43_3);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a43_4 = _mm_load_ss(&A[223]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*36)+15], c43_4);
    __m128 c43_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a43_5 = _mm_load_ss(&A[224]);
    c43_5 = _mm_add_ss(c43_5, _mm_mul_ss(a43_5, b43));
    _mm_store_ss(&C[(l_n*36)+17], c43_5);
    __m128 c43_6 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a43_6 = _mm_load_ss(&A[225]);
    c43_6 = _mm_add_ss(c43_6, _mm_mul_ss(a43_6, b43));
    _mm_store_ss(&C[(l_n*36)+27], c43_6);
    __m128 c43_7 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a43_7 = _mm_load_ss(&A[226]);
    c43_7 = _mm_add_ss(c43_7, _mm_mul_ss(a43_7, b43));
    _mm_store_ss(&C[(l_n*36)+30], c43_7);
    __m128 c43_8 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a43_8 = _mm_load_ss(&A[227]);
    c43_8 = _mm_add_ss(c43_8, _mm_mul_ss(a43_8, b43));
    _mm_store_ss(&C[(l_n*36)+32], c43_8);
#else
    C[(l_n*36)+1] += A[219] * B[(l_n*56)+43];
    C[(l_n*36)+5] += A[220] * B[(l_n*56)+43];
    C[(l_n*36)+7] += A[221] * B[(l_n*56)+43];
    C[(l_n*36)+12] += A[222] * B[(l_n*56)+43];
    C[(l_n*36)+15] += A[223] * B[(l_n*56)+43];
    C[(l_n*36)+17] += A[224] * B[(l_n*56)+43];
    C[(l_n*36)+27] += A[225] * B[(l_n*56)+43];
    C[(l_n*36)+30] += A[226] * B[(l_n*56)+43];
    C[(l_n*36)+32] += A[227] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a44_0 = _mm_load_ss(&A[228]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*36)+0], c44_0);
    __m128 c44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[229]));
    c44_1 = _mm_add_ps(c44_1, _mm_mul_ps(a44_1, b44));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c44_1));
    __m128 c44_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a44_3 = _mm_load_ss(&A[231]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*36)+6], c44_3);
    __m128 c44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[232]));
    c44_4 = _mm_add_ps(c44_4, _mm_mul_ps(a44_4, b44));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c44_4));
    __m128 c44_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a44_6 = _mm_load_ss(&A[234]);
    c44_6 = _mm_add_ss(c44_6, _mm_mul_ss(a44_6, b44));
    _mm_store_ss(&C[(l_n*36)+13], c44_6);
    __m128 c44_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a44_7 = _mm_load_ss(&A[235]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*36)+16], c44_7);
    __m128 c44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[236]));
    c44_8 = _mm_add_ps(c44_8, _mm_mul_ps(a44_8, b44));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c44_8));
    __m128 c44_10 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a44_10 = _mm_load_ss(&A[238]);
    c44_10 = _mm_add_ss(c44_10, _mm_mul_ss(a44_10, b44));
    _mm_store_ss(&C[(l_n*36)+28], c44_10);
    __m128 c44_11 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a44_11 = _mm_load_ss(&A[239]);
    c44_11 = _mm_add_ss(c44_11, _mm_mul_ss(a44_11, b44));
    _mm_store_ss(&C[(l_n*36)+31], c44_11);
    __m128 c44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[240]));
    c44_12 = _mm_add_ps(c44_12, _mm_mul_ps(a44_12, b44));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c44_12));
#else
    C[(l_n*36)+0] += A[228] * B[(l_n*56)+44];
    C[(l_n*36)+2] += A[229] * B[(l_n*56)+44];
    C[(l_n*36)+3] += A[230] * B[(l_n*56)+44];
    C[(l_n*36)+6] += A[231] * B[(l_n*56)+44];
    C[(l_n*36)+8] += A[232] * B[(l_n*56)+44];
    C[(l_n*36)+9] += A[233] * B[(l_n*56)+44];
    C[(l_n*36)+13] += A[234] * B[(l_n*56)+44];
    C[(l_n*36)+16] += A[235] * B[(l_n*56)+44];
    C[(l_n*36)+18] += A[236] * B[(l_n*56)+44];
    C[(l_n*36)+19] += A[237] * B[(l_n*56)+44];
    C[(l_n*36)+28] += A[238] * B[(l_n*56)+44];
    C[(l_n*36)+31] += A[239] * B[(l_n*56)+44];
    C[(l_n*36)+33] += A[240] * B[(l_n*56)+44];
    C[(l_n*36)+34] += A[241] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a46_0 = _mm_load_ss(&A[242]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*36)+0], c46_0);
    __m128 c46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[243]));
    c46_1 = _mm_add_ps(c46_1, _mm_mul_ps(a46_1, b46));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c46_1));
    __m128 c46_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a46_3 = _mm_load_ss(&A[245]);
    c46_3 = _mm_add_ss(c46_3, _mm_mul_ss(a46_3, b46));
    _mm_store_ss(&C[(l_n*36)+4], c46_3);
    __m128 c46_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a46_4 = _mm_load_ss(&A[246]);
    c46_4 = _mm_add_ss(c46_4, _mm_mul_ss(a46_4, b46));
    _mm_store_ss(&C[(l_n*36)+6], c46_4);
    __m128 c46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[247]));
    c46_5 = _mm_add_ps(c46_5, _mm_mul_ps(a46_5, b46));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c46_5));
    __m128 c46_7 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a46_7 = _mm_load_ss(&A[249]);
    c46_7 = _mm_add_ss(c46_7, _mm_mul_ss(a46_7, b46));
    _mm_store_ss(&C[(l_n*36)+14], c46_7);
    __m128 c46_8 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a46_8 = _mm_load_ss(&A[250]);
    c46_8 = _mm_add_ss(c46_8, _mm_mul_ss(a46_8, b46));
    _mm_store_ss(&C[(l_n*36)+16], c46_8);
    __m128 c46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[251]));
    c46_9 = _mm_add_ps(c46_9, _mm_mul_ps(a46_9, b46));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c46_9));
    __m128 c46_11 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a46_11 = _mm_load_ss(&A[253]);
    c46_11 = _mm_add_ss(c46_11, _mm_mul_ss(a46_11, b46));
    _mm_store_ss(&C[(l_n*36)+29], c46_11);
    __m128 c46_12 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a46_12 = _mm_load_ss(&A[254]);
    c46_12 = _mm_add_ss(c46_12, _mm_mul_ss(a46_12, b46));
    _mm_store_ss(&C[(l_n*36)+31], c46_12);
    __m128 c46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[255]));
    c46_13 = _mm_add_ps(c46_13, _mm_mul_ps(a46_13, b46));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c46_13));
#else
    C[(l_n*36)+0] += A[242] * B[(l_n*56)+46];
    C[(l_n*36)+2] += A[243] * B[(l_n*56)+46];
    C[(l_n*36)+3] += A[244] * B[(l_n*56)+46];
    C[(l_n*36)+4] += A[245] * B[(l_n*56)+46];
    C[(l_n*36)+6] += A[246] * B[(l_n*56)+46];
    C[(l_n*36)+8] += A[247] * B[(l_n*56)+46];
    C[(l_n*36)+9] += A[248] * B[(l_n*56)+46];
    C[(l_n*36)+14] += A[249] * B[(l_n*56)+46];
    C[(l_n*36)+16] += A[250] * B[(l_n*56)+46];
    C[(l_n*36)+18] += A[251] * B[(l_n*56)+46];
    C[(l_n*36)+19] += A[252] * B[(l_n*56)+46];
    C[(l_n*36)+29] += A[253] * B[(l_n*56)+46];
    C[(l_n*36)+31] += A[254] * B[(l_n*56)+46];
    C[(l_n*36)+33] += A[255] * B[(l_n*56)+46];
    C[(l_n*36)+34] += A[256] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a47_0 = _mm_load_ss(&A[257]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*36)+1], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a47_1 = _mm_load_ss(&A[258]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*36)+5], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a47_2 = _mm_load_ss(&A[259]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*36)+7], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a47_3 = _mm_load_ss(&A[260]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*36)+15], c47_3);
    __m128 c47_4 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a47_4 = _mm_load_ss(&A[261]);
    c47_4 = _mm_add_ss(c47_4, _mm_mul_ss(a47_4, b47));
    _mm_store_ss(&C[(l_n*36)+17], c47_4);
    __m128 c47_5 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a47_5 = _mm_load_ss(&A[262]);
    c47_5 = _mm_add_ss(c47_5, _mm_mul_ss(a47_5, b47));
    _mm_store_ss(&C[(l_n*36)+30], c47_5);
    __m128 c47_6 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a47_6 = _mm_load_ss(&A[263]);
    c47_6 = _mm_add_ss(c47_6, _mm_mul_ss(a47_6, b47));
    _mm_store_ss(&C[(l_n*36)+32], c47_6);
#else
    C[(l_n*36)+1] += A[257] * B[(l_n*56)+47];
    C[(l_n*36)+5] += A[258] * B[(l_n*56)+47];
    C[(l_n*36)+7] += A[259] * B[(l_n*56)+47];
    C[(l_n*36)+15] += A[260] * B[(l_n*56)+47];
    C[(l_n*36)+17] += A[261] * B[(l_n*56)+47];
    C[(l_n*36)+30] += A[262] * B[(l_n*56)+47];
    C[(l_n*36)+32] += A[263] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a48_0 = _mm_load_ss(&A[264]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*36)+0], c48_0);
    __m128 c48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[265]));
    c48_1 = _mm_add_ps(c48_1, _mm_mul_ps(a48_1, b48));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c48_1));
    __m128 c48_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a48_3 = _mm_load_ss(&A[267]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*36)+6], c48_3);
    __m128 c48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[268]));
    c48_4 = _mm_add_ps(c48_4, _mm_mul_ps(a48_4, b48));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c48_4));
    __m128 c48_6 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a48_6 = _mm_load_ss(&A[270]);
    c48_6 = _mm_add_ss(c48_6, _mm_mul_ss(a48_6, b48));
    _mm_store_ss(&C[(l_n*36)+16], c48_6);
    __m128 c48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[271]));
    c48_7 = _mm_add_ps(c48_7, _mm_mul_ps(a48_7, b48));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c48_7));
    __m128 c48_9 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a48_9 = _mm_load_ss(&A[273]);
    c48_9 = _mm_add_ss(c48_9, _mm_mul_ss(a48_9, b48));
    _mm_store_ss(&C[(l_n*36)+31], c48_9);
    __m128 c48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[274]));
    c48_10 = _mm_add_ps(c48_10, _mm_mul_ps(a48_10, b48));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c48_10));
#else
    C[(l_n*36)+0] += A[264] * B[(l_n*56)+48];
    C[(l_n*36)+2] += A[265] * B[(l_n*56)+48];
    C[(l_n*36)+3] += A[266] * B[(l_n*56)+48];
    C[(l_n*36)+6] += A[267] * B[(l_n*56)+48];
    C[(l_n*36)+8] += A[268] * B[(l_n*56)+48];
    C[(l_n*36)+9] += A[269] * B[(l_n*56)+48];
    C[(l_n*36)+16] += A[270] * B[(l_n*56)+48];
    C[(l_n*36)+18] += A[271] * B[(l_n*56)+48];
    C[(l_n*36)+19] += A[272] * B[(l_n*56)+48];
    C[(l_n*36)+31] += A[273] * B[(l_n*56)+48];
    C[(l_n*36)+33] += A[274] * B[(l_n*56)+48];
    C[(l_n*36)+34] += A[275] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a50_0 = _mm_load_ss(&A[276]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*36)+1], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a50_1 = _mm_load_ss(&A[277]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*36)+7], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a50_2 = _mm_load_ss(&A[278]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*36)+17], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a50_3 = _mm_load_ss(&A[279]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*36)+32], c50_3);
#else
    C[(l_n*36)+1] += A[276] * B[(l_n*56)+50];
    C[(l_n*36)+7] += A[277] * B[(l_n*56)+50];
    C[(l_n*36)+17] += A[278] * B[(l_n*56)+50];
    C[(l_n*36)+32] += A[279] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a51_0 = _mm_load_ss(&A[280]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*36)+0], c51_0);
    __m128 c51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[281]));
    c51_1 = _mm_add_ps(c51_1, _mm_mul_ps(a51_1, b51));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c51_1));
    __m128 c51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[283]));
    c51_3 = _mm_add_ps(c51_3, _mm_mul_ps(a51_3, b51));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c51_3));
    __m128 c51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[285]));
    c51_5 = _mm_add_ps(c51_5, _mm_mul_ps(a51_5, b51));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c51_5));
    __m128 c51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[287]));
    c51_7 = _mm_add_ps(c51_7, _mm_mul_ps(a51_7, b51));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c51_7));
#else
    C[(l_n*36)+0] += A[280] * B[(l_n*56)+51];
    C[(l_n*36)+2] += A[281] * B[(l_n*56)+51];
    C[(l_n*36)+3] += A[282] * B[(l_n*56)+51];
    C[(l_n*36)+8] += A[283] * B[(l_n*56)+51];
    C[(l_n*36)+9] += A[284] * B[(l_n*56)+51];
    C[(l_n*36)+18] += A[285] * B[(l_n*56)+51];
    C[(l_n*36)+19] += A[286] * B[(l_n*56)+51];
    C[(l_n*36)+33] += A[287] * B[(l_n*56)+51];
    C[(l_n*36)+34] += A[288] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a53_0 = _mm_load_ss(&A[289]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*36)+0], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a53_1 = _mm_load_ss(&A[290]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*36)+3], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a53_2 = _mm_load_ss(&A[291]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*36)+9], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a53_3 = _mm_load_ss(&A[292]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*36)+19], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a53_4 = _mm_load_ss(&A[293]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*36)+34], c53_4);
#else
    C[(l_n*36)+0] += A[289] * B[(l_n*56)+53];
    C[(l_n*36)+3] += A[290] * B[(l_n*56)+53];
    C[(l_n*36)+9] += A[291] * B[(l_n*56)+53];
    C[(l_n*36)+19] += A[292] * B[(l_n*56)+53];
    C[(l_n*36)+34] += A[293] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 5292;
#endif
}

void ssparse_starMatrix_m35_n9_k9_ldA36_ldBna6_ldC36_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 35; l_m++) {
    C[0+l_m] += A[216+l_m] * B[0];
    C[0+l_m] += A[252+l_m] * B[1];
    C[0+l_m] += A[288+l_m] * B[2];
    C[36+l_m] += A[216+l_m] * B[3];
    C[36+l_m] += A[252+l_m] * B[4];
    C[36+l_m] += A[288+l_m] * B[5];
    C[72+l_m] += A[216+l_m] * B[6];
    C[72+l_m] += A[252+l_m] * B[7];
    C[72+l_m] += A[288+l_m] * B[8];
    C[108+l_m] += A[216+l_m] * B[9];
    C[108+l_m] += A[252+l_m] * B[10];
    C[144+l_m] += A[252+l_m] * B[11];
    C[144+l_m] += A[288+l_m] * B[12];
    C[180+l_m] += A[216+l_m] * B[13];
    C[180+l_m] += A[288+l_m] * B[14];
    C[216+l_m] += A[0+l_m] * B[15];
    C[216+l_m] += A[108+l_m] * B[16];
    C[216+l_m] += A[180+l_m] * B[17];
    C[252+l_m] += A[36+l_m] * B[18];
    C[252+l_m] += A[108+l_m] * B[19];
    C[252+l_m] += A[144+l_m] * B[20];
    C[288+l_m] += A[72+l_m] * B[21];
    C[288+l_m] += A[144+l_m] * B[22];
    C[288+l_m] += A[180+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1680;
#endif
}

void ssparse_kXiDivMT_m20_n9_k35_ldAna6_ldB36_ldC20_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 20; l_m++) {
      C[(l_n*20)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*20)+0], c1_0);
#else
    C[(l_n*20)+0] += A[0] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*20)+1], c4_0);
#else
    C[(l_n*20)+1] += A[1] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*20)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*20)+0] += A[2] * B[(l_n*36)+5];
    C[(l_n*20)+2] += A[3] * B[(l_n*36)+5];
    C[(l_n*20)+3] += A[4] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*20)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*20)+3], c7_1);
#else
    C[(l_n*20)+0] += A[5] * B[(l_n*36)+7];
    C[(l_n*20)+3] += A[6] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*20)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*20)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*20)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*20)+0] += A[7] * B[(l_n*36)+10];
    C[(l_n*20)+2] += A[8] * B[(l_n*36)+10];
    C[(l_n*20)+3] += A[9] * B[(l_n*36)+10];
    C[(l_n*20)+4] += A[10] * B[(l_n*36)+10];
    C[(l_n*20)+6] += A[11] * B[(l_n*36)+10];
    C[(l_n*20)+8] += A[12] * B[(l_n*36)+10];
    C[(l_n*20)+9] += A[13] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*20)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*20)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*20)+7], c11_2);
#else
    C[(l_n*20)+1] += A[14] * B[(l_n*36)+11];
    C[(l_n*20)+5] += A[15] * B[(l_n*36)+11];
    C[(l_n*20)+7] += A[16] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*20)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*20)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*20)+0] += A[17] * B[(l_n*36)+12];
    C[(l_n*20)+2] += A[18] * B[(l_n*36)+12];
    C[(l_n*20)+3] += A[19] * B[(l_n*36)+12];
    C[(l_n*20)+6] += A[20] * B[(l_n*36)+12];
    C[(l_n*20)+8] += A[21] * B[(l_n*36)+12];
    C[(l_n*20)+9] += A[22] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*20)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*20)+7], c14_1);
#else
    C[(l_n*20)+1] += A[23] * B[(l_n*36)+14];
    C[(l_n*20)+7] += A[24] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*20)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*20)+0] += A[25] * B[(l_n*36)+15];
    C[(l_n*20)+2] += A[26] * B[(l_n*36)+15];
    C[(l_n*20)+3] += A[27] * B[(l_n*36)+15];
    C[(l_n*20)+8] += A[28] * B[(l_n*36)+15];
    C[(l_n*20)+9] += A[29] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*20)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*20)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*20)+9], c17_2);
#else
    C[(l_n*20)+0] += A[30] * B[(l_n*36)+17];
    C[(l_n*20)+3] += A[31] * B[(l_n*36)+17];
    C[(l_n*20)+9] += A[32] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*20)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*20)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*20)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*20)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*20)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*20)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*20)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*20)+17], c20_6);
#else
    C[(l_n*20)+1] += A[33] * B[(l_n*36)+20];
    C[(l_n*20)+5] += A[34] * B[(l_n*36)+20];
    C[(l_n*20)+7] += A[35] * B[(l_n*36)+20];
    C[(l_n*20)+10] += A[36] * B[(l_n*36)+20];
    C[(l_n*20)+12] += A[37] * B[(l_n*36)+20];
    C[(l_n*20)+15] += A[38] * B[(l_n*36)+20];
    C[(l_n*20)+17] += A[39] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*20)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*20)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*20)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*20)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*20)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*20)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*20)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*20)+0] += A[40] * B[(l_n*36)+21];
    C[(l_n*20)+2] += A[41] * B[(l_n*36)+21];
    C[(l_n*20)+3] += A[42] * B[(l_n*36)+21];
    C[(l_n*20)+4] += A[43] * B[(l_n*36)+21];
    C[(l_n*20)+6] += A[44] * B[(l_n*36)+21];
    C[(l_n*20)+8] += A[45] * B[(l_n*36)+21];
    C[(l_n*20)+9] += A[46] * B[(l_n*36)+21];
    C[(l_n*20)+11] += A[47] * B[(l_n*36)+21];
    C[(l_n*20)+13] += A[48] * B[(l_n*36)+21];
    C[(l_n*20)+14] += A[49] * B[(l_n*36)+21];
    C[(l_n*20)+16] += A[50] * B[(l_n*36)+21];
    C[(l_n*20)+18] += A[51] * B[(l_n*36)+21];
    C[(l_n*20)+19] += A[52] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*20)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*20)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*20)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*20)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*20)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*20)+17], c22_5);
#else
    C[(l_n*20)+1] += A[53] * B[(l_n*36)+22];
    C[(l_n*20)+5] += A[54] * B[(l_n*36)+22];
    C[(l_n*20)+7] += A[55] * B[(l_n*36)+22];
    C[(l_n*20)+12] += A[56] * B[(l_n*36)+22];
    C[(l_n*20)+15] += A[57] * B[(l_n*36)+22];
    C[(l_n*20)+17] += A[58] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*20)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*20)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*20)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*20)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*20)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*20)+0] += A[59] * B[(l_n*36)+23];
    C[(l_n*20)+2] += A[60] * B[(l_n*36)+23];
    C[(l_n*20)+3] += A[61] * B[(l_n*36)+23];
    C[(l_n*20)+6] += A[62] * B[(l_n*36)+23];
    C[(l_n*20)+8] += A[63] * B[(l_n*36)+23];
    C[(l_n*20)+9] += A[64] * B[(l_n*36)+23];
    C[(l_n*20)+13] += A[65] * B[(l_n*36)+23];
    C[(l_n*20)+16] += A[66] * B[(l_n*36)+23];
    C[(l_n*20)+18] += A[67] * B[(l_n*36)+23];
    C[(l_n*20)+19] += A[68] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*20)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*20)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*20)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*20)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*20)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*20)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*20)+0] += A[69] * B[(l_n*36)+25];
    C[(l_n*20)+2] += A[70] * B[(l_n*36)+25];
    C[(l_n*20)+3] += A[71] * B[(l_n*36)+25];
    C[(l_n*20)+4] += A[72] * B[(l_n*36)+25];
    C[(l_n*20)+6] += A[73] * B[(l_n*36)+25];
    C[(l_n*20)+8] += A[74] * B[(l_n*36)+25];
    C[(l_n*20)+9] += A[75] * B[(l_n*36)+25];
    C[(l_n*20)+14] += A[76] * B[(l_n*36)+25];
    C[(l_n*20)+16] += A[77] * B[(l_n*36)+25];
    C[(l_n*20)+18] += A[78] * B[(l_n*36)+25];
    C[(l_n*20)+19] += A[79] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*20)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*20)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*20)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*20)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*20)+17], c26_4);
#else
    C[(l_n*20)+1] += A[80] * B[(l_n*36)+26];
    C[(l_n*20)+5] += A[81] * B[(l_n*36)+26];
    C[(l_n*20)+7] += A[82] * B[(l_n*36)+26];
    C[(l_n*20)+15] += A[83] * B[(l_n*36)+26];
    C[(l_n*20)+17] += A[84] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*20)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*20)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*20)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*20)+0] += A[85] * B[(l_n*36)+27];
    C[(l_n*20)+2] += A[86] * B[(l_n*36)+27];
    C[(l_n*20)+3] += A[87] * B[(l_n*36)+27];
    C[(l_n*20)+6] += A[88] * B[(l_n*36)+27];
    C[(l_n*20)+8] += A[89] * B[(l_n*36)+27];
    C[(l_n*20)+9] += A[90] * B[(l_n*36)+27];
    C[(l_n*20)+16] += A[91] * B[(l_n*36)+27];
    C[(l_n*20)+18] += A[92] * B[(l_n*36)+27];
    C[(l_n*20)+19] += A[93] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*20)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*20)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*20)+17], c29_2);
#else
    C[(l_n*20)+1] += A[94] * B[(l_n*36)+29];
    C[(l_n*20)+7] += A[95] * B[(l_n*36)+29];
    C[(l_n*20)+17] += A[96] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*20)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*20)+0] += A[97] * B[(l_n*36)+30];
    C[(l_n*20)+2] += A[98] * B[(l_n*36)+30];
    C[(l_n*20)+3] += A[99] * B[(l_n*36)+30];
    C[(l_n*20)+8] += A[100] * B[(l_n*36)+30];
    C[(l_n*20)+9] += A[101] * B[(l_n*36)+30];
    C[(l_n*20)+18] += A[102] * B[(l_n*36)+30];
    C[(l_n*20)+19] += A[103] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*20)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*20)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*20)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*20)+19], c32_3);
#else
    C[(l_n*20)+0] += A[104] * B[(l_n*36)+32];
    C[(l_n*20)+3] += A[105] * B[(l_n*36)+32];
    C[(l_n*20)+9] += A[106] * B[(l_n*36)+32];
    C[(l_n*20)+19] += A[107] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1944;
#endif
}

void ssparse_starMatrix_m20_n9_k9_ldA20_ldBna6_ldC20_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 20; l_m++) {
    C[0+l_m] += A[120+l_m] * B[0];
    C[0+l_m] += A[140+l_m] * B[1];
    C[0+l_m] += A[160+l_m] * B[2];
    C[20+l_m] += A[120+l_m] * B[3];
    C[20+l_m] += A[140+l_m] * B[4];
    C[20+l_m] += A[160+l_m] * B[5];
    C[40+l_m] += A[120+l_m] * B[6];
    C[40+l_m] += A[140+l_m] * B[7];
    C[40+l_m] += A[160+l_m] * B[8];
    C[60+l_m] += A[120+l_m] * B[9];
    C[60+l_m] += A[140+l_m] * B[10];
    C[80+l_m] += A[140+l_m] * B[11];
    C[80+l_m] += A[160+l_m] * B[12];
    C[100+l_m] += A[120+l_m] * B[13];
    C[100+l_m] += A[160+l_m] * B[14];
    C[120+l_m] += A[0+l_m] * B[15];
    C[120+l_m] += A[60+l_m] * B[16];
    C[120+l_m] += A[100+l_m] * B[17];
    C[140+l_m] += A[20+l_m] * B[18];
    C[140+l_m] += A[60+l_m] * B[19];
    C[140+l_m] += A[80+l_m] * B[20];
    C[160+l_m] += A[40+l_m] * B[21];
    C[160+l_m] += A[80+l_m] * B[22];
    C[160+l_m] += A[100+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 960;
#endif
}

void ssparse_kXiDivMT_m10_n9_k20_ldAna6_ldB20_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*20)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+0], c1_0);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*20)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*20)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+1], c4_0);
#else
    C[(l_n*12)+1] += A[1] * B[(l_n*20)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*20)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*12)+0] += A[2] * B[(l_n*20)+5];
    C[(l_n*12)+2] += A[3] * B[(l_n*20)+5];
    C[(l_n*12)+3] += A[4] * B[(l_n*20)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*20)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+3], c7_1);
#else
    C[(l_n*12)+0] += A[5] * B[(l_n*20)+7];
    C[(l_n*12)+3] += A[6] * B[(l_n*20)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*20)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*12)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*12)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*12)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*20)+10];
    C[(l_n*12)+2] += A[8] * B[(l_n*20)+10];
    C[(l_n*12)+3] += A[9] * B[(l_n*20)+10];
    C[(l_n*12)+4] += A[10] * B[(l_n*20)+10];
    C[(l_n*12)+6] += A[11] * B[(l_n*20)+10];
    C[(l_n*12)+8] += A[12] * B[(l_n*20)+10];
    C[(l_n*12)+9] += A[13] * B[(l_n*20)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*20)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*12)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*12)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*12)+7], c11_2);
#else
    C[(l_n*12)+1] += A[14] * B[(l_n*20)+11];
    C[(l_n*12)+5] += A[15] * B[(l_n*20)+11];
    C[(l_n*12)+7] += A[16] * B[(l_n*20)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*20)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*12)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*12)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*20)+12];
    C[(l_n*12)+2] += A[18] * B[(l_n*20)+12];
    C[(l_n*12)+3] += A[19] * B[(l_n*20)+12];
    C[(l_n*12)+6] += A[20] * B[(l_n*20)+12];
    C[(l_n*12)+8] += A[21] * B[(l_n*20)+12];
    C[(l_n*12)+9] += A[22] * B[(l_n*20)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*20)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*12)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*12)+7], c14_1);
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*20)+14];
    C[(l_n*12)+7] += A[24] * B[(l_n*20)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*20)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*12)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*12)+0] += A[25] * B[(l_n*20)+15];
    C[(l_n*12)+2] += A[26] * B[(l_n*20)+15];
    C[(l_n*12)+3] += A[27] * B[(l_n*20)+15];
    C[(l_n*12)+8] += A[28] * B[(l_n*20)+15];
    C[(l_n*12)+9] += A[29] * B[(l_n*20)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*20)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*12)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*12)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*12)+9], c17_2);
#else
    C[(l_n*12)+0] += A[30] * B[(l_n*20)+17];
    C[(l_n*12)+3] += A[31] * B[(l_n*20)+17];
    C[(l_n*12)+9] += A[32] * B[(l_n*20)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 594;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna6_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_kXiDivMT_m4_n9_k10_ldAna6_ldB12_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 4; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*4)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*4)+1], c4_0);
#else
    C[(l_n*4)+1] += A[1] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*4)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*4)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*4)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*4)+0] += A[2] * B[(l_n*12)+5];
    C[(l_n*4)+2] += A[3] * B[(l_n*12)+5];
    C[(l_n*4)+3] += A[4] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*4)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*4)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*4)+3], c7_1);
#else
    C[(l_n*4)+0] += A[5] * B[(l_n*12)+7];
    C[(l_n*4)+3] += A[6] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 126;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna6_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_kXiDivMT_m1_n9_k4_ldAna6_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna6_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_kXiDivMT_m56_n9_k84_ldAna7_ldB84_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*84)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*84)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*56)+0], c1_0);
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*84)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*84)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*84)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*56)+1], c4_0);
#else
    C[(l_n*56)+1] += A[1] * B[(l_n*84)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*84)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*84)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*56)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*56)+0] += A[2] * B[(l_n*84)+5];
    C[(l_n*56)+2] += A[3] * B[(l_n*84)+5];
    C[(l_n*56)+3] += A[4] * B[(l_n*84)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*84)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*84)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*56)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*56)+3], c7_1);
#else
    C[(l_n*56)+0] += A[5] * B[(l_n*84)+7];
    C[(l_n*56)+3] += A[6] * B[(l_n*84)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*84)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*84)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*56)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*56)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*56)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*56)+0] += A[7] * B[(l_n*84)+10];
    C[(l_n*56)+2] += A[8] * B[(l_n*84)+10];
    C[(l_n*56)+3] += A[9] * B[(l_n*84)+10];
    C[(l_n*56)+4] += A[10] * B[(l_n*84)+10];
    C[(l_n*56)+6] += A[11] * B[(l_n*84)+10];
    C[(l_n*56)+8] += A[12] * B[(l_n*84)+10];
    C[(l_n*56)+9] += A[13] * B[(l_n*84)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*84)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*84)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*56)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*56)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*56)+7], c11_2);
#else
    C[(l_n*56)+1] += A[14] * B[(l_n*84)+11];
    C[(l_n*56)+5] += A[15] * B[(l_n*84)+11];
    C[(l_n*56)+7] += A[16] * B[(l_n*84)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*84)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*84)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*56)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*56)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*56)+0] += A[17] * B[(l_n*84)+12];
    C[(l_n*56)+2] += A[18] * B[(l_n*84)+12];
    C[(l_n*56)+3] += A[19] * B[(l_n*84)+12];
    C[(l_n*56)+6] += A[20] * B[(l_n*84)+12];
    C[(l_n*56)+8] += A[21] * B[(l_n*84)+12];
    C[(l_n*56)+9] += A[22] * B[(l_n*84)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*84)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*84)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*56)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*56)+7], c14_1);
#else
    C[(l_n*56)+1] += A[23] * B[(l_n*84)+14];
    C[(l_n*56)+7] += A[24] * B[(l_n*84)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*84)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*84)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*56)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*56)+0] += A[25] * B[(l_n*84)+15];
    C[(l_n*56)+2] += A[26] * B[(l_n*84)+15];
    C[(l_n*56)+3] += A[27] * B[(l_n*84)+15];
    C[(l_n*56)+8] += A[28] * B[(l_n*84)+15];
    C[(l_n*56)+9] += A[29] * B[(l_n*84)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*84)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*84)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*56)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*56)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*56)+9], c17_2);
#else
    C[(l_n*56)+0] += A[30] * B[(l_n*84)+17];
    C[(l_n*56)+3] += A[31] * B[(l_n*84)+17];
    C[(l_n*56)+9] += A[32] * B[(l_n*84)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*84)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*84)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*56)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*56)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*56)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*56)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*56)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*56)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*56)+17], c20_6);
#else
    C[(l_n*56)+1] += A[33] * B[(l_n*84)+20];
    C[(l_n*56)+5] += A[34] * B[(l_n*84)+20];
    C[(l_n*56)+7] += A[35] * B[(l_n*84)+20];
    C[(l_n*56)+10] += A[36] * B[(l_n*84)+20];
    C[(l_n*56)+12] += A[37] * B[(l_n*84)+20];
    C[(l_n*56)+15] += A[38] * B[(l_n*84)+20];
    C[(l_n*56)+17] += A[39] * B[(l_n*84)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*84)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*84)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*56)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*56)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*56)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*56)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*56)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*56)+0] += A[40] * B[(l_n*84)+21];
    C[(l_n*56)+2] += A[41] * B[(l_n*84)+21];
    C[(l_n*56)+3] += A[42] * B[(l_n*84)+21];
    C[(l_n*56)+4] += A[43] * B[(l_n*84)+21];
    C[(l_n*56)+6] += A[44] * B[(l_n*84)+21];
    C[(l_n*56)+8] += A[45] * B[(l_n*84)+21];
    C[(l_n*56)+9] += A[46] * B[(l_n*84)+21];
    C[(l_n*56)+11] += A[47] * B[(l_n*84)+21];
    C[(l_n*56)+13] += A[48] * B[(l_n*84)+21];
    C[(l_n*56)+14] += A[49] * B[(l_n*84)+21];
    C[(l_n*56)+16] += A[50] * B[(l_n*84)+21];
    C[(l_n*56)+18] += A[51] * B[(l_n*84)+21];
    C[(l_n*56)+19] += A[52] * B[(l_n*84)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*84)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*84)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*56)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*56)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*56)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*56)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*56)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*56)+17], c22_5);
#else
    C[(l_n*56)+1] += A[53] * B[(l_n*84)+22];
    C[(l_n*56)+5] += A[54] * B[(l_n*84)+22];
    C[(l_n*56)+7] += A[55] * B[(l_n*84)+22];
    C[(l_n*56)+12] += A[56] * B[(l_n*84)+22];
    C[(l_n*56)+15] += A[57] * B[(l_n*84)+22];
    C[(l_n*56)+17] += A[58] * B[(l_n*84)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*84)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*84)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*56)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*56)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*56)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*56)+0] += A[59] * B[(l_n*84)+23];
    C[(l_n*56)+2] += A[60] * B[(l_n*84)+23];
    C[(l_n*56)+3] += A[61] * B[(l_n*84)+23];
    C[(l_n*56)+6] += A[62] * B[(l_n*84)+23];
    C[(l_n*56)+8] += A[63] * B[(l_n*84)+23];
    C[(l_n*56)+9] += A[64] * B[(l_n*84)+23];
    C[(l_n*56)+13] += A[65] * B[(l_n*84)+23];
    C[(l_n*56)+16] += A[66] * B[(l_n*84)+23];
    C[(l_n*56)+18] += A[67] * B[(l_n*84)+23];
    C[(l_n*56)+19] += A[68] * B[(l_n*84)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*84)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*84)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*56)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*56)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*56)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*56)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*56)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*56)+0] += A[69] * B[(l_n*84)+25];
    C[(l_n*56)+2] += A[70] * B[(l_n*84)+25];
    C[(l_n*56)+3] += A[71] * B[(l_n*84)+25];
    C[(l_n*56)+4] += A[72] * B[(l_n*84)+25];
    C[(l_n*56)+6] += A[73] * B[(l_n*84)+25];
    C[(l_n*56)+8] += A[74] * B[(l_n*84)+25];
    C[(l_n*56)+9] += A[75] * B[(l_n*84)+25];
    C[(l_n*56)+14] += A[76] * B[(l_n*84)+25];
    C[(l_n*56)+16] += A[77] * B[(l_n*84)+25];
    C[(l_n*56)+18] += A[78] * B[(l_n*84)+25];
    C[(l_n*56)+19] += A[79] * B[(l_n*84)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*84)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*84)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*56)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*56)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*56)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*56)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*56)+17], c26_4);
#else
    C[(l_n*56)+1] += A[80] * B[(l_n*84)+26];
    C[(l_n*56)+5] += A[81] * B[(l_n*84)+26];
    C[(l_n*56)+7] += A[82] * B[(l_n*84)+26];
    C[(l_n*56)+15] += A[83] * B[(l_n*84)+26];
    C[(l_n*56)+17] += A[84] * B[(l_n*84)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*84)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*84)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*56)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*56)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*56)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*56)+0] += A[85] * B[(l_n*84)+27];
    C[(l_n*56)+2] += A[86] * B[(l_n*84)+27];
    C[(l_n*56)+3] += A[87] * B[(l_n*84)+27];
    C[(l_n*56)+6] += A[88] * B[(l_n*84)+27];
    C[(l_n*56)+8] += A[89] * B[(l_n*84)+27];
    C[(l_n*56)+9] += A[90] * B[(l_n*84)+27];
    C[(l_n*56)+16] += A[91] * B[(l_n*84)+27];
    C[(l_n*56)+18] += A[92] * B[(l_n*84)+27];
    C[(l_n*56)+19] += A[93] * B[(l_n*84)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*84)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*84)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*56)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*56)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+17], c29_2);
#else
    C[(l_n*56)+1] += A[94] * B[(l_n*84)+29];
    C[(l_n*56)+7] += A[95] * B[(l_n*84)+29];
    C[(l_n*56)+17] += A[96] * B[(l_n*84)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*84)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*84)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*56)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*56)+0] += A[97] * B[(l_n*84)+30];
    C[(l_n*56)+2] += A[98] * B[(l_n*84)+30];
    C[(l_n*56)+3] += A[99] * B[(l_n*84)+30];
    C[(l_n*56)+8] += A[100] * B[(l_n*84)+30];
    C[(l_n*56)+9] += A[101] * B[(l_n*84)+30];
    C[(l_n*56)+18] += A[102] * B[(l_n*84)+30];
    C[(l_n*56)+19] += A[103] * B[(l_n*84)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*84)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*84)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*56)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*56)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*56)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*56)+19], c32_3);
#else
    C[(l_n*56)+0] += A[104] * B[(l_n*84)+32];
    C[(l_n*56)+3] += A[105] * B[(l_n*84)+32];
    C[(l_n*56)+9] += A[106] * B[(l_n*84)+32];
    C[(l_n*56)+19] += A[107] * B[(l_n*84)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*84)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*84)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a35_0 = _mm_load_ss(&A[108]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*56)+0], c35_0);
    __m128 c35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[109]));
    c35_1 = _mm_add_ps(c35_1, _mm_mul_ps(a35_1, b35));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c35_1));
    __m128 c35_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a35_3 = _mm_load_ss(&A[111]);
    c35_3 = _mm_add_ss(c35_3, _mm_mul_ss(a35_3, b35));
    _mm_store_ss(&C[(l_n*56)+4], c35_3);
    __m128 c35_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a35_4 = _mm_load_ss(&A[112]);
    c35_4 = _mm_add_ss(c35_4, _mm_mul_ss(a35_4, b35));
    _mm_store_ss(&C[(l_n*56)+6], c35_4);
    __m128 c35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[113]));
    c35_5 = _mm_add_ps(c35_5, _mm_mul_ps(a35_5, b35));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c35_5));
    __m128 c35_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a35_7 = _mm_load_ss(&A[115]);
    c35_7 = _mm_add_ss(c35_7, _mm_mul_ss(a35_7, b35));
    _mm_store_ss(&C[(l_n*56)+11], c35_7);
    __m128 c35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[116]));
    c35_8 = _mm_add_ps(c35_8, _mm_mul_ps(a35_8, b35));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c35_8));
    __m128 c35_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a35_10 = _mm_load_ss(&A[118]);
    c35_10 = _mm_add_ss(c35_10, _mm_mul_ss(a35_10, b35));
    _mm_store_ss(&C[(l_n*56)+16], c35_10);
    __m128 c35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[119]));
    c35_11 = _mm_add_ps(c35_11, _mm_mul_ps(a35_11, b35));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c35_11));
    __m128 c35_13 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a35_13 = _mm_load_ss(&A[121]);
    c35_13 = _mm_add_ss(c35_13, _mm_mul_ss(a35_13, b35));
    _mm_store_ss(&C[(l_n*56)+20], c35_13);
    __m128 c35_14 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a35_14 = _mm_load_ss(&A[122]);
    c35_14 = _mm_add_ss(c35_14, _mm_mul_ss(a35_14, b35));
    _mm_store_ss(&C[(l_n*56)+22], c35_14);
    __m128 c35_15 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a35_15 = _mm_load_ss(&A[123]);
    c35_15 = _mm_add_ss(c35_15, _mm_mul_ss(a35_15, b35));
    _mm_store_ss(&C[(l_n*56)+24], c35_15);
    __m128 c35_16 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a35_16 = _mm_load_ss(&A[124]);
    c35_16 = _mm_add_ss(c35_16, _mm_mul_ss(a35_16, b35));
    _mm_store_ss(&C[(l_n*56)+26], c35_16);
    __m128 c35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[125]));
    c35_17 = _mm_add_ps(c35_17, _mm_mul_ps(a35_17, b35));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c35_17));
    __m128 c35_19 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a35_19 = _mm_load_ss(&A[127]);
    c35_19 = _mm_add_ss(c35_19, _mm_mul_ss(a35_19, b35));
    _mm_store_ss(&C[(l_n*56)+31], c35_19);
    __m128 c35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[128]));
    c35_20 = _mm_add_ps(c35_20, _mm_mul_ps(a35_20, b35));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c35_20));
#else
    C[(l_n*56)+0] += A[108] * B[(l_n*84)+35];
    C[(l_n*56)+2] += A[109] * B[(l_n*84)+35];
    C[(l_n*56)+3] += A[110] * B[(l_n*84)+35];
    C[(l_n*56)+4] += A[111] * B[(l_n*84)+35];
    C[(l_n*56)+6] += A[112] * B[(l_n*84)+35];
    C[(l_n*56)+8] += A[113] * B[(l_n*84)+35];
    C[(l_n*56)+9] += A[114] * B[(l_n*84)+35];
    C[(l_n*56)+11] += A[115] * B[(l_n*84)+35];
    C[(l_n*56)+13] += A[116] * B[(l_n*84)+35];
    C[(l_n*56)+14] += A[117] * B[(l_n*84)+35];
    C[(l_n*56)+16] += A[118] * B[(l_n*84)+35];
    C[(l_n*56)+18] += A[119] * B[(l_n*84)+35];
    C[(l_n*56)+19] += A[120] * B[(l_n*84)+35];
    C[(l_n*56)+20] += A[121] * B[(l_n*84)+35];
    C[(l_n*56)+22] += A[122] * B[(l_n*84)+35];
    C[(l_n*56)+24] += A[123] * B[(l_n*84)+35];
    C[(l_n*56)+26] += A[124] * B[(l_n*84)+35];
    C[(l_n*56)+28] += A[125] * B[(l_n*84)+35];
    C[(l_n*56)+29] += A[126] * B[(l_n*84)+35];
    C[(l_n*56)+31] += A[127] * B[(l_n*84)+35];
    C[(l_n*56)+33] += A[128] * B[(l_n*84)+35];
    C[(l_n*56)+34] += A[129] * B[(l_n*84)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*84)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*84)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a36_0 = _mm_load_ss(&A[130]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*56)+1], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a36_1 = _mm_load_ss(&A[131]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*56)+5], c36_1);
    __m128 c36_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a36_2 = _mm_load_ss(&A[132]);
    c36_2 = _mm_add_ss(c36_2, _mm_mul_ss(a36_2, b36));
    _mm_store_ss(&C[(l_n*56)+7], c36_2);
    __m128 c36_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a36_3 = _mm_load_ss(&A[133]);
    c36_3 = _mm_add_ss(c36_3, _mm_mul_ss(a36_3, b36));
    _mm_store_ss(&C[(l_n*56)+10], c36_3);
    __m128 c36_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a36_4 = _mm_load_ss(&A[134]);
    c36_4 = _mm_add_ss(c36_4, _mm_mul_ss(a36_4, b36));
    _mm_store_ss(&C[(l_n*56)+12], c36_4);
    __m128 c36_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a36_5 = _mm_load_ss(&A[135]);
    c36_5 = _mm_add_ss(c36_5, _mm_mul_ss(a36_5, b36));
    _mm_store_ss(&C[(l_n*56)+15], c36_5);
    __m128 c36_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a36_6 = _mm_load_ss(&A[136]);
    c36_6 = _mm_add_ss(c36_6, _mm_mul_ss(a36_6, b36));
    _mm_store_ss(&C[(l_n*56)+17], c36_6);
    __m128 c36_7 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a36_7 = _mm_load_ss(&A[137]);
    c36_7 = _mm_add_ss(c36_7, _mm_mul_ss(a36_7, b36));
    _mm_store_ss(&C[(l_n*56)+21], c36_7);
    __m128 c36_8 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a36_8 = _mm_load_ss(&A[138]);
    c36_8 = _mm_add_ss(c36_8, _mm_mul_ss(a36_8, b36));
    _mm_store_ss(&C[(l_n*56)+23], c36_8);
    __m128 c36_9 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a36_9 = _mm_load_ss(&A[139]);
    c36_9 = _mm_add_ss(c36_9, _mm_mul_ss(a36_9, b36));
    _mm_store_ss(&C[(l_n*56)+25], c36_9);
    __m128 c36_10 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a36_10 = _mm_load_ss(&A[140]);
    c36_10 = _mm_add_ss(c36_10, _mm_mul_ss(a36_10, b36));
    _mm_store_ss(&C[(l_n*56)+27], c36_10);
    __m128 c36_11 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a36_11 = _mm_load_ss(&A[141]);
    c36_11 = _mm_add_ss(c36_11, _mm_mul_ss(a36_11, b36));
    _mm_store_ss(&C[(l_n*56)+30], c36_11);
    __m128 c36_12 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a36_12 = _mm_load_ss(&A[142]);
    c36_12 = _mm_add_ss(c36_12, _mm_mul_ss(a36_12, b36));
    _mm_store_ss(&C[(l_n*56)+32], c36_12);
#else
    C[(l_n*56)+1] += A[130] * B[(l_n*84)+36];
    C[(l_n*56)+5] += A[131] * B[(l_n*84)+36];
    C[(l_n*56)+7] += A[132] * B[(l_n*84)+36];
    C[(l_n*56)+10] += A[133] * B[(l_n*84)+36];
    C[(l_n*56)+12] += A[134] * B[(l_n*84)+36];
    C[(l_n*56)+15] += A[135] * B[(l_n*84)+36];
    C[(l_n*56)+17] += A[136] * B[(l_n*84)+36];
    C[(l_n*56)+21] += A[137] * B[(l_n*84)+36];
    C[(l_n*56)+23] += A[138] * B[(l_n*84)+36];
    C[(l_n*56)+25] += A[139] * B[(l_n*84)+36];
    C[(l_n*56)+27] += A[140] * B[(l_n*84)+36];
    C[(l_n*56)+30] += A[141] * B[(l_n*84)+36];
    C[(l_n*56)+32] += A[142] * B[(l_n*84)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*84)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*84)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a37_0 = _mm_load_ss(&A[143]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*56)+0], c37_0);
    __m128 c37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[144]));
    c37_1 = _mm_add_ps(c37_1, _mm_mul_ps(a37_1, b37));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c37_1));
    __m128 c37_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a37_3 = _mm_load_ss(&A[146]);
    c37_3 = _mm_add_ss(c37_3, _mm_mul_ss(a37_3, b37));
    _mm_store_ss(&C[(l_n*56)+4], c37_3);
    __m128 c37_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a37_4 = _mm_load_ss(&A[147]);
    c37_4 = _mm_add_ss(c37_4, _mm_mul_ss(a37_4, b37));
    _mm_store_ss(&C[(l_n*56)+6], c37_4);
    __m128 c37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[148]));
    c37_5 = _mm_add_ps(c37_5, _mm_mul_ps(a37_5, b37));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c37_5));
    __m128 c37_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a37_7 = _mm_load_ss(&A[150]);
    c37_7 = _mm_add_ss(c37_7, _mm_mul_ss(a37_7, b37));
    _mm_store_ss(&C[(l_n*56)+11], c37_7);
    __m128 c37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[151]));
    c37_8 = _mm_add_ps(c37_8, _mm_mul_ps(a37_8, b37));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c37_8));
    __m128 c37_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a37_10 = _mm_load_ss(&A[153]);
    c37_10 = _mm_add_ss(c37_10, _mm_mul_ss(a37_10, b37));
    _mm_store_ss(&C[(l_n*56)+16], c37_10);
    __m128 c37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[154]));
    c37_11 = _mm_add_ps(c37_11, _mm_mul_ps(a37_11, b37));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c37_11));
    __m128 c37_13 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a37_13 = _mm_load_ss(&A[156]);
    c37_13 = _mm_add_ss(c37_13, _mm_mul_ss(a37_13, b37));
    _mm_store_ss(&C[(l_n*56)+22], c37_13);
    __m128 c37_14 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a37_14 = _mm_load_ss(&A[157]);
    c37_14 = _mm_add_ss(c37_14, _mm_mul_ss(a37_14, b37));
    _mm_store_ss(&C[(l_n*56)+24], c37_14);
    __m128 c37_15 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a37_15 = _mm_load_ss(&A[158]);
    c37_15 = _mm_add_ss(c37_15, _mm_mul_ss(a37_15, b37));
    _mm_store_ss(&C[(l_n*56)+26], c37_15);
    __m128 c37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[159]));
    c37_16 = _mm_add_ps(c37_16, _mm_mul_ps(a37_16, b37));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c37_16));
    __m128 c37_18 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a37_18 = _mm_load_ss(&A[161]);
    c37_18 = _mm_add_ss(c37_18, _mm_mul_ss(a37_18, b37));
    _mm_store_ss(&C[(l_n*56)+31], c37_18);
    __m128 c37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[162]));
    c37_19 = _mm_add_ps(c37_19, _mm_mul_ps(a37_19, b37));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c37_19));
#else
    C[(l_n*56)+0] += A[143] * B[(l_n*84)+37];
    C[(l_n*56)+2] += A[144] * B[(l_n*84)+37];
    C[(l_n*56)+3] += A[145] * B[(l_n*84)+37];
    C[(l_n*56)+4] += A[146] * B[(l_n*84)+37];
    C[(l_n*56)+6] += A[147] * B[(l_n*84)+37];
    C[(l_n*56)+8] += A[148] * B[(l_n*84)+37];
    C[(l_n*56)+9] += A[149] * B[(l_n*84)+37];
    C[(l_n*56)+11] += A[150] * B[(l_n*84)+37];
    C[(l_n*56)+13] += A[151] * B[(l_n*84)+37];
    C[(l_n*56)+14] += A[152] * B[(l_n*84)+37];
    C[(l_n*56)+16] += A[153] * B[(l_n*84)+37];
    C[(l_n*56)+18] += A[154] * B[(l_n*84)+37];
    C[(l_n*56)+19] += A[155] * B[(l_n*84)+37];
    C[(l_n*56)+22] += A[156] * B[(l_n*84)+37];
    C[(l_n*56)+24] += A[157] * B[(l_n*84)+37];
    C[(l_n*56)+26] += A[158] * B[(l_n*84)+37];
    C[(l_n*56)+28] += A[159] * B[(l_n*84)+37];
    C[(l_n*56)+29] += A[160] * B[(l_n*84)+37];
    C[(l_n*56)+31] += A[161] * B[(l_n*84)+37];
    C[(l_n*56)+33] += A[162] * B[(l_n*84)+37];
    C[(l_n*56)+34] += A[163] * B[(l_n*84)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*84)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*84)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a38_0 = _mm_load_ss(&A[164]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*56)+1], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a38_1 = _mm_load_ss(&A[165]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*56)+5], c38_1);
    __m128 c38_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a38_2 = _mm_load_ss(&A[166]);
    c38_2 = _mm_add_ss(c38_2, _mm_mul_ss(a38_2, b38));
    _mm_store_ss(&C[(l_n*56)+7], c38_2);
    __m128 c38_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a38_3 = _mm_load_ss(&A[167]);
    c38_3 = _mm_add_ss(c38_3, _mm_mul_ss(a38_3, b38));
    _mm_store_ss(&C[(l_n*56)+12], c38_3);
    __m128 c38_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a38_4 = _mm_load_ss(&A[168]);
    c38_4 = _mm_add_ss(c38_4, _mm_mul_ss(a38_4, b38));
    _mm_store_ss(&C[(l_n*56)+15], c38_4);
    __m128 c38_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a38_5 = _mm_load_ss(&A[169]);
    c38_5 = _mm_add_ss(c38_5, _mm_mul_ss(a38_5, b38));
    _mm_store_ss(&C[(l_n*56)+17], c38_5);
    __m128 c38_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a38_6 = _mm_load_ss(&A[170]);
    c38_6 = _mm_add_ss(c38_6, _mm_mul_ss(a38_6, b38));
    _mm_store_ss(&C[(l_n*56)+23], c38_6);
    __m128 c38_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a38_7 = _mm_load_ss(&A[171]);
    c38_7 = _mm_add_ss(c38_7, _mm_mul_ss(a38_7, b38));
    _mm_store_ss(&C[(l_n*56)+27], c38_7);
    __m128 c38_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a38_8 = _mm_load_ss(&A[172]);
    c38_8 = _mm_add_ss(c38_8, _mm_mul_ss(a38_8, b38));
    _mm_store_ss(&C[(l_n*56)+30], c38_8);
    __m128 c38_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a38_9 = _mm_load_ss(&A[173]);
    c38_9 = _mm_add_ss(c38_9, _mm_mul_ss(a38_9, b38));
    _mm_store_ss(&C[(l_n*56)+32], c38_9);
#else
    C[(l_n*56)+1] += A[164] * B[(l_n*84)+38];
    C[(l_n*56)+5] += A[165] * B[(l_n*84)+38];
    C[(l_n*56)+7] += A[166] * B[(l_n*84)+38];
    C[(l_n*56)+12] += A[167] * B[(l_n*84)+38];
    C[(l_n*56)+15] += A[168] * B[(l_n*84)+38];
    C[(l_n*56)+17] += A[169] * B[(l_n*84)+38];
    C[(l_n*56)+23] += A[170] * B[(l_n*84)+38];
    C[(l_n*56)+27] += A[171] * B[(l_n*84)+38];
    C[(l_n*56)+30] += A[172] * B[(l_n*84)+38];
    C[(l_n*56)+32] += A[173] * B[(l_n*84)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*84)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*84)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a39_0 = _mm_load_ss(&A[174]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*56)+0], c39_0);
    __m128 c39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[175]));
    c39_1 = _mm_add_ps(c39_1, _mm_mul_ps(a39_1, b39));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c39_1));
    __m128 c39_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a39_3 = _mm_load_ss(&A[177]);
    c39_3 = _mm_add_ss(c39_3, _mm_mul_ss(a39_3, b39));
    _mm_store_ss(&C[(l_n*56)+6], c39_3);
    __m128 c39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[178]));
    c39_4 = _mm_add_ps(c39_4, _mm_mul_ps(a39_4, b39));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c39_4));
    __m128 c39_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a39_6 = _mm_load_ss(&A[180]);
    c39_6 = _mm_add_ss(c39_6, _mm_mul_ss(a39_6, b39));
    _mm_store_ss(&C[(l_n*56)+13], c39_6);
    __m128 c39_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a39_7 = _mm_load_ss(&A[181]);
    c39_7 = _mm_add_ss(c39_7, _mm_mul_ss(a39_7, b39));
    _mm_store_ss(&C[(l_n*56)+16], c39_7);
    __m128 c39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[182]));
    c39_8 = _mm_add_ps(c39_8, _mm_mul_ps(a39_8, b39));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c39_8));
    __m128 c39_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a39_10 = _mm_load_ss(&A[184]);
    c39_10 = _mm_add_ss(c39_10, _mm_mul_ss(a39_10, b39));
    _mm_store_ss(&C[(l_n*56)+24], c39_10);
    __m128 c39_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a39_11 = _mm_load_ss(&A[185]);
    c39_11 = _mm_add_ss(c39_11, _mm_mul_ss(a39_11, b39));
    _mm_store_ss(&C[(l_n*56)+28], c39_11);
    __m128 c39_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a39_12 = _mm_load_ss(&A[186]);
    c39_12 = _mm_add_ss(c39_12, _mm_mul_ss(a39_12, b39));
    _mm_store_ss(&C[(l_n*56)+31], c39_12);
    __m128 c39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[187]));
    c39_13 = _mm_add_ps(c39_13, _mm_mul_ps(a39_13, b39));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c39_13));
#else
    C[(l_n*56)+0] += A[174] * B[(l_n*84)+39];
    C[(l_n*56)+2] += A[175] * B[(l_n*84)+39];
    C[(l_n*56)+3] += A[176] * B[(l_n*84)+39];
    C[(l_n*56)+6] += A[177] * B[(l_n*84)+39];
    C[(l_n*56)+8] += A[178] * B[(l_n*84)+39];
    C[(l_n*56)+9] += A[179] * B[(l_n*84)+39];
    C[(l_n*56)+13] += A[180] * B[(l_n*84)+39];
    C[(l_n*56)+16] += A[181] * B[(l_n*84)+39];
    C[(l_n*56)+18] += A[182] * B[(l_n*84)+39];
    C[(l_n*56)+19] += A[183] * B[(l_n*84)+39];
    C[(l_n*56)+24] += A[184] * B[(l_n*84)+39];
    C[(l_n*56)+28] += A[185] * B[(l_n*84)+39];
    C[(l_n*56)+31] += A[186] * B[(l_n*84)+39];
    C[(l_n*56)+33] += A[187] * B[(l_n*84)+39];
    C[(l_n*56)+34] += A[188] * B[(l_n*84)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*84)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*84)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a41_0 = _mm_load_ss(&A[189]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*56)+1], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a41_1 = _mm_load_ss(&A[190]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*56)+5], c41_1);
    __m128 c41_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a41_2 = _mm_load_ss(&A[191]);
    c41_2 = _mm_add_ss(c41_2, _mm_mul_ss(a41_2, b41));
    _mm_store_ss(&C[(l_n*56)+7], c41_2);
    __m128 c41_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a41_3 = _mm_load_ss(&A[192]);
    c41_3 = _mm_add_ss(c41_3, _mm_mul_ss(a41_3, b41));
    _mm_store_ss(&C[(l_n*56)+10], c41_3);
    __m128 c41_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a41_4 = _mm_load_ss(&A[193]);
    c41_4 = _mm_add_ss(c41_4, _mm_mul_ss(a41_4, b41));
    _mm_store_ss(&C[(l_n*56)+12], c41_4);
    __m128 c41_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a41_5 = _mm_load_ss(&A[194]);
    c41_5 = _mm_add_ss(c41_5, _mm_mul_ss(a41_5, b41));
    _mm_store_ss(&C[(l_n*56)+15], c41_5);
    __m128 c41_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a41_6 = _mm_load_ss(&A[195]);
    c41_6 = _mm_add_ss(c41_6, _mm_mul_ss(a41_6, b41));
    _mm_store_ss(&C[(l_n*56)+17], c41_6);
    __m128 c41_7 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a41_7 = _mm_load_ss(&A[196]);
    c41_7 = _mm_add_ss(c41_7, _mm_mul_ss(a41_7, b41));
    _mm_store_ss(&C[(l_n*56)+25], c41_7);
    __m128 c41_8 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a41_8 = _mm_load_ss(&A[197]);
    c41_8 = _mm_add_ss(c41_8, _mm_mul_ss(a41_8, b41));
    _mm_store_ss(&C[(l_n*56)+27], c41_8);
    __m128 c41_9 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a41_9 = _mm_load_ss(&A[198]);
    c41_9 = _mm_add_ss(c41_9, _mm_mul_ss(a41_9, b41));
    _mm_store_ss(&C[(l_n*56)+30], c41_9);
    __m128 c41_10 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a41_10 = _mm_load_ss(&A[199]);
    c41_10 = _mm_add_ss(c41_10, _mm_mul_ss(a41_10, b41));
    _mm_store_ss(&C[(l_n*56)+32], c41_10);
#else
    C[(l_n*56)+1] += A[189] * B[(l_n*84)+41];
    C[(l_n*56)+5] += A[190] * B[(l_n*84)+41];
    C[(l_n*56)+7] += A[191] * B[(l_n*84)+41];
    C[(l_n*56)+10] += A[192] * B[(l_n*84)+41];
    C[(l_n*56)+12] += A[193] * B[(l_n*84)+41];
    C[(l_n*56)+15] += A[194] * B[(l_n*84)+41];
    C[(l_n*56)+17] += A[195] * B[(l_n*84)+41];
    C[(l_n*56)+25] += A[196] * B[(l_n*84)+41];
    C[(l_n*56)+27] += A[197] * B[(l_n*84)+41];
    C[(l_n*56)+30] += A[198] * B[(l_n*84)+41];
    C[(l_n*56)+32] += A[199] * B[(l_n*84)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*84)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*84)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a42_0 = _mm_load_ss(&A[200]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*56)+0], c42_0);
    __m128 c42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[201]));
    c42_1 = _mm_add_ps(c42_1, _mm_mul_ps(a42_1, b42));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c42_1));
    __m128 c42_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a42_3 = _mm_load_ss(&A[203]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*56)+4], c42_3);
    __m128 c42_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a42_4 = _mm_load_ss(&A[204]);
    c42_4 = _mm_add_ss(c42_4, _mm_mul_ss(a42_4, b42));
    _mm_store_ss(&C[(l_n*56)+6], c42_4);
    __m128 c42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[205]));
    c42_5 = _mm_add_ps(c42_5, _mm_mul_ps(a42_5, b42));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c42_5));
    __m128 c42_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a42_7 = _mm_load_ss(&A[207]);
    c42_7 = _mm_add_ss(c42_7, _mm_mul_ss(a42_7, b42));
    _mm_store_ss(&C[(l_n*56)+11], c42_7);
    __m128 c42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[208]));
    c42_8 = _mm_add_ps(c42_8, _mm_mul_ps(a42_8, b42));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c42_8));
    __m128 c42_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a42_10 = _mm_load_ss(&A[210]);
    c42_10 = _mm_add_ss(c42_10, _mm_mul_ss(a42_10, b42));
    _mm_store_ss(&C[(l_n*56)+16], c42_10);
    __m128 c42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[211]));
    c42_11 = _mm_add_ps(c42_11, _mm_mul_ps(a42_11, b42));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c42_11));
    __m128 c42_13 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a42_13 = _mm_load_ss(&A[213]);
    c42_13 = _mm_add_ss(c42_13, _mm_mul_ss(a42_13, b42));
    _mm_store_ss(&C[(l_n*56)+26], c42_13);
    __m128 c42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&A[214]));
    c42_14 = _mm_add_ps(c42_14, _mm_mul_ps(a42_14, b42));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c42_14));
    __m128 c42_16 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a42_16 = _mm_load_ss(&A[216]);
    c42_16 = _mm_add_ss(c42_16, _mm_mul_ss(a42_16, b42));
    _mm_store_ss(&C[(l_n*56)+31], c42_16);
    __m128 c42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[217]));
    c42_17 = _mm_add_ps(c42_17, _mm_mul_ps(a42_17, b42));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c42_17));
#else
    C[(l_n*56)+0] += A[200] * B[(l_n*84)+42];
    C[(l_n*56)+2] += A[201] * B[(l_n*84)+42];
    C[(l_n*56)+3] += A[202] * B[(l_n*84)+42];
    C[(l_n*56)+4] += A[203] * B[(l_n*84)+42];
    C[(l_n*56)+6] += A[204] * B[(l_n*84)+42];
    C[(l_n*56)+8] += A[205] * B[(l_n*84)+42];
    C[(l_n*56)+9] += A[206] * B[(l_n*84)+42];
    C[(l_n*56)+11] += A[207] * B[(l_n*84)+42];
    C[(l_n*56)+13] += A[208] * B[(l_n*84)+42];
    C[(l_n*56)+14] += A[209] * B[(l_n*84)+42];
    C[(l_n*56)+16] += A[210] * B[(l_n*84)+42];
    C[(l_n*56)+18] += A[211] * B[(l_n*84)+42];
    C[(l_n*56)+19] += A[212] * B[(l_n*84)+42];
    C[(l_n*56)+26] += A[213] * B[(l_n*84)+42];
    C[(l_n*56)+28] += A[214] * B[(l_n*84)+42];
    C[(l_n*56)+29] += A[215] * B[(l_n*84)+42];
    C[(l_n*56)+31] += A[216] * B[(l_n*84)+42];
    C[(l_n*56)+33] += A[217] * B[(l_n*84)+42];
    C[(l_n*56)+34] += A[218] * B[(l_n*84)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*84)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*84)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a43_0 = _mm_load_ss(&A[219]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*56)+1], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a43_1 = _mm_load_ss(&A[220]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*56)+5], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a43_2 = _mm_load_ss(&A[221]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*56)+7], c43_2);
    __m128 c43_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a43_3 = _mm_load_ss(&A[222]);
    c43_3 = _mm_add_ss(c43_3, _mm_mul_ss(a43_3, b43));
    _mm_store_ss(&C[(l_n*56)+12], c43_3);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a43_4 = _mm_load_ss(&A[223]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*56)+15], c43_4);
    __m128 c43_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a43_5 = _mm_load_ss(&A[224]);
    c43_5 = _mm_add_ss(c43_5, _mm_mul_ss(a43_5, b43));
    _mm_store_ss(&C[(l_n*56)+17], c43_5);
    __m128 c43_6 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a43_6 = _mm_load_ss(&A[225]);
    c43_6 = _mm_add_ss(c43_6, _mm_mul_ss(a43_6, b43));
    _mm_store_ss(&C[(l_n*56)+27], c43_6);
    __m128 c43_7 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a43_7 = _mm_load_ss(&A[226]);
    c43_7 = _mm_add_ss(c43_7, _mm_mul_ss(a43_7, b43));
    _mm_store_ss(&C[(l_n*56)+30], c43_7);
    __m128 c43_8 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a43_8 = _mm_load_ss(&A[227]);
    c43_8 = _mm_add_ss(c43_8, _mm_mul_ss(a43_8, b43));
    _mm_store_ss(&C[(l_n*56)+32], c43_8);
#else
    C[(l_n*56)+1] += A[219] * B[(l_n*84)+43];
    C[(l_n*56)+5] += A[220] * B[(l_n*84)+43];
    C[(l_n*56)+7] += A[221] * B[(l_n*84)+43];
    C[(l_n*56)+12] += A[222] * B[(l_n*84)+43];
    C[(l_n*56)+15] += A[223] * B[(l_n*84)+43];
    C[(l_n*56)+17] += A[224] * B[(l_n*84)+43];
    C[(l_n*56)+27] += A[225] * B[(l_n*84)+43];
    C[(l_n*56)+30] += A[226] * B[(l_n*84)+43];
    C[(l_n*56)+32] += A[227] * B[(l_n*84)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*84)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*84)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a44_0 = _mm_load_ss(&A[228]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+0], c44_0);
    __m128 c44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[229]));
    c44_1 = _mm_add_ps(c44_1, _mm_mul_ps(a44_1, b44));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c44_1));
    __m128 c44_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a44_3 = _mm_load_ss(&A[231]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*56)+6], c44_3);
    __m128 c44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[232]));
    c44_4 = _mm_add_ps(c44_4, _mm_mul_ps(a44_4, b44));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c44_4));
    __m128 c44_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a44_6 = _mm_load_ss(&A[234]);
    c44_6 = _mm_add_ss(c44_6, _mm_mul_ss(a44_6, b44));
    _mm_store_ss(&C[(l_n*56)+13], c44_6);
    __m128 c44_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a44_7 = _mm_load_ss(&A[235]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*56)+16], c44_7);
    __m128 c44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[236]));
    c44_8 = _mm_add_ps(c44_8, _mm_mul_ps(a44_8, b44));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c44_8));
    __m128 c44_10 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a44_10 = _mm_load_ss(&A[238]);
    c44_10 = _mm_add_ss(c44_10, _mm_mul_ss(a44_10, b44));
    _mm_store_ss(&C[(l_n*56)+28], c44_10);
    __m128 c44_11 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a44_11 = _mm_load_ss(&A[239]);
    c44_11 = _mm_add_ss(c44_11, _mm_mul_ss(a44_11, b44));
    _mm_store_ss(&C[(l_n*56)+31], c44_11);
    __m128 c44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[240]));
    c44_12 = _mm_add_ps(c44_12, _mm_mul_ps(a44_12, b44));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c44_12));
#else
    C[(l_n*56)+0] += A[228] * B[(l_n*84)+44];
    C[(l_n*56)+2] += A[229] * B[(l_n*84)+44];
    C[(l_n*56)+3] += A[230] * B[(l_n*84)+44];
    C[(l_n*56)+6] += A[231] * B[(l_n*84)+44];
    C[(l_n*56)+8] += A[232] * B[(l_n*84)+44];
    C[(l_n*56)+9] += A[233] * B[(l_n*84)+44];
    C[(l_n*56)+13] += A[234] * B[(l_n*84)+44];
    C[(l_n*56)+16] += A[235] * B[(l_n*84)+44];
    C[(l_n*56)+18] += A[236] * B[(l_n*84)+44];
    C[(l_n*56)+19] += A[237] * B[(l_n*84)+44];
    C[(l_n*56)+28] += A[238] * B[(l_n*84)+44];
    C[(l_n*56)+31] += A[239] * B[(l_n*84)+44];
    C[(l_n*56)+33] += A[240] * B[(l_n*84)+44];
    C[(l_n*56)+34] += A[241] * B[(l_n*84)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*84)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*84)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a46_0 = _mm_load_ss(&A[242]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*56)+0], c46_0);
    __m128 c46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[243]));
    c46_1 = _mm_add_ps(c46_1, _mm_mul_ps(a46_1, b46));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c46_1));
    __m128 c46_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a46_3 = _mm_load_ss(&A[245]);
    c46_3 = _mm_add_ss(c46_3, _mm_mul_ss(a46_3, b46));
    _mm_store_ss(&C[(l_n*56)+4], c46_3);
    __m128 c46_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a46_4 = _mm_load_ss(&A[246]);
    c46_4 = _mm_add_ss(c46_4, _mm_mul_ss(a46_4, b46));
    _mm_store_ss(&C[(l_n*56)+6], c46_4);
    __m128 c46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[247]));
    c46_5 = _mm_add_ps(c46_5, _mm_mul_ps(a46_5, b46));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c46_5));
    __m128 c46_7 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a46_7 = _mm_load_ss(&A[249]);
    c46_7 = _mm_add_ss(c46_7, _mm_mul_ss(a46_7, b46));
    _mm_store_ss(&C[(l_n*56)+14], c46_7);
    __m128 c46_8 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a46_8 = _mm_load_ss(&A[250]);
    c46_8 = _mm_add_ss(c46_8, _mm_mul_ss(a46_8, b46));
    _mm_store_ss(&C[(l_n*56)+16], c46_8);
    __m128 c46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[251]));
    c46_9 = _mm_add_ps(c46_9, _mm_mul_ps(a46_9, b46));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c46_9));
    __m128 c46_11 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a46_11 = _mm_load_ss(&A[253]);
    c46_11 = _mm_add_ss(c46_11, _mm_mul_ss(a46_11, b46));
    _mm_store_ss(&C[(l_n*56)+29], c46_11);
    __m128 c46_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a46_12 = _mm_load_ss(&A[254]);
    c46_12 = _mm_add_ss(c46_12, _mm_mul_ss(a46_12, b46));
    _mm_store_ss(&C[(l_n*56)+31], c46_12);
    __m128 c46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[255]));
    c46_13 = _mm_add_ps(c46_13, _mm_mul_ps(a46_13, b46));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c46_13));
#else
    C[(l_n*56)+0] += A[242] * B[(l_n*84)+46];
    C[(l_n*56)+2] += A[243] * B[(l_n*84)+46];
    C[(l_n*56)+3] += A[244] * B[(l_n*84)+46];
    C[(l_n*56)+4] += A[245] * B[(l_n*84)+46];
    C[(l_n*56)+6] += A[246] * B[(l_n*84)+46];
    C[(l_n*56)+8] += A[247] * B[(l_n*84)+46];
    C[(l_n*56)+9] += A[248] * B[(l_n*84)+46];
    C[(l_n*56)+14] += A[249] * B[(l_n*84)+46];
    C[(l_n*56)+16] += A[250] * B[(l_n*84)+46];
    C[(l_n*56)+18] += A[251] * B[(l_n*84)+46];
    C[(l_n*56)+19] += A[252] * B[(l_n*84)+46];
    C[(l_n*56)+29] += A[253] * B[(l_n*84)+46];
    C[(l_n*56)+31] += A[254] * B[(l_n*84)+46];
    C[(l_n*56)+33] += A[255] * B[(l_n*84)+46];
    C[(l_n*56)+34] += A[256] * B[(l_n*84)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*84)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*84)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a47_0 = _mm_load_ss(&A[257]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*56)+1], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a47_1 = _mm_load_ss(&A[258]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*56)+5], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a47_2 = _mm_load_ss(&A[259]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*56)+7], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a47_3 = _mm_load_ss(&A[260]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*56)+15], c47_3);
    __m128 c47_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a47_4 = _mm_load_ss(&A[261]);
    c47_4 = _mm_add_ss(c47_4, _mm_mul_ss(a47_4, b47));
    _mm_store_ss(&C[(l_n*56)+17], c47_4);
    __m128 c47_5 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a47_5 = _mm_load_ss(&A[262]);
    c47_5 = _mm_add_ss(c47_5, _mm_mul_ss(a47_5, b47));
    _mm_store_ss(&C[(l_n*56)+30], c47_5);
    __m128 c47_6 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a47_6 = _mm_load_ss(&A[263]);
    c47_6 = _mm_add_ss(c47_6, _mm_mul_ss(a47_6, b47));
    _mm_store_ss(&C[(l_n*56)+32], c47_6);
#else
    C[(l_n*56)+1] += A[257] * B[(l_n*84)+47];
    C[(l_n*56)+5] += A[258] * B[(l_n*84)+47];
    C[(l_n*56)+7] += A[259] * B[(l_n*84)+47];
    C[(l_n*56)+15] += A[260] * B[(l_n*84)+47];
    C[(l_n*56)+17] += A[261] * B[(l_n*84)+47];
    C[(l_n*56)+30] += A[262] * B[(l_n*84)+47];
    C[(l_n*56)+32] += A[263] * B[(l_n*84)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*84)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*84)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a48_0 = _mm_load_ss(&A[264]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*56)+0], c48_0);
    __m128 c48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[265]));
    c48_1 = _mm_add_ps(c48_1, _mm_mul_ps(a48_1, b48));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c48_1));
    __m128 c48_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a48_3 = _mm_load_ss(&A[267]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*56)+6], c48_3);
    __m128 c48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[268]));
    c48_4 = _mm_add_ps(c48_4, _mm_mul_ps(a48_4, b48));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c48_4));
    __m128 c48_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a48_6 = _mm_load_ss(&A[270]);
    c48_6 = _mm_add_ss(c48_6, _mm_mul_ss(a48_6, b48));
    _mm_store_ss(&C[(l_n*56)+16], c48_6);
    __m128 c48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[271]));
    c48_7 = _mm_add_ps(c48_7, _mm_mul_ps(a48_7, b48));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c48_7));
    __m128 c48_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a48_9 = _mm_load_ss(&A[273]);
    c48_9 = _mm_add_ss(c48_9, _mm_mul_ss(a48_9, b48));
    _mm_store_ss(&C[(l_n*56)+31], c48_9);
    __m128 c48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[274]));
    c48_10 = _mm_add_ps(c48_10, _mm_mul_ps(a48_10, b48));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c48_10));
#else
    C[(l_n*56)+0] += A[264] * B[(l_n*84)+48];
    C[(l_n*56)+2] += A[265] * B[(l_n*84)+48];
    C[(l_n*56)+3] += A[266] * B[(l_n*84)+48];
    C[(l_n*56)+6] += A[267] * B[(l_n*84)+48];
    C[(l_n*56)+8] += A[268] * B[(l_n*84)+48];
    C[(l_n*56)+9] += A[269] * B[(l_n*84)+48];
    C[(l_n*56)+16] += A[270] * B[(l_n*84)+48];
    C[(l_n*56)+18] += A[271] * B[(l_n*84)+48];
    C[(l_n*56)+19] += A[272] * B[(l_n*84)+48];
    C[(l_n*56)+31] += A[273] * B[(l_n*84)+48];
    C[(l_n*56)+33] += A[274] * B[(l_n*84)+48];
    C[(l_n*56)+34] += A[275] * B[(l_n*84)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*84)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*84)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a50_0 = _mm_load_ss(&A[276]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*56)+1], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a50_1 = _mm_load_ss(&A[277]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*56)+7], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a50_2 = _mm_load_ss(&A[278]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+17], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a50_3 = _mm_load_ss(&A[279]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*56)+32], c50_3);
#else
    C[(l_n*56)+1] += A[276] * B[(l_n*84)+50];
    C[(l_n*56)+7] += A[277] * B[(l_n*84)+50];
    C[(l_n*56)+17] += A[278] * B[(l_n*84)+50];
    C[(l_n*56)+32] += A[279] * B[(l_n*84)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*84)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*84)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a51_0 = _mm_load_ss(&A[280]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*56)+0], c51_0);
    __m128 c51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[281]));
    c51_1 = _mm_add_ps(c51_1, _mm_mul_ps(a51_1, b51));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c51_1));
    __m128 c51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[283]));
    c51_3 = _mm_add_ps(c51_3, _mm_mul_ps(a51_3, b51));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c51_3));
    __m128 c51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[285]));
    c51_5 = _mm_add_ps(c51_5, _mm_mul_ps(a51_5, b51));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c51_5));
    __m128 c51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[287]));
    c51_7 = _mm_add_ps(c51_7, _mm_mul_ps(a51_7, b51));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c51_7));
#else
    C[(l_n*56)+0] += A[280] * B[(l_n*84)+51];
    C[(l_n*56)+2] += A[281] * B[(l_n*84)+51];
    C[(l_n*56)+3] += A[282] * B[(l_n*84)+51];
    C[(l_n*56)+8] += A[283] * B[(l_n*84)+51];
    C[(l_n*56)+9] += A[284] * B[(l_n*84)+51];
    C[(l_n*56)+18] += A[285] * B[(l_n*84)+51];
    C[(l_n*56)+19] += A[286] * B[(l_n*84)+51];
    C[(l_n*56)+33] += A[287] * B[(l_n*84)+51];
    C[(l_n*56)+34] += A[288] * B[(l_n*84)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*84)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*84)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a53_0 = _mm_load_ss(&A[289]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*56)+0], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a53_1 = _mm_load_ss(&A[290]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*56)+3], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a53_2 = _mm_load_ss(&A[291]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*56)+9], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a53_3 = _mm_load_ss(&A[292]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*56)+19], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a53_4 = _mm_load_ss(&A[293]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*56)+34], c53_4);
#else
    C[(l_n*56)+0] += A[289] * B[(l_n*84)+53];
    C[(l_n*56)+3] += A[290] * B[(l_n*84)+53];
    C[(l_n*56)+9] += A[291] * B[(l_n*84)+53];
    C[(l_n*56)+19] += A[292] * B[(l_n*84)+53];
    C[(l_n*56)+34] += A[293] * B[(l_n*84)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b56 = _mm_broadcast_ss(&B[(l_n*84)+56]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b56 = _mm_load_ss(&B[(l_n*84)+56]);    b56 = _mm_shuffle_ps(b56, b56, 0x00);
#endif
    __m128 c56_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a56_0 = _mm_load_ss(&A[294]);
    c56_0 = _mm_add_ss(c56_0, _mm_mul_ss(a56_0, b56));
    _mm_store_ss(&C[(l_n*56)+1], c56_0);
    __m128 c56_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a56_1 = _mm_load_ss(&A[295]);
    c56_1 = _mm_add_ss(c56_1, _mm_mul_ss(a56_1, b56));
    _mm_store_ss(&C[(l_n*56)+5], c56_1);
    __m128 c56_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a56_2 = _mm_load_ss(&A[296]);
    c56_2 = _mm_add_ss(c56_2, _mm_mul_ss(a56_2, b56));
    _mm_store_ss(&C[(l_n*56)+7], c56_2);
    __m128 c56_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a56_3 = _mm_load_ss(&A[297]);
    c56_3 = _mm_add_ss(c56_3, _mm_mul_ss(a56_3, b56));
    _mm_store_ss(&C[(l_n*56)+10], c56_3);
    __m128 c56_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a56_4 = _mm_load_ss(&A[298]);
    c56_4 = _mm_add_ss(c56_4, _mm_mul_ss(a56_4, b56));
    _mm_store_ss(&C[(l_n*56)+12], c56_4);
    __m128 c56_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a56_5 = _mm_load_ss(&A[299]);
    c56_5 = _mm_add_ss(c56_5, _mm_mul_ss(a56_5, b56));
    _mm_store_ss(&C[(l_n*56)+15], c56_5);
    __m128 c56_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a56_6 = _mm_load_ss(&A[300]);
    c56_6 = _mm_add_ss(c56_6, _mm_mul_ss(a56_6, b56));
    _mm_store_ss(&C[(l_n*56)+17], c56_6);
    __m128 c56_7 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a56_7 = _mm_load_ss(&A[301]);
    c56_7 = _mm_add_ss(c56_7, _mm_mul_ss(a56_7, b56));
    _mm_store_ss(&C[(l_n*56)+21], c56_7);
    __m128 c56_8 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a56_8 = _mm_load_ss(&A[302]);
    c56_8 = _mm_add_ss(c56_8, _mm_mul_ss(a56_8, b56));
    _mm_store_ss(&C[(l_n*56)+23], c56_8);
    __m128 c56_9 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a56_9 = _mm_load_ss(&A[303]);
    c56_9 = _mm_add_ss(c56_9, _mm_mul_ss(a56_9, b56));
    _mm_store_ss(&C[(l_n*56)+25], c56_9);
    __m128 c56_10 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a56_10 = _mm_load_ss(&A[304]);
    c56_10 = _mm_add_ss(c56_10, _mm_mul_ss(a56_10, b56));
    _mm_store_ss(&C[(l_n*56)+27], c56_10);
    __m128 c56_11 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a56_11 = _mm_load_ss(&A[305]);
    c56_11 = _mm_add_ss(c56_11, _mm_mul_ss(a56_11, b56));
    _mm_store_ss(&C[(l_n*56)+30], c56_11);
    __m128 c56_12 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a56_12 = _mm_load_ss(&A[306]);
    c56_12 = _mm_add_ss(c56_12, _mm_mul_ss(a56_12, b56));
    _mm_store_ss(&C[(l_n*56)+32], c56_12);
    __m128 c56_13 = _mm_load_ss(&C[(l_n*56)+35]);
    __m128 a56_13 = _mm_load_ss(&A[307]);
    c56_13 = _mm_add_ss(c56_13, _mm_mul_ss(a56_13, b56));
    _mm_store_ss(&C[(l_n*56)+35], c56_13);
    __m128 c56_14 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a56_14 = _mm_load_ss(&A[308]);
    c56_14 = _mm_add_ss(c56_14, _mm_mul_ss(a56_14, b56));
    _mm_store_ss(&C[(l_n*56)+37], c56_14);
    __m128 c56_15 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a56_15 = _mm_load_ss(&A[309]);
    c56_15 = _mm_add_ss(c56_15, _mm_mul_ss(a56_15, b56));
    _mm_store_ss(&C[(l_n*56)+39], c56_15);
    __m128 c56_16 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a56_16 = _mm_load_ss(&A[310]);
    c56_16 = _mm_add_ss(c56_16, _mm_mul_ss(a56_16, b56));
    _mm_store_ss(&C[(l_n*56)+42], c56_16);
    __m128 c56_17 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a56_17 = _mm_load_ss(&A[311]);
    c56_17 = _mm_add_ss(c56_17, _mm_mul_ss(a56_17, b56));
    _mm_store_ss(&C[(l_n*56)+44], c56_17);
    __m128 c56_18 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a56_18 = _mm_load_ss(&A[312]);
    c56_18 = _mm_add_ss(c56_18, _mm_mul_ss(a56_18, b56));
    _mm_store_ss(&C[(l_n*56)+46], c56_18);
    __m128 c56_19 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a56_19 = _mm_load_ss(&A[313]);
    c56_19 = _mm_add_ss(c56_19, _mm_mul_ss(a56_19, b56));
    _mm_store_ss(&C[(l_n*56)+48], c56_19);
    __m128 c56_20 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a56_20 = _mm_load_ss(&A[314]);
    c56_20 = _mm_add_ss(c56_20, _mm_mul_ss(a56_20, b56));
    _mm_store_ss(&C[(l_n*56)+51], c56_20);
    __m128 c56_21 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a56_21 = _mm_load_ss(&A[315]);
    c56_21 = _mm_add_ss(c56_21, _mm_mul_ss(a56_21, b56));
    _mm_store_ss(&C[(l_n*56)+53], c56_21);
#else
    C[(l_n*56)+1] += A[294] * B[(l_n*84)+56];
    C[(l_n*56)+5] += A[295] * B[(l_n*84)+56];
    C[(l_n*56)+7] += A[296] * B[(l_n*84)+56];
    C[(l_n*56)+10] += A[297] * B[(l_n*84)+56];
    C[(l_n*56)+12] += A[298] * B[(l_n*84)+56];
    C[(l_n*56)+15] += A[299] * B[(l_n*84)+56];
    C[(l_n*56)+17] += A[300] * B[(l_n*84)+56];
    C[(l_n*56)+21] += A[301] * B[(l_n*84)+56];
    C[(l_n*56)+23] += A[302] * B[(l_n*84)+56];
    C[(l_n*56)+25] += A[303] * B[(l_n*84)+56];
    C[(l_n*56)+27] += A[304] * B[(l_n*84)+56];
    C[(l_n*56)+30] += A[305] * B[(l_n*84)+56];
    C[(l_n*56)+32] += A[306] * B[(l_n*84)+56];
    C[(l_n*56)+35] += A[307] * B[(l_n*84)+56];
    C[(l_n*56)+37] += A[308] * B[(l_n*84)+56];
    C[(l_n*56)+39] += A[309] * B[(l_n*84)+56];
    C[(l_n*56)+42] += A[310] * B[(l_n*84)+56];
    C[(l_n*56)+44] += A[311] * B[(l_n*84)+56];
    C[(l_n*56)+46] += A[312] * B[(l_n*84)+56];
    C[(l_n*56)+48] += A[313] * B[(l_n*84)+56];
    C[(l_n*56)+51] += A[314] * B[(l_n*84)+56];
    C[(l_n*56)+53] += A[315] * B[(l_n*84)+56];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b57 = _mm_broadcast_ss(&B[(l_n*84)+57]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b57 = _mm_load_ss(&B[(l_n*84)+57]);    b57 = _mm_shuffle_ps(b57, b57, 0x00);
#endif
    __m128 c57_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a57_0 = _mm_load_ss(&A[316]);
    c57_0 = _mm_add_ss(c57_0, _mm_mul_ss(a57_0, b57));
    _mm_store_ss(&C[(l_n*56)+0], c57_0);
    __m128 c57_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a57_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[317]));
    c57_1 = _mm_add_ps(c57_1, _mm_mul_ps(a57_1, b57));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c57_1));
    __m128 c57_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a57_3 = _mm_load_ss(&A[319]);
    c57_3 = _mm_add_ss(c57_3, _mm_mul_ss(a57_3, b57));
    _mm_store_ss(&C[(l_n*56)+4], c57_3);
    __m128 c57_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a57_4 = _mm_load_ss(&A[320]);
    c57_4 = _mm_add_ss(c57_4, _mm_mul_ss(a57_4, b57));
    _mm_store_ss(&C[(l_n*56)+6], c57_4);
    __m128 c57_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a57_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[321]));
    c57_5 = _mm_add_ps(c57_5, _mm_mul_ps(a57_5, b57));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c57_5));
    __m128 c57_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a57_7 = _mm_load_ss(&A[323]);
    c57_7 = _mm_add_ss(c57_7, _mm_mul_ss(a57_7, b57));
    _mm_store_ss(&C[(l_n*56)+11], c57_7);
    __m128 c57_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a57_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[324]));
    c57_8 = _mm_add_ps(c57_8, _mm_mul_ps(a57_8, b57));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c57_8));
    __m128 c57_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a57_10 = _mm_load_ss(&A[326]);
    c57_10 = _mm_add_ss(c57_10, _mm_mul_ss(a57_10, b57));
    _mm_store_ss(&C[(l_n*56)+16], c57_10);
    __m128 c57_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a57_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[327]));
    c57_11 = _mm_add_ps(c57_11, _mm_mul_ps(a57_11, b57));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c57_11));
    __m128 c57_13 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a57_13 = _mm_load_ss(&A[329]);
    c57_13 = _mm_add_ss(c57_13, _mm_mul_ss(a57_13, b57));
    _mm_store_ss(&C[(l_n*56)+20], c57_13);
    __m128 c57_14 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a57_14 = _mm_load_ss(&A[330]);
    c57_14 = _mm_add_ss(c57_14, _mm_mul_ss(a57_14, b57));
    _mm_store_ss(&C[(l_n*56)+22], c57_14);
    __m128 c57_15 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a57_15 = _mm_load_ss(&A[331]);
    c57_15 = _mm_add_ss(c57_15, _mm_mul_ss(a57_15, b57));
    _mm_store_ss(&C[(l_n*56)+24], c57_15);
    __m128 c57_16 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a57_16 = _mm_load_ss(&A[332]);
    c57_16 = _mm_add_ss(c57_16, _mm_mul_ss(a57_16, b57));
    _mm_store_ss(&C[(l_n*56)+26], c57_16);
    __m128 c57_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a57_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[333]));
    c57_17 = _mm_add_ps(c57_17, _mm_mul_ps(a57_17, b57));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c57_17));
    __m128 c57_19 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a57_19 = _mm_load_ss(&A[335]);
    c57_19 = _mm_add_ss(c57_19, _mm_mul_ss(a57_19, b57));
    _mm_store_ss(&C[(l_n*56)+31], c57_19);
    __m128 c57_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a57_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[336]));
    c57_20 = _mm_add_ps(c57_20, _mm_mul_ps(a57_20, b57));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c57_20));
    __m128 c57_22 = _mm_load_ss(&C[(l_n*56)+36]);
    __m128 a57_22 = _mm_load_ss(&A[338]);
    c57_22 = _mm_add_ss(c57_22, _mm_mul_ss(a57_22, b57));
    _mm_store_ss(&C[(l_n*56)+36], c57_22);
    __m128 c57_23 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a57_23 = _mm_load_ss(&A[339]);
    c57_23 = _mm_add_ss(c57_23, _mm_mul_ss(a57_23, b57));
    _mm_store_ss(&C[(l_n*56)+38], c57_23);
    __m128 c57_24 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+40]));
    __m128 a57_24 = _mm_castpd_ps(_mm_load_sd((const double*)&A[340]));
    c57_24 = _mm_add_ps(c57_24, _mm_mul_ps(a57_24, b57));
    _mm_store_sd((double*)&C[(l_n*56)+40], _mm_castps_pd(c57_24));
    __m128 c57_26 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a57_26 = _mm_load_ss(&A[342]);
    c57_26 = _mm_add_ss(c57_26, _mm_mul_ss(a57_26, b57));
    _mm_store_ss(&C[(l_n*56)+43], c57_26);
    __m128 c57_27 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a57_27 = _mm_load_ss(&A[343]);
    c57_27 = _mm_add_ss(c57_27, _mm_mul_ss(a57_27, b57));
    _mm_store_ss(&C[(l_n*56)+45], c57_27);
    __m128 c57_28 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a57_28 = _mm_load_ss(&A[344]);
    c57_28 = _mm_add_ss(c57_28, _mm_mul_ss(a57_28, b57));
    _mm_store_ss(&C[(l_n*56)+47], c57_28);
    __m128 c57_29 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+49]));
    __m128 a57_29 = _mm_castpd_ps(_mm_load_sd((const double*)&A[345]));
    c57_29 = _mm_add_ps(c57_29, _mm_mul_ps(a57_29, b57));
    _mm_store_sd((double*)&C[(l_n*56)+49], _mm_castps_pd(c57_29));
    __m128 c57_31 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a57_31 = _mm_load_ss(&A[347]);
    c57_31 = _mm_add_ss(c57_31, _mm_mul_ss(a57_31, b57));
    _mm_store_ss(&C[(l_n*56)+52], c57_31);
    __m128 c57_32 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a57_32 = _mm_castpd_ps(_mm_load_sd((const double*)&A[348]));
    c57_32 = _mm_add_ps(c57_32, _mm_mul_ps(a57_32, b57));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c57_32));
#else
    C[(l_n*56)+0] += A[316] * B[(l_n*84)+57];
    C[(l_n*56)+2] += A[317] * B[(l_n*84)+57];
    C[(l_n*56)+3] += A[318] * B[(l_n*84)+57];
    C[(l_n*56)+4] += A[319] * B[(l_n*84)+57];
    C[(l_n*56)+6] += A[320] * B[(l_n*84)+57];
    C[(l_n*56)+8] += A[321] * B[(l_n*84)+57];
    C[(l_n*56)+9] += A[322] * B[(l_n*84)+57];
    C[(l_n*56)+11] += A[323] * B[(l_n*84)+57];
    C[(l_n*56)+13] += A[324] * B[(l_n*84)+57];
    C[(l_n*56)+14] += A[325] * B[(l_n*84)+57];
    C[(l_n*56)+16] += A[326] * B[(l_n*84)+57];
    C[(l_n*56)+18] += A[327] * B[(l_n*84)+57];
    C[(l_n*56)+19] += A[328] * B[(l_n*84)+57];
    C[(l_n*56)+20] += A[329] * B[(l_n*84)+57];
    C[(l_n*56)+22] += A[330] * B[(l_n*84)+57];
    C[(l_n*56)+24] += A[331] * B[(l_n*84)+57];
    C[(l_n*56)+26] += A[332] * B[(l_n*84)+57];
    C[(l_n*56)+28] += A[333] * B[(l_n*84)+57];
    C[(l_n*56)+29] += A[334] * B[(l_n*84)+57];
    C[(l_n*56)+31] += A[335] * B[(l_n*84)+57];
    C[(l_n*56)+33] += A[336] * B[(l_n*84)+57];
    C[(l_n*56)+34] += A[337] * B[(l_n*84)+57];
    C[(l_n*56)+36] += A[338] * B[(l_n*84)+57];
    C[(l_n*56)+38] += A[339] * B[(l_n*84)+57];
    C[(l_n*56)+40] += A[340] * B[(l_n*84)+57];
    C[(l_n*56)+41] += A[341] * B[(l_n*84)+57];
    C[(l_n*56)+43] += A[342] * B[(l_n*84)+57];
    C[(l_n*56)+45] += A[343] * B[(l_n*84)+57];
    C[(l_n*56)+47] += A[344] * B[(l_n*84)+57];
    C[(l_n*56)+49] += A[345] * B[(l_n*84)+57];
    C[(l_n*56)+50] += A[346] * B[(l_n*84)+57];
    C[(l_n*56)+52] += A[347] * B[(l_n*84)+57];
    C[(l_n*56)+54] += A[348] * B[(l_n*84)+57];
    C[(l_n*56)+55] += A[349] * B[(l_n*84)+57];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b58 = _mm_broadcast_ss(&B[(l_n*84)+58]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b58 = _mm_load_ss(&B[(l_n*84)+58]);    b58 = _mm_shuffle_ps(b58, b58, 0x00);
#endif
    __m128 c58_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a58_0 = _mm_load_ss(&A[350]);
    c58_0 = _mm_add_ss(c58_0, _mm_mul_ss(a58_0, b58));
    _mm_store_ss(&C[(l_n*56)+1], c58_0);
    __m128 c58_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a58_1 = _mm_load_ss(&A[351]);
    c58_1 = _mm_add_ss(c58_1, _mm_mul_ss(a58_1, b58));
    _mm_store_ss(&C[(l_n*56)+5], c58_1);
    __m128 c58_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a58_2 = _mm_load_ss(&A[352]);
    c58_2 = _mm_add_ss(c58_2, _mm_mul_ss(a58_2, b58));
    _mm_store_ss(&C[(l_n*56)+7], c58_2);
    __m128 c58_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a58_3 = _mm_load_ss(&A[353]);
    c58_3 = _mm_add_ss(c58_3, _mm_mul_ss(a58_3, b58));
    _mm_store_ss(&C[(l_n*56)+10], c58_3);
    __m128 c58_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a58_4 = _mm_load_ss(&A[354]);
    c58_4 = _mm_add_ss(c58_4, _mm_mul_ss(a58_4, b58));
    _mm_store_ss(&C[(l_n*56)+12], c58_4);
    __m128 c58_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a58_5 = _mm_load_ss(&A[355]);
    c58_5 = _mm_add_ss(c58_5, _mm_mul_ss(a58_5, b58));
    _mm_store_ss(&C[(l_n*56)+15], c58_5);
    __m128 c58_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a58_6 = _mm_load_ss(&A[356]);
    c58_6 = _mm_add_ss(c58_6, _mm_mul_ss(a58_6, b58));
    _mm_store_ss(&C[(l_n*56)+17], c58_6);
    __m128 c58_7 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a58_7 = _mm_load_ss(&A[357]);
    c58_7 = _mm_add_ss(c58_7, _mm_mul_ss(a58_7, b58));
    _mm_store_ss(&C[(l_n*56)+21], c58_7);
    __m128 c58_8 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a58_8 = _mm_load_ss(&A[358]);
    c58_8 = _mm_add_ss(c58_8, _mm_mul_ss(a58_8, b58));
    _mm_store_ss(&C[(l_n*56)+23], c58_8);
    __m128 c58_9 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a58_9 = _mm_load_ss(&A[359]);
    c58_9 = _mm_add_ss(c58_9, _mm_mul_ss(a58_9, b58));
    _mm_store_ss(&C[(l_n*56)+25], c58_9);
    __m128 c58_10 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a58_10 = _mm_load_ss(&A[360]);
    c58_10 = _mm_add_ss(c58_10, _mm_mul_ss(a58_10, b58));
    _mm_store_ss(&C[(l_n*56)+27], c58_10);
    __m128 c58_11 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a58_11 = _mm_load_ss(&A[361]);
    c58_11 = _mm_add_ss(c58_11, _mm_mul_ss(a58_11, b58));
    _mm_store_ss(&C[(l_n*56)+30], c58_11);
    __m128 c58_12 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a58_12 = _mm_load_ss(&A[362]);
    c58_12 = _mm_add_ss(c58_12, _mm_mul_ss(a58_12, b58));
    _mm_store_ss(&C[(l_n*56)+32], c58_12);
    __m128 c58_13 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a58_13 = _mm_load_ss(&A[363]);
    c58_13 = _mm_add_ss(c58_13, _mm_mul_ss(a58_13, b58));
    _mm_store_ss(&C[(l_n*56)+37], c58_13);
    __m128 c58_14 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a58_14 = _mm_load_ss(&A[364]);
    c58_14 = _mm_add_ss(c58_14, _mm_mul_ss(a58_14, b58));
    _mm_store_ss(&C[(l_n*56)+39], c58_14);
    __m128 c58_15 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a58_15 = _mm_load_ss(&A[365]);
    c58_15 = _mm_add_ss(c58_15, _mm_mul_ss(a58_15, b58));
    _mm_store_ss(&C[(l_n*56)+42], c58_15);
    __m128 c58_16 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a58_16 = _mm_load_ss(&A[366]);
    c58_16 = _mm_add_ss(c58_16, _mm_mul_ss(a58_16, b58));
    _mm_store_ss(&C[(l_n*56)+44], c58_16);
    __m128 c58_17 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a58_17 = _mm_load_ss(&A[367]);
    c58_17 = _mm_add_ss(c58_17, _mm_mul_ss(a58_17, b58));
    _mm_store_ss(&C[(l_n*56)+46], c58_17);
    __m128 c58_18 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a58_18 = _mm_load_ss(&A[368]);
    c58_18 = _mm_add_ss(c58_18, _mm_mul_ss(a58_18, b58));
    _mm_store_ss(&C[(l_n*56)+48], c58_18);
    __m128 c58_19 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a58_19 = _mm_load_ss(&A[369]);
    c58_19 = _mm_add_ss(c58_19, _mm_mul_ss(a58_19, b58));
    _mm_store_ss(&C[(l_n*56)+51], c58_19);
    __m128 c58_20 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a58_20 = _mm_load_ss(&A[370]);
    c58_20 = _mm_add_ss(c58_20, _mm_mul_ss(a58_20, b58));
    _mm_store_ss(&C[(l_n*56)+53], c58_20);
#else
    C[(l_n*56)+1] += A[350] * B[(l_n*84)+58];
    C[(l_n*56)+5] += A[351] * B[(l_n*84)+58];
    C[(l_n*56)+7] += A[352] * B[(l_n*84)+58];
    C[(l_n*56)+10] += A[353] * B[(l_n*84)+58];
    C[(l_n*56)+12] += A[354] * B[(l_n*84)+58];
    C[(l_n*56)+15] += A[355] * B[(l_n*84)+58];
    C[(l_n*56)+17] += A[356] * B[(l_n*84)+58];
    C[(l_n*56)+21] += A[357] * B[(l_n*84)+58];
    C[(l_n*56)+23] += A[358] * B[(l_n*84)+58];
    C[(l_n*56)+25] += A[359] * B[(l_n*84)+58];
    C[(l_n*56)+27] += A[360] * B[(l_n*84)+58];
    C[(l_n*56)+30] += A[361] * B[(l_n*84)+58];
    C[(l_n*56)+32] += A[362] * B[(l_n*84)+58];
    C[(l_n*56)+37] += A[363] * B[(l_n*84)+58];
    C[(l_n*56)+39] += A[364] * B[(l_n*84)+58];
    C[(l_n*56)+42] += A[365] * B[(l_n*84)+58];
    C[(l_n*56)+44] += A[366] * B[(l_n*84)+58];
    C[(l_n*56)+46] += A[367] * B[(l_n*84)+58];
    C[(l_n*56)+48] += A[368] * B[(l_n*84)+58];
    C[(l_n*56)+51] += A[369] * B[(l_n*84)+58];
    C[(l_n*56)+53] += A[370] * B[(l_n*84)+58];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b59 = _mm_broadcast_ss(&B[(l_n*84)+59]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b59 = _mm_load_ss(&B[(l_n*84)+59]);    b59 = _mm_shuffle_ps(b59, b59, 0x00);
#endif
    __m128 c59_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a59_0 = _mm_load_ss(&A[371]);
    c59_0 = _mm_add_ss(c59_0, _mm_mul_ss(a59_0, b59));
    _mm_store_ss(&C[(l_n*56)+0], c59_0);
    __m128 c59_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a59_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[372]));
    c59_1 = _mm_add_ps(c59_1, _mm_mul_ps(a59_1, b59));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c59_1));
    __m128 c59_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a59_3 = _mm_load_ss(&A[374]);
    c59_3 = _mm_add_ss(c59_3, _mm_mul_ss(a59_3, b59));
    _mm_store_ss(&C[(l_n*56)+4], c59_3);
    __m128 c59_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a59_4 = _mm_load_ss(&A[375]);
    c59_4 = _mm_add_ss(c59_4, _mm_mul_ss(a59_4, b59));
    _mm_store_ss(&C[(l_n*56)+6], c59_4);
    __m128 c59_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a59_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[376]));
    c59_5 = _mm_add_ps(c59_5, _mm_mul_ps(a59_5, b59));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c59_5));
    __m128 c59_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a59_7 = _mm_load_ss(&A[378]);
    c59_7 = _mm_add_ss(c59_7, _mm_mul_ss(a59_7, b59));
    _mm_store_ss(&C[(l_n*56)+11], c59_7);
    __m128 c59_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a59_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[379]));
    c59_8 = _mm_add_ps(c59_8, _mm_mul_ps(a59_8, b59));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c59_8));
    __m128 c59_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a59_10 = _mm_load_ss(&A[381]);
    c59_10 = _mm_add_ss(c59_10, _mm_mul_ss(a59_10, b59));
    _mm_store_ss(&C[(l_n*56)+16], c59_10);
    __m128 c59_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a59_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[382]));
    c59_11 = _mm_add_ps(c59_11, _mm_mul_ps(a59_11, b59));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c59_11));
    __m128 c59_13 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a59_13 = _mm_load_ss(&A[384]);
    c59_13 = _mm_add_ss(c59_13, _mm_mul_ss(a59_13, b59));
    _mm_store_ss(&C[(l_n*56)+22], c59_13);
    __m128 c59_14 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a59_14 = _mm_load_ss(&A[385]);
    c59_14 = _mm_add_ss(c59_14, _mm_mul_ss(a59_14, b59));
    _mm_store_ss(&C[(l_n*56)+24], c59_14);
    __m128 c59_15 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a59_15 = _mm_load_ss(&A[386]);
    c59_15 = _mm_add_ss(c59_15, _mm_mul_ss(a59_15, b59));
    _mm_store_ss(&C[(l_n*56)+26], c59_15);
    __m128 c59_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a59_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[387]));
    c59_16 = _mm_add_ps(c59_16, _mm_mul_ps(a59_16, b59));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c59_16));
    __m128 c59_18 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a59_18 = _mm_load_ss(&A[389]);
    c59_18 = _mm_add_ss(c59_18, _mm_mul_ss(a59_18, b59));
    _mm_store_ss(&C[(l_n*56)+31], c59_18);
    __m128 c59_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a59_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[390]));
    c59_19 = _mm_add_ps(c59_19, _mm_mul_ps(a59_19, b59));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c59_19));
    __m128 c59_21 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a59_21 = _mm_load_ss(&A[392]);
    c59_21 = _mm_add_ss(c59_21, _mm_mul_ss(a59_21, b59));
    _mm_store_ss(&C[(l_n*56)+38], c59_21);
    __m128 c59_22 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a59_22 = _mm_load_ss(&A[393]);
    c59_22 = _mm_add_ss(c59_22, _mm_mul_ss(a59_22, b59));
    _mm_store_ss(&C[(l_n*56)+40], c59_22);
    __m128 c59_23 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a59_23 = _mm_load_ss(&A[394]);
    c59_23 = _mm_add_ss(c59_23, _mm_mul_ss(a59_23, b59));
    _mm_store_ss(&C[(l_n*56)+43], c59_23);
    __m128 c59_24 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a59_24 = _mm_load_ss(&A[395]);
    c59_24 = _mm_add_ss(c59_24, _mm_mul_ss(a59_24, b59));
    _mm_store_ss(&C[(l_n*56)+45], c59_24);
    __m128 c59_25 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a59_25 = _mm_load_ss(&A[396]);
    c59_25 = _mm_add_ss(c59_25, _mm_mul_ss(a59_25, b59));
    _mm_store_ss(&C[(l_n*56)+47], c59_25);
    __m128 c59_26 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+49]));
    __m128 a59_26 = _mm_castpd_ps(_mm_load_sd((const double*)&A[397]));
    c59_26 = _mm_add_ps(c59_26, _mm_mul_ps(a59_26, b59));
    _mm_store_sd((double*)&C[(l_n*56)+49], _mm_castps_pd(c59_26));
    __m128 c59_28 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a59_28 = _mm_load_ss(&A[399]);
    c59_28 = _mm_add_ss(c59_28, _mm_mul_ss(a59_28, b59));
    _mm_store_ss(&C[(l_n*56)+52], c59_28);
    __m128 c59_29 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a59_29 = _mm_castpd_ps(_mm_load_sd((const double*)&A[400]));
    c59_29 = _mm_add_ps(c59_29, _mm_mul_ps(a59_29, b59));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c59_29));
#else
    C[(l_n*56)+0] += A[371] * B[(l_n*84)+59];
    C[(l_n*56)+2] += A[372] * B[(l_n*84)+59];
    C[(l_n*56)+3] += A[373] * B[(l_n*84)+59];
    C[(l_n*56)+4] += A[374] * B[(l_n*84)+59];
    C[(l_n*56)+6] += A[375] * B[(l_n*84)+59];
    C[(l_n*56)+8] += A[376] * B[(l_n*84)+59];
    C[(l_n*56)+9] += A[377] * B[(l_n*84)+59];
    C[(l_n*56)+11] += A[378] * B[(l_n*84)+59];
    C[(l_n*56)+13] += A[379] * B[(l_n*84)+59];
    C[(l_n*56)+14] += A[380] * B[(l_n*84)+59];
    C[(l_n*56)+16] += A[381] * B[(l_n*84)+59];
    C[(l_n*56)+18] += A[382] * B[(l_n*84)+59];
    C[(l_n*56)+19] += A[383] * B[(l_n*84)+59];
    C[(l_n*56)+22] += A[384] * B[(l_n*84)+59];
    C[(l_n*56)+24] += A[385] * B[(l_n*84)+59];
    C[(l_n*56)+26] += A[386] * B[(l_n*84)+59];
    C[(l_n*56)+28] += A[387] * B[(l_n*84)+59];
    C[(l_n*56)+29] += A[388] * B[(l_n*84)+59];
    C[(l_n*56)+31] += A[389] * B[(l_n*84)+59];
    C[(l_n*56)+33] += A[390] * B[(l_n*84)+59];
    C[(l_n*56)+34] += A[391] * B[(l_n*84)+59];
    C[(l_n*56)+38] += A[392] * B[(l_n*84)+59];
    C[(l_n*56)+40] += A[393] * B[(l_n*84)+59];
    C[(l_n*56)+43] += A[394] * B[(l_n*84)+59];
    C[(l_n*56)+45] += A[395] * B[(l_n*84)+59];
    C[(l_n*56)+47] += A[396] * B[(l_n*84)+59];
    C[(l_n*56)+49] += A[397] * B[(l_n*84)+59];
    C[(l_n*56)+50] += A[398] * B[(l_n*84)+59];
    C[(l_n*56)+52] += A[399] * B[(l_n*84)+59];
    C[(l_n*56)+54] += A[400] * B[(l_n*84)+59];
    C[(l_n*56)+55] += A[401] * B[(l_n*84)+59];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b60 = _mm_broadcast_ss(&B[(l_n*84)+60]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b60 = _mm_load_ss(&B[(l_n*84)+60]);    b60 = _mm_shuffle_ps(b60, b60, 0x00);
#endif
    __m128 c60_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a60_0 = _mm_load_ss(&A[402]);
    c60_0 = _mm_add_ss(c60_0, _mm_mul_ss(a60_0, b60));
    _mm_store_ss(&C[(l_n*56)+1], c60_0);
    __m128 c60_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a60_1 = _mm_load_ss(&A[403]);
    c60_1 = _mm_add_ss(c60_1, _mm_mul_ss(a60_1, b60));
    _mm_store_ss(&C[(l_n*56)+5], c60_1);
    __m128 c60_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a60_2 = _mm_load_ss(&A[404]);
    c60_2 = _mm_add_ss(c60_2, _mm_mul_ss(a60_2, b60));
    _mm_store_ss(&C[(l_n*56)+7], c60_2);
    __m128 c60_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a60_3 = _mm_load_ss(&A[405]);
    c60_3 = _mm_add_ss(c60_3, _mm_mul_ss(a60_3, b60));
    _mm_store_ss(&C[(l_n*56)+12], c60_3);
    __m128 c60_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a60_4 = _mm_load_ss(&A[406]);
    c60_4 = _mm_add_ss(c60_4, _mm_mul_ss(a60_4, b60));
    _mm_store_ss(&C[(l_n*56)+15], c60_4);
    __m128 c60_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a60_5 = _mm_load_ss(&A[407]);
    c60_5 = _mm_add_ss(c60_5, _mm_mul_ss(a60_5, b60));
    _mm_store_ss(&C[(l_n*56)+17], c60_5);
    __m128 c60_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a60_6 = _mm_load_ss(&A[408]);
    c60_6 = _mm_add_ss(c60_6, _mm_mul_ss(a60_6, b60));
    _mm_store_ss(&C[(l_n*56)+23], c60_6);
    __m128 c60_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a60_7 = _mm_load_ss(&A[409]);
    c60_7 = _mm_add_ss(c60_7, _mm_mul_ss(a60_7, b60));
    _mm_store_ss(&C[(l_n*56)+27], c60_7);
    __m128 c60_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a60_8 = _mm_load_ss(&A[410]);
    c60_8 = _mm_add_ss(c60_8, _mm_mul_ss(a60_8, b60));
    _mm_store_ss(&C[(l_n*56)+30], c60_8);
    __m128 c60_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a60_9 = _mm_load_ss(&A[411]);
    c60_9 = _mm_add_ss(c60_9, _mm_mul_ss(a60_9, b60));
    _mm_store_ss(&C[(l_n*56)+32], c60_9);
    __m128 c60_10 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a60_10 = _mm_load_ss(&A[412]);
    c60_10 = _mm_add_ss(c60_10, _mm_mul_ss(a60_10, b60));
    _mm_store_ss(&C[(l_n*56)+39], c60_10);
    __m128 c60_11 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a60_11 = _mm_load_ss(&A[413]);
    c60_11 = _mm_add_ss(c60_11, _mm_mul_ss(a60_11, b60));
    _mm_store_ss(&C[(l_n*56)+44], c60_11);
    __m128 c60_12 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a60_12 = _mm_load_ss(&A[414]);
    c60_12 = _mm_add_ss(c60_12, _mm_mul_ss(a60_12, b60));
    _mm_store_ss(&C[(l_n*56)+48], c60_12);
    __m128 c60_13 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a60_13 = _mm_load_ss(&A[415]);
    c60_13 = _mm_add_ss(c60_13, _mm_mul_ss(a60_13, b60));
    _mm_store_ss(&C[(l_n*56)+51], c60_13);
    __m128 c60_14 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a60_14 = _mm_load_ss(&A[416]);
    c60_14 = _mm_add_ss(c60_14, _mm_mul_ss(a60_14, b60));
    _mm_store_ss(&C[(l_n*56)+53], c60_14);
#else
    C[(l_n*56)+1] += A[402] * B[(l_n*84)+60];
    C[(l_n*56)+5] += A[403] * B[(l_n*84)+60];
    C[(l_n*56)+7] += A[404] * B[(l_n*84)+60];
    C[(l_n*56)+12] += A[405] * B[(l_n*84)+60];
    C[(l_n*56)+15] += A[406] * B[(l_n*84)+60];
    C[(l_n*56)+17] += A[407] * B[(l_n*84)+60];
    C[(l_n*56)+23] += A[408] * B[(l_n*84)+60];
    C[(l_n*56)+27] += A[409] * B[(l_n*84)+60];
    C[(l_n*56)+30] += A[410] * B[(l_n*84)+60];
    C[(l_n*56)+32] += A[411] * B[(l_n*84)+60];
    C[(l_n*56)+39] += A[412] * B[(l_n*84)+60];
    C[(l_n*56)+44] += A[413] * B[(l_n*84)+60];
    C[(l_n*56)+48] += A[414] * B[(l_n*84)+60];
    C[(l_n*56)+51] += A[415] * B[(l_n*84)+60];
    C[(l_n*56)+53] += A[416] * B[(l_n*84)+60];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b61 = _mm_broadcast_ss(&B[(l_n*84)+61]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b61 = _mm_load_ss(&B[(l_n*84)+61]);    b61 = _mm_shuffle_ps(b61, b61, 0x00);
#endif
    __m128 c61_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a61_0 = _mm_load_ss(&A[417]);
    c61_0 = _mm_add_ss(c61_0, _mm_mul_ss(a61_0, b61));
    _mm_store_ss(&C[(l_n*56)+0], c61_0);
    __m128 c61_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a61_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[418]));
    c61_1 = _mm_add_ps(c61_1, _mm_mul_ps(a61_1, b61));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c61_1));
    __m128 c61_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a61_3 = _mm_load_ss(&A[420]);
    c61_3 = _mm_add_ss(c61_3, _mm_mul_ss(a61_3, b61));
    _mm_store_ss(&C[(l_n*56)+6], c61_3);
    __m128 c61_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a61_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[421]));
    c61_4 = _mm_add_ps(c61_4, _mm_mul_ps(a61_4, b61));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c61_4));
    __m128 c61_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a61_6 = _mm_load_ss(&A[423]);
    c61_6 = _mm_add_ss(c61_6, _mm_mul_ss(a61_6, b61));
    _mm_store_ss(&C[(l_n*56)+13], c61_6);
    __m128 c61_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a61_7 = _mm_load_ss(&A[424]);
    c61_7 = _mm_add_ss(c61_7, _mm_mul_ss(a61_7, b61));
    _mm_store_ss(&C[(l_n*56)+16], c61_7);
    __m128 c61_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a61_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[425]));
    c61_8 = _mm_add_ps(c61_8, _mm_mul_ps(a61_8, b61));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c61_8));
    __m128 c61_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a61_10 = _mm_load_ss(&A[427]);
    c61_10 = _mm_add_ss(c61_10, _mm_mul_ss(a61_10, b61));
    _mm_store_ss(&C[(l_n*56)+24], c61_10);
    __m128 c61_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a61_11 = _mm_load_ss(&A[428]);
    c61_11 = _mm_add_ss(c61_11, _mm_mul_ss(a61_11, b61));
    _mm_store_ss(&C[(l_n*56)+28], c61_11);
    __m128 c61_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a61_12 = _mm_load_ss(&A[429]);
    c61_12 = _mm_add_ss(c61_12, _mm_mul_ss(a61_12, b61));
    _mm_store_ss(&C[(l_n*56)+31], c61_12);
    __m128 c61_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a61_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[430]));
    c61_13 = _mm_add_ps(c61_13, _mm_mul_ps(a61_13, b61));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c61_13));
    __m128 c61_15 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a61_15 = _mm_load_ss(&A[432]);
    c61_15 = _mm_add_ss(c61_15, _mm_mul_ss(a61_15, b61));
    _mm_store_ss(&C[(l_n*56)+40], c61_15);
    __m128 c61_16 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a61_16 = _mm_load_ss(&A[433]);
    c61_16 = _mm_add_ss(c61_16, _mm_mul_ss(a61_16, b61));
    _mm_store_ss(&C[(l_n*56)+45], c61_16);
    __m128 c61_17 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a61_17 = _mm_load_ss(&A[434]);
    c61_17 = _mm_add_ss(c61_17, _mm_mul_ss(a61_17, b61));
    _mm_store_ss(&C[(l_n*56)+49], c61_17);
    __m128 c61_18 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a61_18 = _mm_load_ss(&A[435]);
    c61_18 = _mm_add_ss(c61_18, _mm_mul_ss(a61_18, b61));
    _mm_store_ss(&C[(l_n*56)+52], c61_18);
    __m128 c61_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a61_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[436]));
    c61_19 = _mm_add_ps(c61_19, _mm_mul_ps(a61_19, b61));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c61_19));
#else
    C[(l_n*56)+0] += A[417] * B[(l_n*84)+61];
    C[(l_n*56)+2] += A[418] * B[(l_n*84)+61];
    C[(l_n*56)+3] += A[419] * B[(l_n*84)+61];
    C[(l_n*56)+6] += A[420] * B[(l_n*84)+61];
    C[(l_n*56)+8] += A[421] * B[(l_n*84)+61];
    C[(l_n*56)+9] += A[422] * B[(l_n*84)+61];
    C[(l_n*56)+13] += A[423] * B[(l_n*84)+61];
    C[(l_n*56)+16] += A[424] * B[(l_n*84)+61];
    C[(l_n*56)+18] += A[425] * B[(l_n*84)+61];
    C[(l_n*56)+19] += A[426] * B[(l_n*84)+61];
    C[(l_n*56)+24] += A[427] * B[(l_n*84)+61];
    C[(l_n*56)+28] += A[428] * B[(l_n*84)+61];
    C[(l_n*56)+31] += A[429] * B[(l_n*84)+61];
    C[(l_n*56)+33] += A[430] * B[(l_n*84)+61];
    C[(l_n*56)+34] += A[431] * B[(l_n*84)+61];
    C[(l_n*56)+40] += A[432] * B[(l_n*84)+61];
    C[(l_n*56)+45] += A[433] * B[(l_n*84)+61];
    C[(l_n*56)+49] += A[434] * B[(l_n*84)+61];
    C[(l_n*56)+52] += A[435] * B[(l_n*84)+61];
    C[(l_n*56)+54] += A[436] * B[(l_n*84)+61];
    C[(l_n*56)+55] += A[437] * B[(l_n*84)+61];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b63 = _mm_broadcast_ss(&B[(l_n*84)+63]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b63 = _mm_load_ss(&B[(l_n*84)+63]);    b63 = _mm_shuffle_ps(b63, b63, 0x00);
#endif
    __m128 c63_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a63_0 = _mm_load_ss(&A[438]);
    c63_0 = _mm_add_ss(c63_0, _mm_mul_ss(a63_0, b63));
    _mm_store_ss(&C[(l_n*56)+0], c63_0);
    __m128 c63_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a63_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[439]));
    c63_1 = _mm_add_ps(c63_1, _mm_mul_ps(a63_1, b63));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c63_1));
    __m128 c63_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a63_3 = _mm_load_ss(&A[441]);
    c63_3 = _mm_add_ss(c63_3, _mm_mul_ss(a63_3, b63));
    _mm_store_ss(&C[(l_n*56)+4], c63_3);
    __m128 c63_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a63_4 = _mm_load_ss(&A[442]);
    c63_4 = _mm_add_ss(c63_4, _mm_mul_ss(a63_4, b63));
    _mm_store_ss(&C[(l_n*56)+6], c63_4);
    __m128 c63_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a63_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[443]));
    c63_5 = _mm_add_ps(c63_5, _mm_mul_ps(a63_5, b63));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c63_5));
    __m128 c63_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a63_7 = _mm_load_ss(&A[445]);
    c63_7 = _mm_add_ss(c63_7, _mm_mul_ss(a63_7, b63));
    _mm_store_ss(&C[(l_n*56)+11], c63_7);
    __m128 c63_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a63_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[446]));
    c63_8 = _mm_add_ps(c63_8, _mm_mul_ps(a63_8, b63));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c63_8));
    __m128 c63_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a63_10 = _mm_load_ss(&A[448]);
    c63_10 = _mm_add_ss(c63_10, _mm_mul_ss(a63_10, b63));
    _mm_store_ss(&C[(l_n*56)+16], c63_10);
    __m128 c63_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a63_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[449]));
    c63_11 = _mm_add_ps(c63_11, _mm_mul_ps(a63_11, b63));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c63_11));
    __m128 c63_13 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a63_13 = _mm_load_ss(&A[451]);
    c63_13 = _mm_add_ss(c63_13, _mm_mul_ss(a63_13, b63));
    _mm_store_ss(&C[(l_n*56)+20], c63_13);
    __m128 c63_14 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a63_14 = _mm_load_ss(&A[452]);
    c63_14 = _mm_add_ss(c63_14, _mm_mul_ss(a63_14, b63));
    _mm_store_ss(&C[(l_n*56)+22], c63_14);
    __m128 c63_15 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a63_15 = _mm_load_ss(&A[453]);
    c63_15 = _mm_add_ss(c63_15, _mm_mul_ss(a63_15, b63));
    _mm_store_ss(&C[(l_n*56)+24], c63_15);
    __m128 c63_16 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a63_16 = _mm_load_ss(&A[454]);
    c63_16 = _mm_add_ss(c63_16, _mm_mul_ss(a63_16, b63));
    _mm_store_ss(&C[(l_n*56)+26], c63_16);
    __m128 c63_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a63_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[455]));
    c63_17 = _mm_add_ps(c63_17, _mm_mul_ps(a63_17, b63));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c63_17));
    __m128 c63_19 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a63_19 = _mm_load_ss(&A[457]);
    c63_19 = _mm_add_ss(c63_19, _mm_mul_ss(a63_19, b63));
    _mm_store_ss(&C[(l_n*56)+31], c63_19);
    __m128 c63_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a63_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[458]));
    c63_20 = _mm_add_ps(c63_20, _mm_mul_ps(a63_20, b63));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c63_20));
    __m128 c63_22 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a63_22 = _mm_load_ss(&A[460]);
    c63_22 = _mm_add_ss(c63_22, _mm_mul_ss(a63_22, b63));
    _mm_store_ss(&C[(l_n*56)+41], c63_22);
    __m128 c63_23 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a63_23 = _mm_load_ss(&A[461]);
    c63_23 = _mm_add_ss(c63_23, _mm_mul_ss(a63_23, b63));
    _mm_store_ss(&C[(l_n*56)+43], c63_23);
    __m128 c63_24 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a63_24 = _mm_load_ss(&A[462]);
    c63_24 = _mm_add_ss(c63_24, _mm_mul_ss(a63_24, b63));
    _mm_store_ss(&C[(l_n*56)+45], c63_24);
    __m128 c63_25 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a63_25 = _mm_load_ss(&A[463]);
    c63_25 = _mm_add_ss(c63_25, _mm_mul_ss(a63_25, b63));
    _mm_store_ss(&C[(l_n*56)+47], c63_25);
    __m128 c63_26 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+49]));
    __m128 a63_26 = _mm_castpd_ps(_mm_load_sd((const double*)&A[464]));
    c63_26 = _mm_add_ps(c63_26, _mm_mul_ps(a63_26, b63));
    _mm_store_sd((double*)&C[(l_n*56)+49], _mm_castps_pd(c63_26));
    __m128 c63_28 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a63_28 = _mm_load_ss(&A[466]);
    c63_28 = _mm_add_ss(c63_28, _mm_mul_ss(a63_28, b63));
    _mm_store_ss(&C[(l_n*56)+52], c63_28);
    __m128 c63_29 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a63_29 = _mm_castpd_ps(_mm_load_sd((const double*)&A[467]));
    c63_29 = _mm_add_ps(c63_29, _mm_mul_ps(a63_29, b63));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c63_29));
#else
    C[(l_n*56)+0] += A[438] * B[(l_n*84)+63];
    C[(l_n*56)+2] += A[439] * B[(l_n*84)+63];
    C[(l_n*56)+3] += A[440] * B[(l_n*84)+63];
    C[(l_n*56)+4] += A[441] * B[(l_n*84)+63];
    C[(l_n*56)+6] += A[442] * B[(l_n*84)+63];
    C[(l_n*56)+8] += A[443] * B[(l_n*84)+63];
    C[(l_n*56)+9] += A[444] * B[(l_n*84)+63];
    C[(l_n*56)+11] += A[445] * B[(l_n*84)+63];
    C[(l_n*56)+13] += A[446] * B[(l_n*84)+63];
    C[(l_n*56)+14] += A[447] * B[(l_n*84)+63];
    C[(l_n*56)+16] += A[448] * B[(l_n*84)+63];
    C[(l_n*56)+18] += A[449] * B[(l_n*84)+63];
    C[(l_n*56)+19] += A[450] * B[(l_n*84)+63];
    C[(l_n*56)+20] += A[451] * B[(l_n*84)+63];
    C[(l_n*56)+22] += A[452] * B[(l_n*84)+63];
    C[(l_n*56)+24] += A[453] * B[(l_n*84)+63];
    C[(l_n*56)+26] += A[454] * B[(l_n*84)+63];
    C[(l_n*56)+28] += A[455] * B[(l_n*84)+63];
    C[(l_n*56)+29] += A[456] * B[(l_n*84)+63];
    C[(l_n*56)+31] += A[457] * B[(l_n*84)+63];
    C[(l_n*56)+33] += A[458] * B[(l_n*84)+63];
    C[(l_n*56)+34] += A[459] * B[(l_n*84)+63];
    C[(l_n*56)+41] += A[460] * B[(l_n*84)+63];
    C[(l_n*56)+43] += A[461] * B[(l_n*84)+63];
    C[(l_n*56)+45] += A[462] * B[(l_n*84)+63];
    C[(l_n*56)+47] += A[463] * B[(l_n*84)+63];
    C[(l_n*56)+49] += A[464] * B[(l_n*84)+63];
    C[(l_n*56)+50] += A[465] * B[(l_n*84)+63];
    C[(l_n*56)+52] += A[466] * B[(l_n*84)+63];
    C[(l_n*56)+54] += A[467] * B[(l_n*84)+63];
    C[(l_n*56)+55] += A[468] * B[(l_n*84)+63];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b64 = _mm_broadcast_ss(&B[(l_n*84)+64]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b64 = _mm_load_ss(&B[(l_n*84)+64]);    b64 = _mm_shuffle_ps(b64, b64, 0x00);
#endif
    __m128 c64_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a64_0 = _mm_load_ss(&A[469]);
    c64_0 = _mm_add_ss(c64_0, _mm_mul_ss(a64_0, b64));
    _mm_store_ss(&C[(l_n*56)+1], c64_0);
    __m128 c64_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a64_1 = _mm_load_ss(&A[470]);
    c64_1 = _mm_add_ss(c64_1, _mm_mul_ss(a64_1, b64));
    _mm_store_ss(&C[(l_n*56)+5], c64_1);
    __m128 c64_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a64_2 = _mm_load_ss(&A[471]);
    c64_2 = _mm_add_ss(c64_2, _mm_mul_ss(a64_2, b64));
    _mm_store_ss(&C[(l_n*56)+7], c64_2);
    __m128 c64_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a64_3 = _mm_load_ss(&A[472]);
    c64_3 = _mm_add_ss(c64_3, _mm_mul_ss(a64_3, b64));
    _mm_store_ss(&C[(l_n*56)+10], c64_3);
    __m128 c64_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a64_4 = _mm_load_ss(&A[473]);
    c64_4 = _mm_add_ss(c64_4, _mm_mul_ss(a64_4, b64));
    _mm_store_ss(&C[(l_n*56)+12], c64_4);
    __m128 c64_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a64_5 = _mm_load_ss(&A[474]);
    c64_5 = _mm_add_ss(c64_5, _mm_mul_ss(a64_5, b64));
    _mm_store_ss(&C[(l_n*56)+15], c64_5);
    __m128 c64_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a64_6 = _mm_load_ss(&A[475]);
    c64_6 = _mm_add_ss(c64_6, _mm_mul_ss(a64_6, b64));
    _mm_store_ss(&C[(l_n*56)+17], c64_6);
    __m128 c64_7 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a64_7 = _mm_load_ss(&A[476]);
    c64_7 = _mm_add_ss(c64_7, _mm_mul_ss(a64_7, b64));
    _mm_store_ss(&C[(l_n*56)+21], c64_7);
    __m128 c64_8 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a64_8 = _mm_load_ss(&A[477]);
    c64_8 = _mm_add_ss(c64_8, _mm_mul_ss(a64_8, b64));
    _mm_store_ss(&C[(l_n*56)+23], c64_8);
    __m128 c64_9 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a64_9 = _mm_load_ss(&A[478]);
    c64_9 = _mm_add_ss(c64_9, _mm_mul_ss(a64_9, b64));
    _mm_store_ss(&C[(l_n*56)+25], c64_9);
    __m128 c64_10 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a64_10 = _mm_load_ss(&A[479]);
    c64_10 = _mm_add_ss(c64_10, _mm_mul_ss(a64_10, b64));
    _mm_store_ss(&C[(l_n*56)+27], c64_10);
    __m128 c64_11 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a64_11 = _mm_load_ss(&A[480]);
    c64_11 = _mm_add_ss(c64_11, _mm_mul_ss(a64_11, b64));
    _mm_store_ss(&C[(l_n*56)+30], c64_11);
    __m128 c64_12 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a64_12 = _mm_load_ss(&A[481]);
    c64_12 = _mm_add_ss(c64_12, _mm_mul_ss(a64_12, b64));
    _mm_store_ss(&C[(l_n*56)+32], c64_12);
    __m128 c64_13 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a64_13 = _mm_load_ss(&A[482]);
    c64_13 = _mm_add_ss(c64_13, _mm_mul_ss(a64_13, b64));
    _mm_store_ss(&C[(l_n*56)+42], c64_13);
    __m128 c64_14 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a64_14 = _mm_load_ss(&A[483]);
    c64_14 = _mm_add_ss(c64_14, _mm_mul_ss(a64_14, b64));
    _mm_store_ss(&C[(l_n*56)+44], c64_14);
    __m128 c64_15 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a64_15 = _mm_load_ss(&A[484]);
    c64_15 = _mm_add_ss(c64_15, _mm_mul_ss(a64_15, b64));
    _mm_store_ss(&C[(l_n*56)+46], c64_15);
    __m128 c64_16 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a64_16 = _mm_load_ss(&A[485]);
    c64_16 = _mm_add_ss(c64_16, _mm_mul_ss(a64_16, b64));
    _mm_store_ss(&C[(l_n*56)+48], c64_16);
    __m128 c64_17 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a64_17 = _mm_load_ss(&A[486]);
    c64_17 = _mm_add_ss(c64_17, _mm_mul_ss(a64_17, b64));
    _mm_store_ss(&C[(l_n*56)+51], c64_17);
    __m128 c64_18 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a64_18 = _mm_load_ss(&A[487]);
    c64_18 = _mm_add_ss(c64_18, _mm_mul_ss(a64_18, b64));
    _mm_store_ss(&C[(l_n*56)+53], c64_18);
#else
    C[(l_n*56)+1] += A[469] * B[(l_n*84)+64];
    C[(l_n*56)+5] += A[470] * B[(l_n*84)+64];
    C[(l_n*56)+7] += A[471] * B[(l_n*84)+64];
    C[(l_n*56)+10] += A[472] * B[(l_n*84)+64];
    C[(l_n*56)+12] += A[473] * B[(l_n*84)+64];
    C[(l_n*56)+15] += A[474] * B[(l_n*84)+64];
    C[(l_n*56)+17] += A[475] * B[(l_n*84)+64];
    C[(l_n*56)+21] += A[476] * B[(l_n*84)+64];
    C[(l_n*56)+23] += A[477] * B[(l_n*84)+64];
    C[(l_n*56)+25] += A[478] * B[(l_n*84)+64];
    C[(l_n*56)+27] += A[479] * B[(l_n*84)+64];
    C[(l_n*56)+30] += A[480] * B[(l_n*84)+64];
    C[(l_n*56)+32] += A[481] * B[(l_n*84)+64];
    C[(l_n*56)+42] += A[482] * B[(l_n*84)+64];
    C[(l_n*56)+44] += A[483] * B[(l_n*84)+64];
    C[(l_n*56)+46] += A[484] * B[(l_n*84)+64];
    C[(l_n*56)+48] += A[485] * B[(l_n*84)+64];
    C[(l_n*56)+51] += A[486] * B[(l_n*84)+64];
    C[(l_n*56)+53] += A[487] * B[(l_n*84)+64];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b65 = _mm_broadcast_ss(&B[(l_n*84)+65]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b65 = _mm_load_ss(&B[(l_n*84)+65]);    b65 = _mm_shuffle_ps(b65, b65, 0x00);
#endif
    __m128 c65_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a65_0 = _mm_load_ss(&A[488]);
    c65_0 = _mm_add_ss(c65_0, _mm_mul_ss(a65_0, b65));
    _mm_store_ss(&C[(l_n*56)+0], c65_0);
    __m128 c65_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a65_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[489]));
    c65_1 = _mm_add_ps(c65_1, _mm_mul_ps(a65_1, b65));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c65_1));
    __m128 c65_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a65_3 = _mm_load_ss(&A[491]);
    c65_3 = _mm_add_ss(c65_3, _mm_mul_ss(a65_3, b65));
    _mm_store_ss(&C[(l_n*56)+4], c65_3);
    __m128 c65_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a65_4 = _mm_load_ss(&A[492]);
    c65_4 = _mm_add_ss(c65_4, _mm_mul_ss(a65_4, b65));
    _mm_store_ss(&C[(l_n*56)+6], c65_4);
    __m128 c65_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a65_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[493]));
    c65_5 = _mm_add_ps(c65_5, _mm_mul_ps(a65_5, b65));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c65_5));
    __m128 c65_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a65_7 = _mm_load_ss(&A[495]);
    c65_7 = _mm_add_ss(c65_7, _mm_mul_ss(a65_7, b65));
    _mm_store_ss(&C[(l_n*56)+11], c65_7);
    __m128 c65_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a65_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[496]));
    c65_8 = _mm_add_ps(c65_8, _mm_mul_ps(a65_8, b65));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c65_8));
    __m128 c65_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a65_10 = _mm_load_ss(&A[498]);
    c65_10 = _mm_add_ss(c65_10, _mm_mul_ss(a65_10, b65));
    _mm_store_ss(&C[(l_n*56)+16], c65_10);
    __m128 c65_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a65_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[499]));
    c65_11 = _mm_add_ps(c65_11, _mm_mul_ps(a65_11, b65));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c65_11));
    __m128 c65_13 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a65_13 = _mm_load_ss(&A[501]);
    c65_13 = _mm_add_ss(c65_13, _mm_mul_ss(a65_13, b65));
    _mm_store_ss(&C[(l_n*56)+22], c65_13);
    __m128 c65_14 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a65_14 = _mm_load_ss(&A[502]);
    c65_14 = _mm_add_ss(c65_14, _mm_mul_ss(a65_14, b65));
    _mm_store_ss(&C[(l_n*56)+24], c65_14);
    __m128 c65_15 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a65_15 = _mm_load_ss(&A[503]);
    c65_15 = _mm_add_ss(c65_15, _mm_mul_ss(a65_15, b65));
    _mm_store_ss(&C[(l_n*56)+26], c65_15);
    __m128 c65_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a65_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[504]));
    c65_16 = _mm_add_ps(c65_16, _mm_mul_ps(a65_16, b65));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c65_16));
    __m128 c65_18 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a65_18 = _mm_load_ss(&A[506]);
    c65_18 = _mm_add_ss(c65_18, _mm_mul_ss(a65_18, b65));
    _mm_store_ss(&C[(l_n*56)+31], c65_18);
    __m128 c65_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a65_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[507]));
    c65_19 = _mm_add_ps(c65_19, _mm_mul_ps(a65_19, b65));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c65_19));
    __m128 c65_21 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a65_21 = _mm_load_ss(&A[509]);
    c65_21 = _mm_add_ss(c65_21, _mm_mul_ss(a65_21, b65));
    _mm_store_ss(&C[(l_n*56)+43], c65_21);
    __m128 c65_22 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a65_22 = _mm_load_ss(&A[510]);
    c65_22 = _mm_add_ss(c65_22, _mm_mul_ss(a65_22, b65));
    _mm_store_ss(&C[(l_n*56)+45], c65_22);
    __m128 c65_23 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a65_23 = _mm_load_ss(&A[511]);
    c65_23 = _mm_add_ss(c65_23, _mm_mul_ss(a65_23, b65));
    _mm_store_ss(&C[(l_n*56)+47], c65_23);
    __m128 c65_24 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+49]));
    __m128 a65_24 = _mm_castpd_ps(_mm_load_sd((const double*)&A[512]));
    c65_24 = _mm_add_ps(c65_24, _mm_mul_ps(a65_24, b65));
    _mm_store_sd((double*)&C[(l_n*56)+49], _mm_castps_pd(c65_24));
    __m128 c65_26 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a65_26 = _mm_load_ss(&A[514]);
    c65_26 = _mm_add_ss(c65_26, _mm_mul_ss(a65_26, b65));
    _mm_store_ss(&C[(l_n*56)+52], c65_26);
    __m128 c65_27 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a65_27 = _mm_castpd_ps(_mm_load_sd((const double*)&A[515]));
    c65_27 = _mm_add_ps(c65_27, _mm_mul_ps(a65_27, b65));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c65_27));
#else
    C[(l_n*56)+0] += A[488] * B[(l_n*84)+65];
    C[(l_n*56)+2] += A[489] * B[(l_n*84)+65];
    C[(l_n*56)+3] += A[490] * B[(l_n*84)+65];
    C[(l_n*56)+4] += A[491] * B[(l_n*84)+65];
    C[(l_n*56)+6] += A[492] * B[(l_n*84)+65];
    C[(l_n*56)+8] += A[493] * B[(l_n*84)+65];
    C[(l_n*56)+9] += A[494] * B[(l_n*84)+65];
    C[(l_n*56)+11] += A[495] * B[(l_n*84)+65];
    C[(l_n*56)+13] += A[496] * B[(l_n*84)+65];
    C[(l_n*56)+14] += A[497] * B[(l_n*84)+65];
    C[(l_n*56)+16] += A[498] * B[(l_n*84)+65];
    C[(l_n*56)+18] += A[499] * B[(l_n*84)+65];
    C[(l_n*56)+19] += A[500] * B[(l_n*84)+65];
    C[(l_n*56)+22] += A[501] * B[(l_n*84)+65];
    C[(l_n*56)+24] += A[502] * B[(l_n*84)+65];
    C[(l_n*56)+26] += A[503] * B[(l_n*84)+65];
    C[(l_n*56)+28] += A[504] * B[(l_n*84)+65];
    C[(l_n*56)+29] += A[505] * B[(l_n*84)+65];
    C[(l_n*56)+31] += A[506] * B[(l_n*84)+65];
    C[(l_n*56)+33] += A[507] * B[(l_n*84)+65];
    C[(l_n*56)+34] += A[508] * B[(l_n*84)+65];
    C[(l_n*56)+43] += A[509] * B[(l_n*84)+65];
    C[(l_n*56)+45] += A[510] * B[(l_n*84)+65];
    C[(l_n*56)+47] += A[511] * B[(l_n*84)+65];
    C[(l_n*56)+49] += A[512] * B[(l_n*84)+65];
    C[(l_n*56)+50] += A[513] * B[(l_n*84)+65];
    C[(l_n*56)+52] += A[514] * B[(l_n*84)+65];
    C[(l_n*56)+54] += A[515] * B[(l_n*84)+65];
    C[(l_n*56)+55] += A[516] * B[(l_n*84)+65];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b66 = _mm_broadcast_ss(&B[(l_n*84)+66]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b66 = _mm_load_ss(&B[(l_n*84)+66]);    b66 = _mm_shuffle_ps(b66, b66, 0x00);
#endif
    __m128 c66_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a66_0 = _mm_load_ss(&A[517]);
    c66_0 = _mm_add_ss(c66_0, _mm_mul_ss(a66_0, b66));
    _mm_store_ss(&C[(l_n*56)+1], c66_0);
    __m128 c66_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a66_1 = _mm_load_ss(&A[518]);
    c66_1 = _mm_add_ss(c66_1, _mm_mul_ss(a66_1, b66));
    _mm_store_ss(&C[(l_n*56)+5], c66_1);
    __m128 c66_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a66_2 = _mm_load_ss(&A[519]);
    c66_2 = _mm_add_ss(c66_2, _mm_mul_ss(a66_2, b66));
    _mm_store_ss(&C[(l_n*56)+7], c66_2);
    __m128 c66_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a66_3 = _mm_load_ss(&A[520]);
    c66_3 = _mm_add_ss(c66_3, _mm_mul_ss(a66_3, b66));
    _mm_store_ss(&C[(l_n*56)+12], c66_3);
    __m128 c66_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a66_4 = _mm_load_ss(&A[521]);
    c66_4 = _mm_add_ss(c66_4, _mm_mul_ss(a66_4, b66));
    _mm_store_ss(&C[(l_n*56)+15], c66_4);
    __m128 c66_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a66_5 = _mm_load_ss(&A[522]);
    c66_5 = _mm_add_ss(c66_5, _mm_mul_ss(a66_5, b66));
    _mm_store_ss(&C[(l_n*56)+17], c66_5);
    __m128 c66_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a66_6 = _mm_load_ss(&A[523]);
    c66_6 = _mm_add_ss(c66_6, _mm_mul_ss(a66_6, b66));
    _mm_store_ss(&C[(l_n*56)+23], c66_6);
    __m128 c66_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a66_7 = _mm_load_ss(&A[524]);
    c66_7 = _mm_add_ss(c66_7, _mm_mul_ss(a66_7, b66));
    _mm_store_ss(&C[(l_n*56)+27], c66_7);
    __m128 c66_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a66_8 = _mm_load_ss(&A[525]);
    c66_8 = _mm_add_ss(c66_8, _mm_mul_ss(a66_8, b66));
    _mm_store_ss(&C[(l_n*56)+30], c66_8);
    __m128 c66_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a66_9 = _mm_load_ss(&A[526]);
    c66_9 = _mm_add_ss(c66_9, _mm_mul_ss(a66_9, b66));
    _mm_store_ss(&C[(l_n*56)+32], c66_9);
    __m128 c66_10 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a66_10 = _mm_load_ss(&A[527]);
    c66_10 = _mm_add_ss(c66_10, _mm_mul_ss(a66_10, b66));
    _mm_store_ss(&C[(l_n*56)+44], c66_10);
    __m128 c66_11 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a66_11 = _mm_load_ss(&A[528]);
    c66_11 = _mm_add_ss(c66_11, _mm_mul_ss(a66_11, b66));
    _mm_store_ss(&C[(l_n*56)+48], c66_11);
    __m128 c66_12 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a66_12 = _mm_load_ss(&A[529]);
    c66_12 = _mm_add_ss(c66_12, _mm_mul_ss(a66_12, b66));
    _mm_store_ss(&C[(l_n*56)+51], c66_12);
    __m128 c66_13 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a66_13 = _mm_load_ss(&A[530]);
    c66_13 = _mm_add_ss(c66_13, _mm_mul_ss(a66_13, b66));
    _mm_store_ss(&C[(l_n*56)+53], c66_13);
#else
    C[(l_n*56)+1] += A[517] * B[(l_n*84)+66];
    C[(l_n*56)+5] += A[518] * B[(l_n*84)+66];
    C[(l_n*56)+7] += A[519] * B[(l_n*84)+66];
    C[(l_n*56)+12] += A[520] * B[(l_n*84)+66];
    C[(l_n*56)+15] += A[521] * B[(l_n*84)+66];
    C[(l_n*56)+17] += A[522] * B[(l_n*84)+66];
    C[(l_n*56)+23] += A[523] * B[(l_n*84)+66];
    C[(l_n*56)+27] += A[524] * B[(l_n*84)+66];
    C[(l_n*56)+30] += A[525] * B[(l_n*84)+66];
    C[(l_n*56)+32] += A[526] * B[(l_n*84)+66];
    C[(l_n*56)+44] += A[527] * B[(l_n*84)+66];
    C[(l_n*56)+48] += A[528] * B[(l_n*84)+66];
    C[(l_n*56)+51] += A[529] * B[(l_n*84)+66];
    C[(l_n*56)+53] += A[530] * B[(l_n*84)+66];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b67 = _mm_broadcast_ss(&B[(l_n*84)+67]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b67 = _mm_load_ss(&B[(l_n*84)+67]);    b67 = _mm_shuffle_ps(b67, b67, 0x00);
#endif
    __m128 c67_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a67_0 = _mm_load_ss(&A[531]);
    c67_0 = _mm_add_ss(c67_0, _mm_mul_ss(a67_0, b67));
    _mm_store_ss(&C[(l_n*56)+0], c67_0);
    __m128 c67_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a67_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[532]));
    c67_1 = _mm_add_ps(c67_1, _mm_mul_ps(a67_1, b67));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c67_1));
    __m128 c67_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a67_3 = _mm_load_ss(&A[534]);
    c67_3 = _mm_add_ss(c67_3, _mm_mul_ss(a67_3, b67));
    _mm_store_ss(&C[(l_n*56)+6], c67_3);
    __m128 c67_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a67_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[535]));
    c67_4 = _mm_add_ps(c67_4, _mm_mul_ps(a67_4, b67));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c67_4));
    __m128 c67_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a67_6 = _mm_load_ss(&A[537]);
    c67_6 = _mm_add_ss(c67_6, _mm_mul_ss(a67_6, b67));
    _mm_store_ss(&C[(l_n*56)+13], c67_6);
    __m128 c67_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a67_7 = _mm_load_ss(&A[538]);
    c67_7 = _mm_add_ss(c67_7, _mm_mul_ss(a67_7, b67));
    _mm_store_ss(&C[(l_n*56)+16], c67_7);
    __m128 c67_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a67_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[539]));
    c67_8 = _mm_add_ps(c67_8, _mm_mul_ps(a67_8, b67));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c67_8));
    __m128 c67_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a67_10 = _mm_load_ss(&A[541]);
    c67_10 = _mm_add_ss(c67_10, _mm_mul_ss(a67_10, b67));
    _mm_store_ss(&C[(l_n*56)+24], c67_10);
    __m128 c67_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a67_11 = _mm_load_ss(&A[542]);
    c67_11 = _mm_add_ss(c67_11, _mm_mul_ss(a67_11, b67));
    _mm_store_ss(&C[(l_n*56)+28], c67_11);
    __m128 c67_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a67_12 = _mm_load_ss(&A[543]);
    c67_12 = _mm_add_ss(c67_12, _mm_mul_ss(a67_12, b67));
    _mm_store_ss(&C[(l_n*56)+31], c67_12);
    __m128 c67_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a67_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[544]));
    c67_13 = _mm_add_ps(c67_13, _mm_mul_ps(a67_13, b67));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c67_13));
    __m128 c67_15 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a67_15 = _mm_load_ss(&A[546]);
    c67_15 = _mm_add_ss(c67_15, _mm_mul_ss(a67_15, b67));
    _mm_store_ss(&C[(l_n*56)+45], c67_15);
    __m128 c67_16 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a67_16 = _mm_load_ss(&A[547]);
    c67_16 = _mm_add_ss(c67_16, _mm_mul_ss(a67_16, b67));
    _mm_store_ss(&C[(l_n*56)+49], c67_16);
    __m128 c67_17 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a67_17 = _mm_load_ss(&A[548]);
    c67_17 = _mm_add_ss(c67_17, _mm_mul_ss(a67_17, b67));
    _mm_store_ss(&C[(l_n*56)+52], c67_17);
    __m128 c67_18 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a67_18 = _mm_castpd_ps(_mm_load_sd((const double*)&A[549]));
    c67_18 = _mm_add_ps(c67_18, _mm_mul_ps(a67_18, b67));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c67_18));
#else
    C[(l_n*56)+0] += A[531] * B[(l_n*84)+67];
    C[(l_n*56)+2] += A[532] * B[(l_n*84)+67];
    C[(l_n*56)+3] += A[533] * B[(l_n*84)+67];
    C[(l_n*56)+6] += A[534] * B[(l_n*84)+67];
    C[(l_n*56)+8] += A[535] * B[(l_n*84)+67];
    C[(l_n*56)+9] += A[536] * B[(l_n*84)+67];
    C[(l_n*56)+13] += A[537] * B[(l_n*84)+67];
    C[(l_n*56)+16] += A[538] * B[(l_n*84)+67];
    C[(l_n*56)+18] += A[539] * B[(l_n*84)+67];
    C[(l_n*56)+19] += A[540] * B[(l_n*84)+67];
    C[(l_n*56)+24] += A[541] * B[(l_n*84)+67];
    C[(l_n*56)+28] += A[542] * B[(l_n*84)+67];
    C[(l_n*56)+31] += A[543] * B[(l_n*84)+67];
    C[(l_n*56)+33] += A[544] * B[(l_n*84)+67];
    C[(l_n*56)+34] += A[545] * B[(l_n*84)+67];
    C[(l_n*56)+45] += A[546] * B[(l_n*84)+67];
    C[(l_n*56)+49] += A[547] * B[(l_n*84)+67];
    C[(l_n*56)+52] += A[548] * B[(l_n*84)+67];
    C[(l_n*56)+54] += A[549] * B[(l_n*84)+67];
    C[(l_n*56)+55] += A[550] * B[(l_n*84)+67];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b69 = _mm_broadcast_ss(&B[(l_n*84)+69]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b69 = _mm_load_ss(&B[(l_n*84)+69]);    b69 = _mm_shuffle_ps(b69, b69, 0x00);
#endif
    __m128 c69_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a69_0 = _mm_load_ss(&A[551]);
    c69_0 = _mm_add_ss(c69_0, _mm_mul_ss(a69_0, b69));
    _mm_store_ss(&C[(l_n*56)+1], c69_0);
    __m128 c69_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a69_1 = _mm_load_ss(&A[552]);
    c69_1 = _mm_add_ss(c69_1, _mm_mul_ss(a69_1, b69));
    _mm_store_ss(&C[(l_n*56)+5], c69_1);
    __m128 c69_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a69_2 = _mm_load_ss(&A[553]);
    c69_2 = _mm_add_ss(c69_2, _mm_mul_ss(a69_2, b69));
    _mm_store_ss(&C[(l_n*56)+7], c69_2);
    __m128 c69_3 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a69_3 = _mm_load_ss(&A[554]);
    c69_3 = _mm_add_ss(c69_3, _mm_mul_ss(a69_3, b69));
    _mm_store_ss(&C[(l_n*56)+10], c69_3);
    __m128 c69_4 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a69_4 = _mm_load_ss(&A[555]);
    c69_4 = _mm_add_ss(c69_4, _mm_mul_ss(a69_4, b69));
    _mm_store_ss(&C[(l_n*56)+12], c69_4);
    __m128 c69_5 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a69_5 = _mm_load_ss(&A[556]);
    c69_5 = _mm_add_ss(c69_5, _mm_mul_ss(a69_5, b69));
    _mm_store_ss(&C[(l_n*56)+15], c69_5);
    __m128 c69_6 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a69_6 = _mm_load_ss(&A[557]);
    c69_6 = _mm_add_ss(c69_6, _mm_mul_ss(a69_6, b69));
    _mm_store_ss(&C[(l_n*56)+17], c69_6);
    __m128 c69_7 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a69_7 = _mm_load_ss(&A[558]);
    c69_7 = _mm_add_ss(c69_7, _mm_mul_ss(a69_7, b69));
    _mm_store_ss(&C[(l_n*56)+25], c69_7);
    __m128 c69_8 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a69_8 = _mm_load_ss(&A[559]);
    c69_8 = _mm_add_ss(c69_8, _mm_mul_ss(a69_8, b69));
    _mm_store_ss(&C[(l_n*56)+27], c69_8);
    __m128 c69_9 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a69_9 = _mm_load_ss(&A[560]);
    c69_9 = _mm_add_ss(c69_9, _mm_mul_ss(a69_9, b69));
    _mm_store_ss(&C[(l_n*56)+30], c69_9);
    __m128 c69_10 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a69_10 = _mm_load_ss(&A[561]);
    c69_10 = _mm_add_ss(c69_10, _mm_mul_ss(a69_10, b69));
    _mm_store_ss(&C[(l_n*56)+32], c69_10);
    __m128 c69_11 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a69_11 = _mm_load_ss(&A[562]);
    c69_11 = _mm_add_ss(c69_11, _mm_mul_ss(a69_11, b69));
    _mm_store_ss(&C[(l_n*56)+46], c69_11);
    __m128 c69_12 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a69_12 = _mm_load_ss(&A[563]);
    c69_12 = _mm_add_ss(c69_12, _mm_mul_ss(a69_12, b69));
    _mm_store_ss(&C[(l_n*56)+48], c69_12);
    __m128 c69_13 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a69_13 = _mm_load_ss(&A[564]);
    c69_13 = _mm_add_ss(c69_13, _mm_mul_ss(a69_13, b69));
    _mm_store_ss(&C[(l_n*56)+51], c69_13);
    __m128 c69_14 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a69_14 = _mm_load_ss(&A[565]);
    c69_14 = _mm_add_ss(c69_14, _mm_mul_ss(a69_14, b69));
    _mm_store_ss(&C[(l_n*56)+53], c69_14);
#else
    C[(l_n*56)+1] += A[551] * B[(l_n*84)+69];
    C[(l_n*56)+5] += A[552] * B[(l_n*84)+69];
    C[(l_n*56)+7] += A[553] * B[(l_n*84)+69];
    C[(l_n*56)+10] += A[554] * B[(l_n*84)+69];
    C[(l_n*56)+12] += A[555] * B[(l_n*84)+69];
    C[(l_n*56)+15] += A[556] * B[(l_n*84)+69];
    C[(l_n*56)+17] += A[557] * B[(l_n*84)+69];
    C[(l_n*56)+25] += A[558] * B[(l_n*84)+69];
    C[(l_n*56)+27] += A[559] * B[(l_n*84)+69];
    C[(l_n*56)+30] += A[560] * B[(l_n*84)+69];
    C[(l_n*56)+32] += A[561] * B[(l_n*84)+69];
    C[(l_n*56)+46] += A[562] * B[(l_n*84)+69];
    C[(l_n*56)+48] += A[563] * B[(l_n*84)+69];
    C[(l_n*56)+51] += A[564] * B[(l_n*84)+69];
    C[(l_n*56)+53] += A[565] * B[(l_n*84)+69];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b70 = _mm_broadcast_ss(&B[(l_n*84)+70]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b70 = _mm_load_ss(&B[(l_n*84)+70]);    b70 = _mm_shuffle_ps(b70, b70, 0x00);
#endif
    __m128 c70_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a70_0 = _mm_load_ss(&A[566]);
    c70_0 = _mm_add_ss(c70_0, _mm_mul_ss(a70_0, b70));
    _mm_store_ss(&C[(l_n*56)+0], c70_0);
    __m128 c70_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a70_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[567]));
    c70_1 = _mm_add_ps(c70_1, _mm_mul_ps(a70_1, b70));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c70_1));
    __m128 c70_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a70_3 = _mm_load_ss(&A[569]);
    c70_3 = _mm_add_ss(c70_3, _mm_mul_ss(a70_3, b70));
    _mm_store_ss(&C[(l_n*56)+4], c70_3);
    __m128 c70_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a70_4 = _mm_load_ss(&A[570]);
    c70_4 = _mm_add_ss(c70_4, _mm_mul_ss(a70_4, b70));
    _mm_store_ss(&C[(l_n*56)+6], c70_4);
    __m128 c70_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a70_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[571]));
    c70_5 = _mm_add_ps(c70_5, _mm_mul_ps(a70_5, b70));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c70_5));
    __m128 c70_7 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a70_7 = _mm_load_ss(&A[573]);
    c70_7 = _mm_add_ss(c70_7, _mm_mul_ss(a70_7, b70));
    _mm_store_ss(&C[(l_n*56)+11], c70_7);
    __m128 c70_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+13]));
    __m128 a70_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[574]));
    c70_8 = _mm_add_ps(c70_8, _mm_mul_ps(a70_8, b70));
    _mm_store_sd((double*)&C[(l_n*56)+13], _mm_castps_pd(c70_8));
    __m128 c70_10 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a70_10 = _mm_load_ss(&A[576]);
    c70_10 = _mm_add_ss(c70_10, _mm_mul_ss(a70_10, b70));
    _mm_store_ss(&C[(l_n*56)+16], c70_10);
    __m128 c70_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a70_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[577]));
    c70_11 = _mm_add_ps(c70_11, _mm_mul_ps(a70_11, b70));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c70_11));
    __m128 c70_13 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a70_13 = _mm_load_ss(&A[579]);
    c70_13 = _mm_add_ss(c70_13, _mm_mul_ss(a70_13, b70));
    _mm_store_ss(&C[(l_n*56)+26], c70_13);
    __m128 c70_14 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+28]));
    __m128 a70_14 = _mm_castpd_ps(_mm_load_sd((const double*)&A[580]));
    c70_14 = _mm_add_ps(c70_14, _mm_mul_ps(a70_14, b70));
    _mm_store_sd((double*)&C[(l_n*56)+28], _mm_castps_pd(c70_14));
    __m128 c70_16 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a70_16 = _mm_load_ss(&A[582]);
    c70_16 = _mm_add_ss(c70_16, _mm_mul_ss(a70_16, b70));
    _mm_store_ss(&C[(l_n*56)+31], c70_16);
    __m128 c70_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a70_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[583]));
    c70_17 = _mm_add_ps(c70_17, _mm_mul_ps(a70_17, b70));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c70_17));
    __m128 c70_19 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a70_19 = _mm_load_ss(&A[585]);
    c70_19 = _mm_add_ss(c70_19, _mm_mul_ss(a70_19, b70));
    _mm_store_ss(&C[(l_n*56)+47], c70_19);
    __m128 c70_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+49]));
    __m128 a70_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[586]));
    c70_20 = _mm_add_ps(c70_20, _mm_mul_ps(a70_20, b70));
    _mm_store_sd((double*)&C[(l_n*56)+49], _mm_castps_pd(c70_20));
    __m128 c70_22 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a70_22 = _mm_load_ss(&A[588]);
    c70_22 = _mm_add_ss(c70_22, _mm_mul_ss(a70_22, b70));
    _mm_store_ss(&C[(l_n*56)+52], c70_22);
    __m128 c70_23 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a70_23 = _mm_castpd_ps(_mm_load_sd((const double*)&A[589]));
    c70_23 = _mm_add_ps(c70_23, _mm_mul_ps(a70_23, b70));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c70_23));
#else
    C[(l_n*56)+0] += A[566] * B[(l_n*84)+70];
    C[(l_n*56)+2] += A[567] * B[(l_n*84)+70];
    C[(l_n*56)+3] += A[568] * B[(l_n*84)+70];
    C[(l_n*56)+4] += A[569] * B[(l_n*84)+70];
    C[(l_n*56)+6] += A[570] * B[(l_n*84)+70];
    C[(l_n*56)+8] += A[571] * B[(l_n*84)+70];
    C[(l_n*56)+9] += A[572] * B[(l_n*84)+70];
    C[(l_n*56)+11] += A[573] * B[(l_n*84)+70];
    C[(l_n*56)+13] += A[574] * B[(l_n*84)+70];
    C[(l_n*56)+14] += A[575] * B[(l_n*84)+70];
    C[(l_n*56)+16] += A[576] * B[(l_n*84)+70];
    C[(l_n*56)+18] += A[577] * B[(l_n*84)+70];
    C[(l_n*56)+19] += A[578] * B[(l_n*84)+70];
    C[(l_n*56)+26] += A[579] * B[(l_n*84)+70];
    C[(l_n*56)+28] += A[580] * B[(l_n*84)+70];
    C[(l_n*56)+29] += A[581] * B[(l_n*84)+70];
    C[(l_n*56)+31] += A[582] * B[(l_n*84)+70];
    C[(l_n*56)+33] += A[583] * B[(l_n*84)+70];
    C[(l_n*56)+34] += A[584] * B[(l_n*84)+70];
    C[(l_n*56)+47] += A[585] * B[(l_n*84)+70];
    C[(l_n*56)+49] += A[586] * B[(l_n*84)+70];
    C[(l_n*56)+50] += A[587] * B[(l_n*84)+70];
    C[(l_n*56)+52] += A[588] * B[(l_n*84)+70];
    C[(l_n*56)+54] += A[589] * B[(l_n*84)+70];
    C[(l_n*56)+55] += A[590] * B[(l_n*84)+70];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b71 = _mm_broadcast_ss(&B[(l_n*84)+71]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b71 = _mm_load_ss(&B[(l_n*84)+71]);    b71 = _mm_shuffle_ps(b71, b71, 0x00);
#endif
    __m128 c71_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a71_0 = _mm_load_ss(&A[591]);
    c71_0 = _mm_add_ss(c71_0, _mm_mul_ss(a71_0, b71));
    _mm_store_ss(&C[(l_n*56)+1], c71_0);
    __m128 c71_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a71_1 = _mm_load_ss(&A[592]);
    c71_1 = _mm_add_ss(c71_1, _mm_mul_ss(a71_1, b71));
    _mm_store_ss(&C[(l_n*56)+5], c71_1);
    __m128 c71_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a71_2 = _mm_load_ss(&A[593]);
    c71_2 = _mm_add_ss(c71_2, _mm_mul_ss(a71_2, b71));
    _mm_store_ss(&C[(l_n*56)+7], c71_2);
    __m128 c71_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a71_3 = _mm_load_ss(&A[594]);
    c71_3 = _mm_add_ss(c71_3, _mm_mul_ss(a71_3, b71));
    _mm_store_ss(&C[(l_n*56)+12], c71_3);
    __m128 c71_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a71_4 = _mm_load_ss(&A[595]);
    c71_4 = _mm_add_ss(c71_4, _mm_mul_ss(a71_4, b71));
    _mm_store_ss(&C[(l_n*56)+15], c71_4);
    __m128 c71_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a71_5 = _mm_load_ss(&A[596]);
    c71_5 = _mm_add_ss(c71_5, _mm_mul_ss(a71_5, b71));
    _mm_store_ss(&C[(l_n*56)+17], c71_5);
    __m128 c71_6 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a71_6 = _mm_load_ss(&A[597]);
    c71_6 = _mm_add_ss(c71_6, _mm_mul_ss(a71_6, b71));
    _mm_store_ss(&C[(l_n*56)+27], c71_6);
    __m128 c71_7 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a71_7 = _mm_load_ss(&A[598]);
    c71_7 = _mm_add_ss(c71_7, _mm_mul_ss(a71_7, b71));
    _mm_store_ss(&C[(l_n*56)+30], c71_7);
    __m128 c71_8 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a71_8 = _mm_load_ss(&A[599]);
    c71_8 = _mm_add_ss(c71_8, _mm_mul_ss(a71_8, b71));
    _mm_store_ss(&C[(l_n*56)+32], c71_8);
    __m128 c71_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a71_9 = _mm_load_ss(&A[600]);
    c71_9 = _mm_add_ss(c71_9, _mm_mul_ss(a71_9, b71));
    _mm_store_ss(&C[(l_n*56)+48], c71_9);
    __m128 c71_10 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a71_10 = _mm_load_ss(&A[601]);
    c71_10 = _mm_add_ss(c71_10, _mm_mul_ss(a71_10, b71));
    _mm_store_ss(&C[(l_n*56)+51], c71_10);
    __m128 c71_11 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a71_11 = _mm_load_ss(&A[602]);
    c71_11 = _mm_add_ss(c71_11, _mm_mul_ss(a71_11, b71));
    _mm_store_ss(&C[(l_n*56)+53], c71_11);
#else
    C[(l_n*56)+1] += A[591] * B[(l_n*84)+71];
    C[(l_n*56)+5] += A[592] * B[(l_n*84)+71];
    C[(l_n*56)+7] += A[593] * B[(l_n*84)+71];
    C[(l_n*56)+12] += A[594] * B[(l_n*84)+71];
    C[(l_n*56)+15] += A[595] * B[(l_n*84)+71];
    C[(l_n*56)+17] += A[596] * B[(l_n*84)+71];
    C[(l_n*56)+27] += A[597] * B[(l_n*84)+71];
    C[(l_n*56)+30] += A[598] * B[(l_n*84)+71];
    C[(l_n*56)+32] += A[599] * B[(l_n*84)+71];
    C[(l_n*56)+48] += A[600] * B[(l_n*84)+71];
    C[(l_n*56)+51] += A[601] * B[(l_n*84)+71];
    C[(l_n*56)+53] += A[602] * B[(l_n*84)+71];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b72 = _mm_broadcast_ss(&B[(l_n*84)+72]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b72 = _mm_load_ss(&B[(l_n*84)+72]);    b72 = _mm_shuffle_ps(b72, b72, 0x00);
#endif
    __m128 c72_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a72_0 = _mm_load_ss(&A[603]);
    c72_0 = _mm_add_ss(c72_0, _mm_mul_ss(a72_0, b72));
    _mm_store_ss(&C[(l_n*56)+0], c72_0);
    __m128 c72_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a72_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[604]));
    c72_1 = _mm_add_ps(c72_1, _mm_mul_ps(a72_1, b72));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c72_1));
    __m128 c72_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a72_3 = _mm_load_ss(&A[606]);
    c72_3 = _mm_add_ss(c72_3, _mm_mul_ss(a72_3, b72));
    _mm_store_ss(&C[(l_n*56)+6], c72_3);
    __m128 c72_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a72_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[607]));
    c72_4 = _mm_add_ps(c72_4, _mm_mul_ps(a72_4, b72));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c72_4));
    __m128 c72_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a72_6 = _mm_load_ss(&A[609]);
    c72_6 = _mm_add_ss(c72_6, _mm_mul_ss(a72_6, b72));
    _mm_store_ss(&C[(l_n*56)+13], c72_6);
    __m128 c72_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a72_7 = _mm_load_ss(&A[610]);
    c72_7 = _mm_add_ss(c72_7, _mm_mul_ss(a72_7, b72));
    _mm_store_ss(&C[(l_n*56)+16], c72_7);
    __m128 c72_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a72_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[611]));
    c72_8 = _mm_add_ps(c72_8, _mm_mul_ps(a72_8, b72));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c72_8));
    __m128 c72_10 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a72_10 = _mm_load_ss(&A[613]);
    c72_10 = _mm_add_ss(c72_10, _mm_mul_ss(a72_10, b72));
    _mm_store_ss(&C[(l_n*56)+28], c72_10);
    __m128 c72_11 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a72_11 = _mm_load_ss(&A[614]);
    c72_11 = _mm_add_ss(c72_11, _mm_mul_ss(a72_11, b72));
    _mm_store_ss(&C[(l_n*56)+31], c72_11);
    __m128 c72_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a72_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[615]));
    c72_12 = _mm_add_ps(c72_12, _mm_mul_ps(a72_12, b72));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c72_12));
    __m128 c72_14 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a72_14 = _mm_load_ss(&A[617]);
    c72_14 = _mm_add_ss(c72_14, _mm_mul_ss(a72_14, b72));
    _mm_store_ss(&C[(l_n*56)+49], c72_14);
    __m128 c72_15 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a72_15 = _mm_load_ss(&A[618]);
    c72_15 = _mm_add_ss(c72_15, _mm_mul_ss(a72_15, b72));
    _mm_store_ss(&C[(l_n*56)+52], c72_15);
    __m128 c72_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a72_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[619]));
    c72_16 = _mm_add_ps(c72_16, _mm_mul_ps(a72_16, b72));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c72_16));
#else
    C[(l_n*56)+0] += A[603] * B[(l_n*84)+72];
    C[(l_n*56)+2] += A[604] * B[(l_n*84)+72];
    C[(l_n*56)+3] += A[605] * B[(l_n*84)+72];
    C[(l_n*56)+6] += A[606] * B[(l_n*84)+72];
    C[(l_n*56)+8] += A[607] * B[(l_n*84)+72];
    C[(l_n*56)+9] += A[608] * B[(l_n*84)+72];
    C[(l_n*56)+13] += A[609] * B[(l_n*84)+72];
    C[(l_n*56)+16] += A[610] * B[(l_n*84)+72];
    C[(l_n*56)+18] += A[611] * B[(l_n*84)+72];
    C[(l_n*56)+19] += A[612] * B[(l_n*84)+72];
    C[(l_n*56)+28] += A[613] * B[(l_n*84)+72];
    C[(l_n*56)+31] += A[614] * B[(l_n*84)+72];
    C[(l_n*56)+33] += A[615] * B[(l_n*84)+72];
    C[(l_n*56)+34] += A[616] * B[(l_n*84)+72];
    C[(l_n*56)+49] += A[617] * B[(l_n*84)+72];
    C[(l_n*56)+52] += A[618] * B[(l_n*84)+72];
    C[(l_n*56)+54] += A[619] * B[(l_n*84)+72];
    C[(l_n*56)+55] += A[620] * B[(l_n*84)+72];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b74 = _mm_broadcast_ss(&B[(l_n*84)+74]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b74 = _mm_load_ss(&B[(l_n*84)+74]);    b74 = _mm_shuffle_ps(b74, b74, 0x00);
#endif
    __m128 c74_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a74_0 = _mm_load_ss(&A[621]);
    c74_0 = _mm_add_ss(c74_0, _mm_mul_ss(a74_0, b74));
    _mm_store_ss(&C[(l_n*56)+0], c74_0);
    __m128 c74_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a74_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[622]));
    c74_1 = _mm_add_ps(c74_1, _mm_mul_ps(a74_1, b74));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c74_1));
    __m128 c74_3 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a74_3 = _mm_load_ss(&A[624]);
    c74_3 = _mm_add_ss(c74_3, _mm_mul_ss(a74_3, b74));
    _mm_store_ss(&C[(l_n*56)+4], c74_3);
    __m128 c74_4 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a74_4 = _mm_load_ss(&A[625]);
    c74_4 = _mm_add_ss(c74_4, _mm_mul_ss(a74_4, b74));
    _mm_store_ss(&C[(l_n*56)+6], c74_4);
    __m128 c74_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a74_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[626]));
    c74_5 = _mm_add_ps(c74_5, _mm_mul_ps(a74_5, b74));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c74_5));
    __m128 c74_7 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a74_7 = _mm_load_ss(&A[628]);
    c74_7 = _mm_add_ss(c74_7, _mm_mul_ss(a74_7, b74));
    _mm_store_ss(&C[(l_n*56)+14], c74_7);
    __m128 c74_8 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a74_8 = _mm_load_ss(&A[629]);
    c74_8 = _mm_add_ss(c74_8, _mm_mul_ss(a74_8, b74));
    _mm_store_ss(&C[(l_n*56)+16], c74_8);
    __m128 c74_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a74_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[630]));
    c74_9 = _mm_add_ps(c74_9, _mm_mul_ps(a74_9, b74));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c74_9));
    __m128 c74_11 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a74_11 = _mm_load_ss(&A[632]);
    c74_11 = _mm_add_ss(c74_11, _mm_mul_ss(a74_11, b74));
    _mm_store_ss(&C[(l_n*56)+29], c74_11);
    __m128 c74_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a74_12 = _mm_load_ss(&A[633]);
    c74_12 = _mm_add_ss(c74_12, _mm_mul_ss(a74_12, b74));
    _mm_store_ss(&C[(l_n*56)+31], c74_12);
    __m128 c74_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a74_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[634]));
    c74_13 = _mm_add_ps(c74_13, _mm_mul_ps(a74_13, b74));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c74_13));
    __m128 c74_15 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a74_15 = _mm_load_ss(&A[636]);
    c74_15 = _mm_add_ss(c74_15, _mm_mul_ss(a74_15, b74));
    _mm_store_ss(&C[(l_n*56)+50], c74_15);
    __m128 c74_16 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a74_16 = _mm_load_ss(&A[637]);
    c74_16 = _mm_add_ss(c74_16, _mm_mul_ss(a74_16, b74));
    _mm_store_ss(&C[(l_n*56)+52], c74_16);
    __m128 c74_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a74_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[638]));
    c74_17 = _mm_add_ps(c74_17, _mm_mul_ps(a74_17, b74));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c74_17));
#else
    C[(l_n*56)+0] += A[621] * B[(l_n*84)+74];
    C[(l_n*56)+2] += A[622] * B[(l_n*84)+74];
    C[(l_n*56)+3] += A[623] * B[(l_n*84)+74];
    C[(l_n*56)+4] += A[624] * B[(l_n*84)+74];
    C[(l_n*56)+6] += A[625] * B[(l_n*84)+74];
    C[(l_n*56)+8] += A[626] * B[(l_n*84)+74];
    C[(l_n*56)+9] += A[627] * B[(l_n*84)+74];
    C[(l_n*56)+14] += A[628] * B[(l_n*84)+74];
    C[(l_n*56)+16] += A[629] * B[(l_n*84)+74];
    C[(l_n*56)+18] += A[630] * B[(l_n*84)+74];
    C[(l_n*56)+19] += A[631] * B[(l_n*84)+74];
    C[(l_n*56)+29] += A[632] * B[(l_n*84)+74];
    C[(l_n*56)+31] += A[633] * B[(l_n*84)+74];
    C[(l_n*56)+33] += A[634] * B[(l_n*84)+74];
    C[(l_n*56)+34] += A[635] * B[(l_n*84)+74];
    C[(l_n*56)+50] += A[636] * B[(l_n*84)+74];
    C[(l_n*56)+52] += A[637] * B[(l_n*84)+74];
    C[(l_n*56)+54] += A[638] * B[(l_n*84)+74];
    C[(l_n*56)+55] += A[639] * B[(l_n*84)+74];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b75 = _mm_broadcast_ss(&B[(l_n*84)+75]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b75 = _mm_load_ss(&B[(l_n*84)+75]);    b75 = _mm_shuffle_ps(b75, b75, 0x00);
#endif
    __m128 c75_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a75_0 = _mm_load_ss(&A[640]);
    c75_0 = _mm_add_ss(c75_0, _mm_mul_ss(a75_0, b75));
    _mm_store_ss(&C[(l_n*56)+1], c75_0);
    __m128 c75_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a75_1 = _mm_load_ss(&A[641]);
    c75_1 = _mm_add_ss(c75_1, _mm_mul_ss(a75_1, b75));
    _mm_store_ss(&C[(l_n*56)+5], c75_1);
    __m128 c75_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a75_2 = _mm_load_ss(&A[642]);
    c75_2 = _mm_add_ss(c75_2, _mm_mul_ss(a75_2, b75));
    _mm_store_ss(&C[(l_n*56)+7], c75_2);
    __m128 c75_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a75_3 = _mm_load_ss(&A[643]);
    c75_3 = _mm_add_ss(c75_3, _mm_mul_ss(a75_3, b75));
    _mm_store_ss(&C[(l_n*56)+15], c75_3);
    __m128 c75_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a75_4 = _mm_load_ss(&A[644]);
    c75_4 = _mm_add_ss(c75_4, _mm_mul_ss(a75_4, b75));
    _mm_store_ss(&C[(l_n*56)+17], c75_4);
    __m128 c75_5 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a75_5 = _mm_load_ss(&A[645]);
    c75_5 = _mm_add_ss(c75_5, _mm_mul_ss(a75_5, b75));
    _mm_store_ss(&C[(l_n*56)+30], c75_5);
    __m128 c75_6 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a75_6 = _mm_load_ss(&A[646]);
    c75_6 = _mm_add_ss(c75_6, _mm_mul_ss(a75_6, b75));
    _mm_store_ss(&C[(l_n*56)+32], c75_6);
    __m128 c75_7 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a75_7 = _mm_load_ss(&A[647]);
    c75_7 = _mm_add_ss(c75_7, _mm_mul_ss(a75_7, b75));
    _mm_store_ss(&C[(l_n*56)+51], c75_7);
    __m128 c75_8 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a75_8 = _mm_load_ss(&A[648]);
    c75_8 = _mm_add_ss(c75_8, _mm_mul_ss(a75_8, b75));
    _mm_store_ss(&C[(l_n*56)+53], c75_8);
#else
    C[(l_n*56)+1] += A[640] * B[(l_n*84)+75];
    C[(l_n*56)+5] += A[641] * B[(l_n*84)+75];
    C[(l_n*56)+7] += A[642] * B[(l_n*84)+75];
    C[(l_n*56)+15] += A[643] * B[(l_n*84)+75];
    C[(l_n*56)+17] += A[644] * B[(l_n*84)+75];
    C[(l_n*56)+30] += A[645] * B[(l_n*84)+75];
    C[(l_n*56)+32] += A[646] * B[(l_n*84)+75];
    C[(l_n*56)+51] += A[647] * B[(l_n*84)+75];
    C[(l_n*56)+53] += A[648] * B[(l_n*84)+75];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b76 = _mm_broadcast_ss(&B[(l_n*84)+76]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b76 = _mm_load_ss(&B[(l_n*84)+76]);    b76 = _mm_shuffle_ps(b76, b76, 0x00);
#endif
    __m128 c76_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a76_0 = _mm_load_ss(&A[649]);
    c76_0 = _mm_add_ss(c76_0, _mm_mul_ss(a76_0, b76));
    _mm_store_ss(&C[(l_n*56)+0], c76_0);
    __m128 c76_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a76_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[650]));
    c76_1 = _mm_add_ps(c76_1, _mm_mul_ps(a76_1, b76));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c76_1));
    __m128 c76_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a76_3 = _mm_load_ss(&A[652]);
    c76_3 = _mm_add_ss(c76_3, _mm_mul_ss(a76_3, b76));
    _mm_store_ss(&C[(l_n*56)+6], c76_3);
    __m128 c76_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a76_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[653]));
    c76_4 = _mm_add_ps(c76_4, _mm_mul_ps(a76_4, b76));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c76_4));
    __m128 c76_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a76_6 = _mm_load_ss(&A[655]);
    c76_6 = _mm_add_ss(c76_6, _mm_mul_ss(a76_6, b76));
    _mm_store_ss(&C[(l_n*56)+16], c76_6);
    __m128 c76_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a76_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[656]));
    c76_7 = _mm_add_ps(c76_7, _mm_mul_ps(a76_7, b76));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c76_7));
    __m128 c76_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a76_9 = _mm_load_ss(&A[658]);
    c76_9 = _mm_add_ss(c76_9, _mm_mul_ss(a76_9, b76));
    _mm_store_ss(&C[(l_n*56)+31], c76_9);
    __m128 c76_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a76_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[659]));
    c76_10 = _mm_add_ps(c76_10, _mm_mul_ps(a76_10, b76));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c76_10));
    __m128 c76_12 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a76_12 = _mm_load_ss(&A[661]);
    c76_12 = _mm_add_ss(c76_12, _mm_mul_ss(a76_12, b76));
    _mm_store_ss(&C[(l_n*56)+52], c76_12);
    __m128 c76_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a76_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[662]));
    c76_13 = _mm_add_ps(c76_13, _mm_mul_ps(a76_13, b76));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c76_13));
#else
    C[(l_n*56)+0] += A[649] * B[(l_n*84)+76];
    C[(l_n*56)+2] += A[650] * B[(l_n*84)+76];
    C[(l_n*56)+3] += A[651] * B[(l_n*84)+76];
    C[(l_n*56)+6] += A[652] * B[(l_n*84)+76];
    C[(l_n*56)+8] += A[653] * B[(l_n*84)+76];
    C[(l_n*56)+9] += A[654] * B[(l_n*84)+76];
    C[(l_n*56)+16] += A[655] * B[(l_n*84)+76];
    C[(l_n*56)+18] += A[656] * B[(l_n*84)+76];
    C[(l_n*56)+19] += A[657] * B[(l_n*84)+76];
    C[(l_n*56)+31] += A[658] * B[(l_n*84)+76];
    C[(l_n*56)+33] += A[659] * B[(l_n*84)+76];
    C[(l_n*56)+34] += A[660] * B[(l_n*84)+76];
    C[(l_n*56)+52] += A[661] * B[(l_n*84)+76];
    C[(l_n*56)+54] += A[662] * B[(l_n*84)+76];
    C[(l_n*56)+55] += A[663] * B[(l_n*84)+76];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b78 = _mm_broadcast_ss(&B[(l_n*84)+78]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b78 = _mm_load_ss(&B[(l_n*84)+78]);    b78 = _mm_shuffle_ps(b78, b78, 0x00);
#endif
    __m128 c78_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a78_0 = _mm_load_ss(&A[664]);
    c78_0 = _mm_add_ss(c78_0, _mm_mul_ss(a78_0, b78));
    _mm_store_ss(&C[(l_n*56)+1], c78_0);
    __m128 c78_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a78_1 = _mm_load_ss(&A[665]);
    c78_1 = _mm_add_ss(c78_1, _mm_mul_ss(a78_1, b78));
    _mm_store_ss(&C[(l_n*56)+7], c78_1);
    __m128 c78_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a78_2 = _mm_load_ss(&A[666]);
    c78_2 = _mm_add_ss(c78_2, _mm_mul_ss(a78_2, b78));
    _mm_store_ss(&C[(l_n*56)+17], c78_2);
    __m128 c78_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a78_3 = _mm_load_ss(&A[667]);
    c78_3 = _mm_add_ss(c78_3, _mm_mul_ss(a78_3, b78));
    _mm_store_ss(&C[(l_n*56)+32], c78_3);
    __m128 c78_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a78_4 = _mm_load_ss(&A[668]);
    c78_4 = _mm_add_ss(c78_4, _mm_mul_ss(a78_4, b78));
    _mm_store_ss(&C[(l_n*56)+53], c78_4);
#else
    C[(l_n*56)+1] += A[664] * B[(l_n*84)+78];
    C[(l_n*56)+7] += A[665] * B[(l_n*84)+78];
    C[(l_n*56)+17] += A[666] * B[(l_n*84)+78];
    C[(l_n*56)+32] += A[667] * B[(l_n*84)+78];
    C[(l_n*56)+53] += A[668] * B[(l_n*84)+78];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b79 = _mm_broadcast_ss(&B[(l_n*84)+79]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b79 = _mm_load_ss(&B[(l_n*84)+79]);    b79 = _mm_shuffle_ps(b79, b79, 0x00);
#endif
    __m128 c79_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a79_0 = _mm_load_ss(&A[669]);
    c79_0 = _mm_add_ss(c79_0, _mm_mul_ss(a79_0, b79));
    _mm_store_ss(&C[(l_n*56)+0], c79_0);
    __m128 c79_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a79_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[670]));
    c79_1 = _mm_add_ps(c79_1, _mm_mul_ps(a79_1, b79));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c79_1));
    __m128 c79_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a79_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[672]));
    c79_3 = _mm_add_ps(c79_3, _mm_mul_ps(a79_3, b79));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c79_3));
    __m128 c79_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a79_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[674]));
    c79_5 = _mm_add_ps(c79_5, _mm_mul_ps(a79_5, b79));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c79_5));
    __m128 c79_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a79_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[676]));
    c79_7 = _mm_add_ps(c79_7, _mm_mul_ps(a79_7, b79));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c79_7));
    __m128 c79_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a79_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[678]));
    c79_9 = _mm_add_ps(c79_9, _mm_mul_ps(a79_9, b79));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c79_9));
#else
    C[(l_n*56)+0] += A[669] * B[(l_n*84)+79];
    C[(l_n*56)+2] += A[670] * B[(l_n*84)+79];
    C[(l_n*56)+3] += A[671] * B[(l_n*84)+79];
    C[(l_n*56)+8] += A[672] * B[(l_n*84)+79];
    C[(l_n*56)+9] += A[673] * B[(l_n*84)+79];
    C[(l_n*56)+18] += A[674] * B[(l_n*84)+79];
    C[(l_n*56)+19] += A[675] * B[(l_n*84)+79];
    C[(l_n*56)+33] += A[676] * B[(l_n*84)+79];
    C[(l_n*56)+34] += A[677] * B[(l_n*84)+79];
    C[(l_n*56)+54] += A[678] * B[(l_n*84)+79];
    C[(l_n*56)+55] += A[679] * B[(l_n*84)+79];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b81 = _mm_broadcast_ss(&B[(l_n*84)+81]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b81 = _mm_load_ss(&B[(l_n*84)+81]);    b81 = _mm_shuffle_ps(b81, b81, 0x00);
#endif
    __m128 c81_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a81_0 = _mm_load_ss(&A[680]);
    c81_0 = _mm_add_ss(c81_0, _mm_mul_ss(a81_0, b81));
    _mm_store_ss(&C[(l_n*56)+0], c81_0);
    __m128 c81_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a81_1 = _mm_load_ss(&A[681]);
    c81_1 = _mm_add_ss(c81_1, _mm_mul_ss(a81_1, b81));
    _mm_store_ss(&C[(l_n*56)+3], c81_1);
    __m128 c81_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a81_2 = _mm_load_ss(&A[682]);
    c81_2 = _mm_add_ss(c81_2, _mm_mul_ss(a81_2, b81));
    _mm_store_ss(&C[(l_n*56)+9], c81_2);
    __m128 c81_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a81_3 = _mm_load_ss(&A[683]);
    c81_3 = _mm_add_ss(c81_3, _mm_mul_ss(a81_3, b81));
    _mm_store_ss(&C[(l_n*56)+19], c81_3);
    __m128 c81_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a81_4 = _mm_load_ss(&A[684]);
    c81_4 = _mm_add_ss(c81_4, _mm_mul_ss(a81_4, b81));
    _mm_store_ss(&C[(l_n*56)+34], c81_4);
    __m128 c81_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a81_5 = _mm_load_ss(&A[685]);
    c81_5 = _mm_add_ss(c81_5, _mm_mul_ss(a81_5, b81));
    _mm_store_ss(&C[(l_n*56)+55], c81_5);
#else
    C[(l_n*56)+0] += A[680] * B[(l_n*84)+81];
    C[(l_n*56)+3] += A[681] * B[(l_n*84)+81];
    C[(l_n*56)+9] += A[682] * B[(l_n*84)+81];
    C[(l_n*56)+19] += A[683] * B[(l_n*84)+81];
    C[(l_n*56)+34] += A[684] * B[(l_n*84)+81];
    C[(l_n*56)+55] += A[685] * B[(l_n*84)+81];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 12348;
#endif
}

void ssparse_starMatrix_m56_n9_k9_ldA56_ldBna7_ldC56_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 56; l_m++) {
    C[0+l_m] += A[336+l_m] * B[0];
    C[0+l_m] += A[392+l_m] * B[1];
    C[0+l_m] += A[448+l_m] * B[2];
    C[56+l_m] += A[336+l_m] * B[3];
    C[56+l_m] += A[392+l_m] * B[4];
    C[56+l_m] += A[448+l_m] * B[5];
    C[112+l_m] += A[336+l_m] * B[6];
    C[112+l_m] += A[392+l_m] * B[7];
    C[112+l_m] += A[448+l_m] * B[8];
    C[168+l_m] += A[336+l_m] * B[9];
    C[168+l_m] += A[392+l_m] * B[10];
    C[224+l_m] += A[392+l_m] * B[11];
    C[224+l_m] += A[448+l_m] * B[12];
    C[280+l_m] += A[336+l_m] * B[13];
    C[280+l_m] += A[448+l_m] * B[14];
    C[336+l_m] += A[0+l_m] * B[15];
    C[336+l_m] += A[168+l_m] * B[16];
    C[336+l_m] += A[280+l_m] * B[17];
    C[392+l_m] += A[56+l_m] * B[18];
    C[392+l_m] += A[168+l_m] * B[19];
    C[392+l_m] += A[224+l_m] * B[20];
    C[448+l_m] += A[112+l_m] * B[21];
    C[448+l_m] += A[224+l_m] * B[22];
    C[448+l_m] += A[280+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 2688;
#endif
}

void ssparse_kXiDivMT_m35_n9_k56_ldAna7_ldB56_ldC36_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 35; l_m++) {
      C[(l_n*36)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*36)+0], c1_0);
#else
    C[(l_n*36)+0] += A[0] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*36)+1], c4_0);
#else
    C[(l_n*36)+1] += A[1] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*36)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*36)+0] += A[2] * B[(l_n*56)+5];
    C[(l_n*36)+2] += A[3] * B[(l_n*56)+5];
    C[(l_n*36)+3] += A[4] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*36)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*36)+3], c7_1);
#else
    C[(l_n*36)+0] += A[5] * B[(l_n*56)+7];
    C[(l_n*36)+3] += A[6] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*36)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*36)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*36)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*36)+0] += A[7] * B[(l_n*56)+10];
    C[(l_n*36)+2] += A[8] * B[(l_n*56)+10];
    C[(l_n*36)+3] += A[9] * B[(l_n*56)+10];
    C[(l_n*36)+4] += A[10] * B[(l_n*56)+10];
    C[(l_n*36)+6] += A[11] * B[(l_n*56)+10];
    C[(l_n*36)+8] += A[12] * B[(l_n*56)+10];
    C[(l_n*36)+9] += A[13] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*36)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*36)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*36)+7], c11_2);
#else
    C[(l_n*36)+1] += A[14] * B[(l_n*56)+11];
    C[(l_n*36)+5] += A[15] * B[(l_n*56)+11];
    C[(l_n*36)+7] += A[16] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*36)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*36)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*36)+0] += A[17] * B[(l_n*56)+12];
    C[(l_n*36)+2] += A[18] * B[(l_n*56)+12];
    C[(l_n*36)+3] += A[19] * B[(l_n*56)+12];
    C[(l_n*36)+6] += A[20] * B[(l_n*56)+12];
    C[(l_n*36)+8] += A[21] * B[(l_n*56)+12];
    C[(l_n*36)+9] += A[22] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*36)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*36)+7], c14_1);
#else
    C[(l_n*36)+1] += A[23] * B[(l_n*56)+14];
    C[(l_n*36)+7] += A[24] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*36)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*36)+0] += A[25] * B[(l_n*56)+15];
    C[(l_n*36)+2] += A[26] * B[(l_n*56)+15];
    C[(l_n*36)+3] += A[27] * B[(l_n*56)+15];
    C[(l_n*36)+8] += A[28] * B[(l_n*56)+15];
    C[(l_n*36)+9] += A[29] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*36)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*36)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*36)+9], c17_2);
#else
    C[(l_n*36)+0] += A[30] * B[(l_n*56)+17];
    C[(l_n*36)+3] += A[31] * B[(l_n*56)+17];
    C[(l_n*36)+9] += A[32] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*36)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*36)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*36)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*36)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*36)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*36)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*36)+17], c20_6);
#else
    C[(l_n*36)+1] += A[33] * B[(l_n*56)+20];
    C[(l_n*36)+5] += A[34] * B[(l_n*56)+20];
    C[(l_n*36)+7] += A[35] * B[(l_n*56)+20];
    C[(l_n*36)+10] += A[36] * B[(l_n*56)+20];
    C[(l_n*36)+12] += A[37] * B[(l_n*56)+20];
    C[(l_n*36)+15] += A[38] * B[(l_n*56)+20];
    C[(l_n*36)+17] += A[39] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*36)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*36)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*36)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*36)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*36)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*36)+0] += A[40] * B[(l_n*56)+21];
    C[(l_n*36)+2] += A[41] * B[(l_n*56)+21];
    C[(l_n*36)+3] += A[42] * B[(l_n*56)+21];
    C[(l_n*36)+4] += A[43] * B[(l_n*56)+21];
    C[(l_n*36)+6] += A[44] * B[(l_n*56)+21];
    C[(l_n*36)+8] += A[45] * B[(l_n*56)+21];
    C[(l_n*36)+9] += A[46] * B[(l_n*56)+21];
    C[(l_n*36)+11] += A[47] * B[(l_n*56)+21];
    C[(l_n*36)+13] += A[48] * B[(l_n*56)+21];
    C[(l_n*36)+14] += A[49] * B[(l_n*56)+21];
    C[(l_n*36)+16] += A[50] * B[(l_n*56)+21];
    C[(l_n*36)+18] += A[51] * B[(l_n*56)+21];
    C[(l_n*36)+19] += A[52] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*36)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*36)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*36)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*36)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*36)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*36)+17], c22_5);
#else
    C[(l_n*36)+1] += A[53] * B[(l_n*56)+22];
    C[(l_n*36)+5] += A[54] * B[(l_n*56)+22];
    C[(l_n*36)+7] += A[55] * B[(l_n*56)+22];
    C[(l_n*36)+12] += A[56] * B[(l_n*56)+22];
    C[(l_n*36)+15] += A[57] * B[(l_n*56)+22];
    C[(l_n*36)+17] += A[58] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*36)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*36)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*36)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*36)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*36)+0] += A[59] * B[(l_n*56)+23];
    C[(l_n*36)+2] += A[60] * B[(l_n*56)+23];
    C[(l_n*36)+3] += A[61] * B[(l_n*56)+23];
    C[(l_n*36)+6] += A[62] * B[(l_n*56)+23];
    C[(l_n*36)+8] += A[63] * B[(l_n*56)+23];
    C[(l_n*36)+9] += A[64] * B[(l_n*56)+23];
    C[(l_n*36)+13] += A[65] * B[(l_n*56)+23];
    C[(l_n*36)+16] += A[66] * B[(l_n*56)+23];
    C[(l_n*36)+18] += A[67] * B[(l_n*56)+23];
    C[(l_n*36)+19] += A[68] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*36)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*36)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*36)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*36)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*36)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*36)+0] += A[69] * B[(l_n*56)+25];
    C[(l_n*36)+2] += A[70] * B[(l_n*56)+25];
    C[(l_n*36)+3] += A[71] * B[(l_n*56)+25];
    C[(l_n*36)+4] += A[72] * B[(l_n*56)+25];
    C[(l_n*36)+6] += A[73] * B[(l_n*56)+25];
    C[(l_n*36)+8] += A[74] * B[(l_n*56)+25];
    C[(l_n*36)+9] += A[75] * B[(l_n*56)+25];
    C[(l_n*36)+14] += A[76] * B[(l_n*56)+25];
    C[(l_n*36)+16] += A[77] * B[(l_n*56)+25];
    C[(l_n*36)+18] += A[78] * B[(l_n*56)+25];
    C[(l_n*36)+19] += A[79] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*36)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*36)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*36)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*36)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*36)+17], c26_4);
#else
    C[(l_n*36)+1] += A[80] * B[(l_n*56)+26];
    C[(l_n*36)+5] += A[81] * B[(l_n*56)+26];
    C[(l_n*36)+7] += A[82] * B[(l_n*56)+26];
    C[(l_n*36)+15] += A[83] * B[(l_n*56)+26];
    C[(l_n*36)+17] += A[84] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*36)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*36)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*36)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*36)+0] += A[85] * B[(l_n*56)+27];
    C[(l_n*36)+2] += A[86] * B[(l_n*56)+27];
    C[(l_n*36)+3] += A[87] * B[(l_n*56)+27];
    C[(l_n*36)+6] += A[88] * B[(l_n*56)+27];
    C[(l_n*36)+8] += A[89] * B[(l_n*56)+27];
    C[(l_n*36)+9] += A[90] * B[(l_n*56)+27];
    C[(l_n*36)+16] += A[91] * B[(l_n*56)+27];
    C[(l_n*36)+18] += A[92] * B[(l_n*56)+27];
    C[(l_n*36)+19] += A[93] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*36)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*36)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*36)+17], c29_2);
#else
    C[(l_n*36)+1] += A[94] * B[(l_n*56)+29];
    C[(l_n*36)+7] += A[95] * B[(l_n*56)+29];
    C[(l_n*36)+17] += A[96] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*36)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*36)+0] += A[97] * B[(l_n*56)+30];
    C[(l_n*36)+2] += A[98] * B[(l_n*56)+30];
    C[(l_n*36)+3] += A[99] * B[(l_n*56)+30];
    C[(l_n*36)+8] += A[100] * B[(l_n*56)+30];
    C[(l_n*36)+9] += A[101] * B[(l_n*56)+30];
    C[(l_n*36)+18] += A[102] * B[(l_n*56)+30];
    C[(l_n*36)+19] += A[103] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*36)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*36)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*36)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*36)+19], c32_3);
#else
    C[(l_n*36)+0] += A[104] * B[(l_n*56)+32];
    C[(l_n*36)+3] += A[105] * B[(l_n*56)+32];
    C[(l_n*36)+9] += A[106] * B[(l_n*56)+32];
    C[(l_n*36)+19] += A[107] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a35_0 = _mm_load_ss(&A[108]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*36)+0], c35_0);
    __m128 c35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a35_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[109]));
    c35_1 = _mm_add_ps(c35_1, _mm_mul_ps(a35_1, b35));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c35_1));
    __m128 c35_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a35_3 = _mm_load_ss(&A[111]);
    c35_3 = _mm_add_ss(c35_3, _mm_mul_ss(a35_3, b35));
    _mm_store_ss(&C[(l_n*36)+4], c35_3);
    __m128 c35_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a35_4 = _mm_load_ss(&A[112]);
    c35_4 = _mm_add_ss(c35_4, _mm_mul_ss(a35_4, b35));
    _mm_store_ss(&C[(l_n*36)+6], c35_4);
    __m128 c35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a35_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[113]));
    c35_5 = _mm_add_ps(c35_5, _mm_mul_ps(a35_5, b35));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c35_5));
    __m128 c35_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a35_7 = _mm_load_ss(&A[115]);
    c35_7 = _mm_add_ss(c35_7, _mm_mul_ss(a35_7, b35));
    _mm_store_ss(&C[(l_n*36)+11], c35_7);
    __m128 c35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a35_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[116]));
    c35_8 = _mm_add_ps(c35_8, _mm_mul_ps(a35_8, b35));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c35_8));
    __m128 c35_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a35_10 = _mm_load_ss(&A[118]);
    c35_10 = _mm_add_ss(c35_10, _mm_mul_ss(a35_10, b35));
    _mm_store_ss(&C[(l_n*36)+16], c35_10);
    __m128 c35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a35_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[119]));
    c35_11 = _mm_add_ps(c35_11, _mm_mul_ps(a35_11, b35));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c35_11));
    __m128 c35_13 = _mm_load_ss(&C[(l_n*36)+20]);
    __m128 a35_13 = _mm_load_ss(&A[121]);
    c35_13 = _mm_add_ss(c35_13, _mm_mul_ss(a35_13, b35));
    _mm_store_ss(&C[(l_n*36)+20], c35_13);
    __m128 c35_14 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a35_14 = _mm_load_ss(&A[122]);
    c35_14 = _mm_add_ss(c35_14, _mm_mul_ss(a35_14, b35));
    _mm_store_ss(&C[(l_n*36)+22], c35_14);
    __m128 c35_15 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a35_15 = _mm_load_ss(&A[123]);
    c35_15 = _mm_add_ss(c35_15, _mm_mul_ss(a35_15, b35));
    _mm_store_ss(&C[(l_n*36)+24], c35_15);
    __m128 c35_16 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a35_16 = _mm_load_ss(&A[124]);
    c35_16 = _mm_add_ss(c35_16, _mm_mul_ss(a35_16, b35));
    _mm_store_ss(&C[(l_n*36)+26], c35_16);
    __m128 c35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a35_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[125]));
    c35_17 = _mm_add_ps(c35_17, _mm_mul_ps(a35_17, b35));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c35_17));
    __m128 c35_19 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a35_19 = _mm_load_ss(&A[127]);
    c35_19 = _mm_add_ss(c35_19, _mm_mul_ss(a35_19, b35));
    _mm_store_ss(&C[(l_n*36)+31], c35_19);
    __m128 c35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a35_20 = _mm_castpd_ps(_mm_load_sd((const double*)&A[128]));
    c35_20 = _mm_add_ps(c35_20, _mm_mul_ps(a35_20, b35));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c35_20));
#else
    C[(l_n*36)+0] += A[108] * B[(l_n*56)+35];
    C[(l_n*36)+2] += A[109] * B[(l_n*56)+35];
    C[(l_n*36)+3] += A[110] * B[(l_n*56)+35];
    C[(l_n*36)+4] += A[111] * B[(l_n*56)+35];
    C[(l_n*36)+6] += A[112] * B[(l_n*56)+35];
    C[(l_n*36)+8] += A[113] * B[(l_n*56)+35];
    C[(l_n*36)+9] += A[114] * B[(l_n*56)+35];
    C[(l_n*36)+11] += A[115] * B[(l_n*56)+35];
    C[(l_n*36)+13] += A[116] * B[(l_n*56)+35];
    C[(l_n*36)+14] += A[117] * B[(l_n*56)+35];
    C[(l_n*36)+16] += A[118] * B[(l_n*56)+35];
    C[(l_n*36)+18] += A[119] * B[(l_n*56)+35];
    C[(l_n*36)+19] += A[120] * B[(l_n*56)+35];
    C[(l_n*36)+20] += A[121] * B[(l_n*56)+35];
    C[(l_n*36)+22] += A[122] * B[(l_n*56)+35];
    C[(l_n*36)+24] += A[123] * B[(l_n*56)+35];
    C[(l_n*36)+26] += A[124] * B[(l_n*56)+35];
    C[(l_n*36)+28] += A[125] * B[(l_n*56)+35];
    C[(l_n*36)+29] += A[126] * B[(l_n*56)+35];
    C[(l_n*36)+31] += A[127] * B[(l_n*56)+35];
    C[(l_n*36)+33] += A[128] * B[(l_n*56)+35];
    C[(l_n*36)+34] += A[129] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a36_0 = _mm_load_ss(&A[130]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*36)+1], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a36_1 = _mm_load_ss(&A[131]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*36)+5], c36_1);
    __m128 c36_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a36_2 = _mm_load_ss(&A[132]);
    c36_2 = _mm_add_ss(c36_2, _mm_mul_ss(a36_2, b36));
    _mm_store_ss(&C[(l_n*36)+7], c36_2);
    __m128 c36_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a36_3 = _mm_load_ss(&A[133]);
    c36_3 = _mm_add_ss(c36_3, _mm_mul_ss(a36_3, b36));
    _mm_store_ss(&C[(l_n*36)+10], c36_3);
    __m128 c36_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a36_4 = _mm_load_ss(&A[134]);
    c36_4 = _mm_add_ss(c36_4, _mm_mul_ss(a36_4, b36));
    _mm_store_ss(&C[(l_n*36)+12], c36_4);
    __m128 c36_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a36_5 = _mm_load_ss(&A[135]);
    c36_5 = _mm_add_ss(c36_5, _mm_mul_ss(a36_5, b36));
    _mm_store_ss(&C[(l_n*36)+15], c36_5);
    __m128 c36_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a36_6 = _mm_load_ss(&A[136]);
    c36_6 = _mm_add_ss(c36_6, _mm_mul_ss(a36_6, b36));
    _mm_store_ss(&C[(l_n*36)+17], c36_6);
    __m128 c36_7 = _mm_load_ss(&C[(l_n*36)+21]);
    __m128 a36_7 = _mm_load_ss(&A[137]);
    c36_7 = _mm_add_ss(c36_7, _mm_mul_ss(a36_7, b36));
    _mm_store_ss(&C[(l_n*36)+21], c36_7);
    __m128 c36_8 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a36_8 = _mm_load_ss(&A[138]);
    c36_8 = _mm_add_ss(c36_8, _mm_mul_ss(a36_8, b36));
    _mm_store_ss(&C[(l_n*36)+23], c36_8);
    __m128 c36_9 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a36_9 = _mm_load_ss(&A[139]);
    c36_9 = _mm_add_ss(c36_9, _mm_mul_ss(a36_9, b36));
    _mm_store_ss(&C[(l_n*36)+25], c36_9);
    __m128 c36_10 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a36_10 = _mm_load_ss(&A[140]);
    c36_10 = _mm_add_ss(c36_10, _mm_mul_ss(a36_10, b36));
    _mm_store_ss(&C[(l_n*36)+27], c36_10);
    __m128 c36_11 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a36_11 = _mm_load_ss(&A[141]);
    c36_11 = _mm_add_ss(c36_11, _mm_mul_ss(a36_11, b36));
    _mm_store_ss(&C[(l_n*36)+30], c36_11);
    __m128 c36_12 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a36_12 = _mm_load_ss(&A[142]);
    c36_12 = _mm_add_ss(c36_12, _mm_mul_ss(a36_12, b36));
    _mm_store_ss(&C[(l_n*36)+32], c36_12);
#else
    C[(l_n*36)+1] += A[130] * B[(l_n*56)+36];
    C[(l_n*36)+5] += A[131] * B[(l_n*56)+36];
    C[(l_n*36)+7] += A[132] * B[(l_n*56)+36];
    C[(l_n*36)+10] += A[133] * B[(l_n*56)+36];
    C[(l_n*36)+12] += A[134] * B[(l_n*56)+36];
    C[(l_n*36)+15] += A[135] * B[(l_n*56)+36];
    C[(l_n*36)+17] += A[136] * B[(l_n*56)+36];
    C[(l_n*36)+21] += A[137] * B[(l_n*56)+36];
    C[(l_n*36)+23] += A[138] * B[(l_n*56)+36];
    C[(l_n*36)+25] += A[139] * B[(l_n*56)+36];
    C[(l_n*36)+27] += A[140] * B[(l_n*56)+36];
    C[(l_n*36)+30] += A[141] * B[(l_n*56)+36];
    C[(l_n*36)+32] += A[142] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a37_0 = _mm_load_ss(&A[143]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*36)+0], c37_0);
    __m128 c37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a37_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[144]));
    c37_1 = _mm_add_ps(c37_1, _mm_mul_ps(a37_1, b37));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c37_1));
    __m128 c37_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a37_3 = _mm_load_ss(&A[146]);
    c37_3 = _mm_add_ss(c37_3, _mm_mul_ss(a37_3, b37));
    _mm_store_ss(&C[(l_n*36)+4], c37_3);
    __m128 c37_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a37_4 = _mm_load_ss(&A[147]);
    c37_4 = _mm_add_ss(c37_4, _mm_mul_ss(a37_4, b37));
    _mm_store_ss(&C[(l_n*36)+6], c37_4);
    __m128 c37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a37_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[148]));
    c37_5 = _mm_add_ps(c37_5, _mm_mul_ps(a37_5, b37));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c37_5));
    __m128 c37_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a37_7 = _mm_load_ss(&A[150]);
    c37_7 = _mm_add_ss(c37_7, _mm_mul_ss(a37_7, b37));
    _mm_store_ss(&C[(l_n*36)+11], c37_7);
    __m128 c37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a37_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[151]));
    c37_8 = _mm_add_ps(c37_8, _mm_mul_ps(a37_8, b37));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c37_8));
    __m128 c37_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a37_10 = _mm_load_ss(&A[153]);
    c37_10 = _mm_add_ss(c37_10, _mm_mul_ss(a37_10, b37));
    _mm_store_ss(&C[(l_n*36)+16], c37_10);
    __m128 c37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a37_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[154]));
    c37_11 = _mm_add_ps(c37_11, _mm_mul_ps(a37_11, b37));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c37_11));
    __m128 c37_13 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a37_13 = _mm_load_ss(&A[156]);
    c37_13 = _mm_add_ss(c37_13, _mm_mul_ss(a37_13, b37));
    _mm_store_ss(&C[(l_n*36)+22], c37_13);
    __m128 c37_14 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a37_14 = _mm_load_ss(&A[157]);
    c37_14 = _mm_add_ss(c37_14, _mm_mul_ss(a37_14, b37));
    _mm_store_ss(&C[(l_n*36)+24], c37_14);
    __m128 c37_15 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a37_15 = _mm_load_ss(&A[158]);
    c37_15 = _mm_add_ss(c37_15, _mm_mul_ss(a37_15, b37));
    _mm_store_ss(&C[(l_n*36)+26], c37_15);
    __m128 c37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a37_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[159]));
    c37_16 = _mm_add_ps(c37_16, _mm_mul_ps(a37_16, b37));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c37_16));
    __m128 c37_18 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a37_18 = _mm_load_ss(&A[161]);
    c37_18 = _mm_add_ss(c37_18, _mm_mul_ss(a37_18, b37));
    _mm_store_ss(&C[(l_n*36)+31], c37_18);
    __m128 c37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a37_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[162]));
    c37_19 = _mm_add_ps(c37_19, _mm_mul_ps(a37_19, b37));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c37_19));
#else
    C[(l_n*36)+0] += A[143] * B[(l_n*56)+37];
    C[(l_n*36)+2] += A[144] * B[(l_n*56)+37];
    C[(l_n*36)+3] += A[145] * B[(l_n*56)+37];
    C[(l_n*36)+4] += A[146] * B[(l_n*56)+37];
    C[(l_n*36)+6] += A[147] * B[(l_n*56)+37];
    C[(l_n*36)+8] += A[148] * B[(l_n*56)+37];
    C[(l_n*36)+9] += A[149] * B[(l_n*56)+37];
    C[(l_n*36)+11] += A[150] * B[(l_n*56)+37];
    C[(l_n*36)+13] += A[151] * B[(l_n*56)+37];
    C[(l_n*36)+14] += A[152] * B[(l_n*56)+37];
    C[(l_n*36)+16] += A[153] * B[(l_n*56)+37];
    C[(l_n*36)+18] += A[154] * B[(l_n*56)+37];
    C[(l_n*36)+19] += A[155] * B[(l_n*56)+37];
    C[(l_n*36)+22] += A[156] * B[(l_n*56)+37];
    C[(l_n*36)+24] += A[157] * B[(l_n*56)+37];
    C[(l_n*36)+26] += A[158] * B[(l_n*56)+37];
    C[(l_n*36)+28] += A[159] * B[(l_n*56)+37];
    C[(l_n*36)+29] += A[160] * B[(l_n*56)+37];
    C[(l_n*36)+31] += A[161] * B[(l_n*56)+37];
    C[(l_n*36)+33] += A[162] * B[(l_n*56)+37];
    C[(l_n*36)+34] += A[163] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a38_0 = _mm_load_ss(&A[164]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*36)+1], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a38_1 = _mm_load_ss(&A[165]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*36)+5], c38_1);
    __m128 c38_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a38_2 = _mm_load_ss(&A[166]);
    c38_2 = _mm_add_ss(c38_2, _mm_mul_ss(a38_2, b38));
    _mm_store_ss(&C[(l_n*36)+7], c38_2);
    __m128 c38_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a38_3 = _mm_load_ss(&A[167]);
    c38_3 = _mm_add_ss(c38_3, _mm_mul_ss(a38_3, b38));
    _mm_store_ss(&C[(l_n*36)+12], c38_3);
    __m128 c38_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a38_4 = _mm_load_ss(&A[168]);
    c38_4 = _mm_add_ss(c38_4, _mm_mul_ss(a38_4, b38));
    _mm_store_ss(&C[(l_n*36)+15], c38_4);
    __m128 c38_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a38_5 = _mm_load_ss(&A[169]);
    c38_5 = _mm_add_ss(c38_5, _mm_mul_ss(a38_5, b38));
    _mm_store_ss(&C[(l_n*36)+17], c38_5);
    __m128 c38_6 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a38_6 = _mm_load_ss(&A[170]);
    c38_6 = _mm_add_ss(c38_6, _mm_mul_ss(a38_6, b38));
    _mm_store_ss(&C[(l_n*36)+23], c38_6);
    __m128 c38_7 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a38_7 = _mm_load_ss(&A[171]);
    c38_7 = _mm_add_ss(c38_7, _mm_mul_ss(a38_7, b38));
    _mm_store_ss(&C[(l_n*36)+27], c38_7);
    __m128 c38_8 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a38_8 = _mm_load_ss(&A[172]);
    c38_8 = _mm_add_ss(c38_8, _mm_mul_ss(a38_8, b38));
    _mm_store_ss(&C[(l_n*36)+30], c38_8);
    __m128 c38_9 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a38_9 = _mm_load_ss(&A[173]);
    c38_9 = _mm_add_ss(c38_9, _mm_mul_ss(a38_9, b38));
    _mm_store_ss(&C[(l_n*36)+32], c38_9);
#else
    C[(l_n*36)+1] += A[164] * B[(l_n*56)+38];
    C[(l_n*36)+5] += A[165] * B[(l_n*56)+38];
    C[(l_n*36)+7] += A[166] * B[(l_n*56)+38];
    C[(l_n*36)+12] += A[167] * B[(l_n*56)+38];
    C[(l_n*36)+15] += A[168] * B[(l_n*56)+38];
    C[(l_n*36)+17] += A[169] * B[(l_n*56)+38];
    C[(l_n*36)+23] += A[170] * B[(l_n*56)+38];
    C[(l_n*36)+27] += A[171] * B[(l_n*56)+38];
    C[(l_n*36)+30] += A[172] * B[(l_n*56)+38];
    C[(l_n*36)+32] += A[173] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a39_0 = _mm_load_ss(&A[174]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*36)+0], c39_0);
    __m128 c39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a39_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[175]));
    c39_1 = _mm_add_ps(c39_1, _mm_mul_ps(a39_1, b39));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c39_1));
    __m128 c39_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a39_3 = _mm_load_ss(&A[177]);
    c39_3 = _mm_add_ss(c39_3, _mm_mul_ss(a39_3, b39));
    _mm_store_ss(&C[(l_n*36)+6], c39_3);
    __m128 c39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[178]));
    c39_4 = _mm_add_ps(c39_4, _mm_mul_ps(a39_4, b39));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c39_4));
    __m128 c39_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a39_6 = _mm_load_ss(&A[180]);
    c39_6 = _mm_add_ss(c39_6, _mm_mul_ss(a39_6, b39));
    _mm_store_ss(&C[(l_n*36)+13], c39_6);
    __m128 c39_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a39_7 = _mm_load_ss(&A[181]);
    c39_7 = _mm_add_ss(c39_7, _mm_mul_ss(a39_7, b39));
    _mm_store_ss(&C[(l_n*36)+16], c39_7);
    __m128 c39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a39_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[182]));
    c39_8 = _mm_add_ps(c39_8, _mm_mul_ps(a39_8, b39));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c39_8));
    __m128 c39_10 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a39_10 = _mm_load_ss(&A[184]);
    c39_10 = _mm_add_ss(c39_10, _mm_mul_ss(a39_10, b39));
    _mm_store_ss(&C[(l_n*36)+24], c39_10);
    __m128 c39_11 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a39_11 = _mm_load_ss(&A[185]);
    c39_11 = _mm_add_ss(c39_11, _mm_mul_ss(a39_11, b39));
    _mm_store_ss(&C[(l_n*36)+28], c39_11);
    __m128 c39_12 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a39_12 = _mm_load_ss(&A[186]);
    c39_12 = _mm_add_ss(c39_12, _mm_mul_ss(a39_12, b39));
    _mm_store_ss(&C[(l_n*36)+31], c39_12);
    __m128 c39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a39_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[187]));
    c39_13 = _mm_add_ps(c39_13, _mm_mul_ps(a39_13, b39));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c39_13));
#else
    C[(l_n*36)+0] += A[174] * B[(l_n*56)+39];
    C[(l_n*36)+2] += A[175] * B[(l_n*56)+39];
    C[(l_n*36)+3] += A[176] * B[(l_n*56)+39];
    C[(l_n*36)+6] += A[177] * B[(l_n*56)+39];
    C[(l_n*36)+8] += A[178] * B[(l_n*56)+39];
    C[(l_n*36)+9] += A[179] * B[(l_n*56)+39];
    C[(l_n*36)+13] += A[180] * B[(l_n*56)+39];
    C[(l_n*36)+16] += A[181] * B[(l_n*56)+39];
    C[(l_n*36)+18] += A[182] * B[(l_n*56)+39];
    C[(l_n*36)+19] += A[183] * B[(l_n*56)+39];
    C[(l_n*36)+24] += A[184] * B[(l_n*56)+39];
    C[(l_n*36)+28] += A[185] * B[(l_n*56)+39];
    C[(l_n*36)+31] += A[186] * B[(l_n*56)+39];
    C[(l_n*36)+33] += A[187] * B[(l_n*56)+39];
    C[(l_n*36)+34] += A[188] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a41_0 = _mm_load_ss(&A[189]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*36)+1], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a41_1 = _mm_load_ss(&A[190]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*36)+5], c41_1);
    __m128 c41_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a41_2 = _mm_load_ss(&A[191]);
    c41_2 = _mm_add_ss(c41_2, _mm_mul_ss(a41_2, b41));
    _mm_store_ss(&C[(l_n*36)+7], c41_2);
    __m128 c41_3 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a41_3 = _mm_load_ss(&A[192]);
    c41_3 = _mm_add_ss(c41_3, _mm_mul_ss(a41_3, b41));
    _mm_store_ss(&C[(l_n*36)+10], c41_3);
    __m128 c41_4 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a41_4 = _mm_load_ss(&A[193]);
    c41_4 = _mm_add_ss(c41_4, _mm_mul_ss(a41_4, b41));
    _mm_store_ss(&C[(l_n*36)+12], c41_4);
    __m128 c41_5 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a41_5 = _mm_load_ss(&A[194]);
    c41_5 = _mm_add_ss(c41_5, _mm_mul_ss(a41_5, b41));
    _mm_store_ss(&C[(l_n*36)+15], c41_5);
    __m128 c41_6 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a41_6 = _mm_load_ss(&A[195]);
    c41_6 = _mm_add_ss(c41_6, _mm_mul_ss(a41_6, b41));
    _mm_store_ss(&C[(l_n*36)+17], c41_6);
    __m128 c41_7 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a41_7 = _mm_load_ss(&A[196]);
    c41_7 = _mm_add_ss(c41_7, _mm_mul_ss(a41_7, b41));
    _mm_store_ss(&C[(l_n*36)+25], c41_7);
    __m128 c41_8 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a41_8 = _mm_load_ss(&A[197]);
    c41_8 = _mm_add_ss(c41_8, _mm_mul_ss(a41_8, b41));
    _mm_store_ss(&C[(l_n*36)+27], c41_8);
    __m128 c41_9 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a41_9 = _mm_load_ss(&A[198]);
    c41_9 = _mm_add_ss(c41_9, _mm_mul_ss(a41_9, b41));
    _mm_store_ss(&C[(l_n*36)+30], c41_9);
    __m128 c41_10 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a41_10 = _mm_load_ss(&A[199]);
    c41_10 = _mm_add_ss(c41_10, _mm_mul_ss(a41_10, b41));
    _mm_store_ss(&C[(l_n*36)+32], c41_10);
#else
    C[(l_n*36)+1] += A[189] * B[(l_n*56)+41];
    C[(l_n*36)+5] += A[190] * B[(l_n*56)+41];
    C[(l_n*36)+7] += A[191] * B[(l_n*56)+41];
    C[(l_n*36)+10] += A[192] * B[(l_n*56)+41];
    C[(l_n*36)+12] += A[193] * B[(l_n*56)+41];
    C[(l_n*36)+15] += A[194] * B[(l_n*56)+41];
    C[(l_n*36)+17] += A[195] * B[(l_n*56)+41];
    C[(l_n*36)+25] += A[196] * B[(l_n*56)+41];
    C[(l_n*36)+27] += A[197] * B[(l_n*56)+41];
    C[(l_n*36)+30] += A[198] * B[(l_n*56)+41];
    C[(l_n*36)+32] += A[199] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a42_0 = _mm_load_ss(&A[200]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*36)+0], c42_0);
    __m128 c42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a42_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[201]));
    c42_1 = _mm_add_ps(c42_1, _mm_mul_ps(a42_1, b42));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c42_1));
    __m128 c42_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a42_3 = _mm_load_ss(&A[203]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*36)+4], c42_3);
    __m128 c42_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a42_4 = _mm_load_ss(&A[204]);
    c42_4 = _mm_add_ss(c42_4, _mm_mul_ss(a42_4, b42));
    _mm_store_ss(&C[(l_n*36)+6], c42_4);
    __m128 c42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a42_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[205]));
    c42_5 = _mm_add_ps(c42_5, _mm_mul_ps(a42_5, b42));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c42_5));
    __m128 c42_7 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a42_7 = _mm_load_ss(&A[207]);
    c42_7 = _mm_add_ss(c42_7, _mm_mul_ss(a42_7, b42));
    _mm_store_ss(&C[(l_n*36)+11], c42_7);
    __m128 c42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+13]));
    __m128 a42_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[208]));
    c42_8 = _mm_add_ps(c42_8, _mm_mul_ps(a42_8, b42));
    _mm_store_sd((double*)&C[(l_n*36)+13], _mm_castps_pd(c42_8));
    __m128 c42_10 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a42_10 = _mm_load_ss(&A[210]);
    c42_10 = _mm_add_ss(c42_10, _mm_mul_ss(a42_10, b42));
    _mm_store_ss(&C[(l_n*36)+16], c42_10);
    __m128 c42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a42_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[211]));
    c42_11 = _mm_add_ps(c42_11, _mm_mul_ps(a42_11, b42));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c42_11));
    __m128 c42_13 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a42_13 = _mm_load_ss(&A[213]);
    c42_13 = _mm_add_ss(c42_13, _mm_mul_ss(a42_13, b42));
    _mm_store_ss(&C[(l_n*36)+26], c42_13);
    __m128 c42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+28]));
    __m128 a42_14 = _mm_castpd_ps(_mm_load_sd((const double*)&A[214]));
    c42_14 = _mm_add_ps(c42_14, _mm_mul_ps(a42_14, b42));
    _mm_store_sd((double*)&C[(l_n*36)+28], _mm_castps_pd(c42_14));
    __m128 c42_16 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a42_16 = _mm_load_ss(&A[216]);
    c42_16 = _mm_add_ss(c42_16, _mm_mul_ss(a42_16, b42));
    _mm_store_ss(&C[(l_n*36)+31], c42_16);
    __m128 c42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a42_17 = _mm_castpd_ps(_mm_load_sd((const double*)&A[217]));
    c42_17 = _mm_add_ps(c42_17, _mm_mul_ps(a42_17, b42));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c42_17));
#else
    C[(l_n*36)+0] += A[200] * B[(l_n*56)+42];
    C[(l_n*36)+2] += A[201] * B[(l_n*56)+42];
    C[(l_n*36)+3] += A[202] * B[(l_n*56)+42];
    C[(l_n*36)+4] += A[203] * B[(l_n*56)+42];
    C[(l_n*36)+6] += A[204] * B[(l_n*56)+42];
    C[(l_n*36)+8] += A[205] * B[(l_n*56)+42];
    C[(l_n*36)+9] += A[206] * B[(l_n*56)+42];
    C[(l_n*36)+11] += A[207] * B[(l_n*56)+42];
    C[(l_n*36)+13] += A[208] * B[(l_n*56)+42];
    C[(l_n*36)+14] += A[209] * B[(l_n*56)+42];
    C[(l_n*36)+16] += A[210] * B[(l_n*56)+42];
    C[(l_n*36)+18] += A[211] * B[(l_n*56)+42];
    C[(l_n*36)+19] += A[212] * B[(l_n*56)+42];
    C[(l_n*36)+26] += A[213] * B[(l_n*56)+42];
    C[(l_n*36)+28] += A[214] * B[(l_n*56)+42];
    C[(l_n*36)+29] += A[215] * B[(l_n*56)+42];
    C[(l_n*36)+31] += A[216] * B[(l_n*56)+42];
    C[(l_n*36)+33] += A[217] * B[(l_n*56)+42];
    C[(l_n*36)+34] += A[218] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a43_0 = _mm_load_ss(&A[219]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*36)+1], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a43_1 = _mm_load_ss(&A[220]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*36)+5], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a43_2 = _mm_load_ss(&A[221]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*36)+7], c43_2);
    __m128 c43_3 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a43_3 = _mm_load_ss(&A[222]);
    c43_3 = _mm_add_ss(c43_3, _mm_mul_ss(a43_3, b43));
    _mm_store_ss(&C[(l_n*36)+12], c43_3);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a43_4 = _mm_load_ss(&A[223]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*36)+15], c43_4);
    __m128 c43_5 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a43_5 = _mm_load_ss(&A[224]);
    c43_5 = _mm_add_ss(c43_5, _mm_mul_ss(a43_5, b43));
    _mm_store_ss(&C[(l_n*36)+17], c43_5);
    __m128 c43_6 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a43_6 = _mm_load_ss(&A[225]);
    c43_6 = _mm_add_ss(c43_6, _mm_mul_ss(a43_6, b43));
    _mm_store_ss(&C[(l_n*36)+27], c43_6);
    __m128 c43_7 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a43_7 = _mm_load_ss(&A[226]);
    c43_7 = _mm_add_ss(c43_7, _mm_mul_ss(a43_7, b43));
    _mm_store_ss(&C[(l_n*36)+30], c43_7);
    __m128 c43_8 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a43_8 = _mm_load_ss(&A[227]);
    c43_8 = _mm_add_ss(c43_8, _mm_mul_ss(a43_8, b43));
    _mm_store_ss(&C[(l_n*36)+32], c43_8);
#else
    C[(l_n*36)+1] += A[219] * B[(l_n*56)+43];
    C[(l_n*36)+5] += A[220] * B[(l_n*56)+43];
    C[(l_n*36)+7] += A[221] * B[(l_n*56)+43];
    C[(l_n*36)+12] += A[222] * B[(l_n*56)+43];
    C[(l_n*36)+15] += A[223] * B[(l_n*56)+43];
    C[(l_n*36)+17] += A[224] * B[(l_n*56)+43];
    C[(l_n*36)+27] += A[225] * B[(l_n*56)+43];
    C[(l_n*36)+30] += A[226] * B[(l_n*56)+43];
    C[(l_n*36)+32] += A[227] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a44_0 = _mm_load_ss(&A[228]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*36)+0], c44_0);
    __m128 c44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[229]));
    c44_1 = _mm_add_ps(c44_1, _mm_mul_ps(a44_1, b44));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c44_1));
    __m128 c44_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a44_3 = _mm_load_ss(&A[231]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*36)+6], c44_3);
    __m128 c44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a44_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[232]));
    c44_4 = _mm_add_ps(c44_4, _mm_mul_ps(a44_4, b44));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c44_4));
    __m128 c44_6 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a44_6 = _mm_load_ss(&A[234]);
    c44_6 = _mm_add_ss(c44_6, _mm_mul_ss(a44_6, b44));
    _mm_store_ss(&C[(l_n*36)+13], c44_6);
    __m128 c44_7 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a44_7 = _mm_load_ss(&A[235]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*36)+16], c44_7);
    __m128 c44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a44_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[236]));
    c44_8 = _mm_add_ps(c44_8, _mm_mul_ps(a44_8, b44));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c44_8));
    __m128 c44_10 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a44_10 = _mm_load_ss(&A[238]);
    c44_10 = _mm_add_ss(c44_10, _mm_mul_ss(a44_10, b44));
    _mm_store_ss(&C[(l_n*36)+28], c44_10);
    __m128 c44_11 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a44_11 = _mm_load_ss(&A[239]);
    c44_11 = _mm_add_ss(c44_11, _mm_mul_ss(a44_11, b44));
    _mm_store_ss(&C[(l_n*36)+31], c44_11);
    __m128 c44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a44_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[240]));
    c44_12 = _mm_add_ps(c44_12, _mm_mul_ps(a44_12, b44));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c44_12));
#else
    C[(l_n*36)+0] += A[228] * B[(l_n*56)+44];
    C[(l_n*36)+2] += A[229] * B[(l_n*56)+44];
    C[(l_n*36)+3] += A[230] * B[(l_n*56)+44];
    C[(l_n*36)+6] += A[231] * B[(l_n*56)+44];
    C[(l_n*36)+8] += A[232] * B[(l_n*56)+44];
    C[(l_n*36)+9] += A[233] * B[(l_n*56)+44];
    C[(l_n*36)+13] += A[234] * B[(l_n*56)+44];
    C[(l_n*36)+16] += A[235] * B[(l_n*56)+44];
    C[(l_n*36)+18] += A[236] * B[(l_n*56)+44];
    C[(l_n*36)+19] += A[237] * B[(l_n*56)+44];
    C[(l_n*36)+28] += A[238] * B[(l_n*56)+44];
    C[(l_n*36)+31] += A[239] * B[(l_n*56)+44];
    C[(l_n*36)+33] += A[240] * B[(l_n*56)+44];
    C[(l_n*36)+34] += A[241] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a46_0 = _mm_load_ss(&A[242]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*36)+0], c46_0);
    __m128 c46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a46_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[243]));
    c46_1 = _mm_add_ps(c46_1, _mm_mul_ps(a46_1, b46));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c46_1));
    __m128 c46_3 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a46_3 = _mm_load_ss(&A[245]);
    c46_3 = _mm_add_ss(c46_3, _mm_mul_ss(a46_3, b46));
    _mm_store_ss(&C[(l_n*36)+4], c46_3);
    __m128 c46_4 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a46_4 = _mm_load_ss(&A[246]);
    c46_4 = _mm_add_ss(c46_4, _mm_mul_ss(a46_4, b46));
    _mm_store_ss(&C[(l_n*36)+6], c46_4);
    __m128 c46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a46_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[247]));
    c46_5 = _mm_add_ps(c46_5, _mm_mul_ps(a46_5, b46));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c46_5));
    __m128 c46_7 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a46_7 = _mm_load_ss(&A[249]);
    c46_7 = _mm_add_ss(c46_7, _mm_mul_ss(a46_7, b46));
    _mm_store_ss(&C[(l_n*36)+14], c46_7);
    __m128 c46_8 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a46_8 = _mm_load_ss(&A[250]);
    c46_8 = _mm_add_ss(c46_8, _mm_mul_ss(a46_8, b46));
    _mm_store_ss(&C[(l_n*36)+16], c46_8);
    __m128 c46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a46_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[251]));
    c46_9 = _mm_add_ps(c46_9, _mm_mul_ps(a46_9, b46));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c46_9));
    __m128 c46_11 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a46_11 = _mm_load_ss(&A[253]);
    c46_11 = _mm_add_ss(c46_11, _mm_mul_ss(a46_11, b46));
    _mm_store_ss(&C[(l_n*36)+29], c46_11);
    __m128 c46_12 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a46_12 = _mm_load_ss(&A[254]);
    c46_12 = _mm_add_ss(c46_12, _mm_mul_ss(a46_12, b46));
    _mm_store_ss(&C[(l_n*36)+31], c46_12);
    __m128 c46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a46_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[255]));
    c46_13 = _mm_add_ps(c46_13, _mm_mul_ps(a46_13, b46));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c46_13));
#else
    C[(l_n*36)+0] += A[242] * B[(l_n*56)+46];
    C[(l_n*36)+2] += A[243] * B[(l_n*56)+46];
    C[(l_n*36)+3] += A[244] * B[(l_n*56)+46];
    C[(l_n*36)+4] += A[245] * B[(l_n*56)+46];
    C[(l_n*36)+6] += A[246] * B[(l_n*56)+46];
    C[(l_n*36)+8] += A[247] * B[(l_n*56)+46];
    C[(l_n*36)+9] += A[248] * B[(l_n*56)+46];
    C[(l_n*36)+14] += A[249] * B[(l_n*56)+46];
    C[(l_n*36)+16] += A[250] * B[(l_n*56)+46];
    C[(l_n*36)+18] += A[251] * B[(l_n*56)+46];
    C[(l_n*36)+19] += A[252] * B[(l_n*56)+46];
    C[(l_n*36)+29] += A[253] * B[(l_n*56)+46];
    C[(l_n*36)+31] += A[254] * B[(l_n*56)+46];
    C[(l_n*36)+33] += A[255] * B[(l_n*56)+46];
    C[(l_n*36)+34] += A[256] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a47_0 = _mm_load_ss(&A[257]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*36)+1], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a47_1 = _mm_load_ss(&A[258]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*36)+5], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a47_2 = _mm_load_ss(&A[259]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*36)+7], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a47_3 = _mm_load_ss(&A[260]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*36)+15], c47_3);
    __m128 c47_4 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a47_4 = _mm_load_ss(&A[261]);
    c47_4 = _mm_add_ss(c47_4, _mm_mul_ss(a47_4, b47));
    _mm_store_ss(&C[(l_n*36)+17], c47_4);
    __m128 c47_5 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a47_5 = _mm_load_ss(&A[262]);
    c47_5 = _mm_add_ss(c47_5, _mm_mul_ss(a47_5, b47));
    _mm_store_ss(&C[(l_n*36)+30], c47_5);
    __m128 c47_6 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a47_6 = _mm_load_ss(&A[263]);
    c47_6 = _mm_add_ss(c47_6, _mm_mul_ss(a47_6, b47));
    _mm_store_ss(&C[(l_n*36)+32], c47_6);
#else
    C[(l_n*36)+1] += A[257] * B[(l_n*56)+47];
    C[(l_n*36)+5] += A[258] * B[(l_n*56)+47];
    C[(l_n*36)+7] += A[259] * B[(l_n*56)+47];
    C[(l_n*36)+15] += A[260] * B[(l_n*56)+47];
    C[(l_n*36)+17] += A[261] * B[(l_n*56)+47];
    C[(l_n*36)+30] += A[262] * B[(l_n*56)+47];
    C[(l_n*36)+32] += A[263] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a48_0 = _mm_load_ss(&A[264]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*36)+0], c48_0);
    __m128 c48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a48_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[265]));
    c48_1 = _mm_add_ps(c48_1, _mm_mul_ps(a48_1, b48));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c48_1));
    __m128 c48_3 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a48_3 = _mm_load_ss(&A[267]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*36)+6], c48_3);
    __m128 c48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a48_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[268]));
    c48_4 = _mm_add_ps(c48_4, _mm_mul_ps(a48_4, b48));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c48_4));
    __m128 c48_6 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a48_6 = _mm_load_ss(&A[270]);
    c48_6 = _mm_add_ss(c48_6, _mm_mul_ss(a48_6, b48));
    _mm_store_ss(&C[(l_n*36)+16], c48_6);
    __m128 c48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a48_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[271]));
    c48_7 = _mm_add_ps(c48_7, _mm_mul_ps(a48_7, b48));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c48_7));
    __m128 c48_9 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a48_9 = _mm_load_ss(&A[273]);
    c48_9 = _mm_add_ss(c48_9, _mm_mul_ss(a48_9, b48));
    _mm_store_ss(&C[(l_n*36)+31], c48_9);
    __m128 c48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a48_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[274]));
    c48_10 = _mm_add_ps(c48_10, _mm_mul_ps(a48_10, b48));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c48_10));
#else
    C[(l_n*36)+0] += A[264] * B[(l_n*56)+48];
    C[(l_n*36)+2] += A[265] * B[(l_n*56)+48];
    C[(l_n*36)+3] += A[266] * B[(l_n*56)+48];
    C[(l_n*36)+6] += A[267] * B[(l_n*56)+48];
    C[(l_n*36)+8] += A[268] * B[(l_n*56)+48];
    C[(l_n*36)+9] += A[269] * B[(l_n*56)+48];
    C[(l_n*36)+16] += A[270] * B[(l_n*56)+48];
    C[(l_n*36)+18] += A[271] * B[(l_n*56)+48];
    C[(l_n*36)+19] += A[272] * B[(l_n*56)+48];
    C[(l_n*36)+31] += A[273] * B[(l_n*56)+48];
    C[(l_n*36)+33] += A[274] * B[(l_n*56)+48];
    C[(l_n*36)+34] += A[275] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a50_0 = _mm_load_ss(&A[276]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*36)+1], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a50_1 = _mm_load_ss(&A[277]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*36)+7], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a50_2 = _mm_load_ss(&A[278]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*36)+17], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a50_3 = _mm_load_ss(&A[279]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*36)+32], c50_3);
#else
    C[(l_n*36)+1] += A[276] * B[(l_n*56)+50];
    C[(l_n*36)+7] += A[277] * B[(l_n*56)+50];
    C[(l_n*36)+17] += A[278] * B[(l_n*56)+50];
    C[(l_n*36)+32] += A[279] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a51_0 = _mm_load_ss(&A[280]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*36)+0], c51_0);
    __m128 c51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+2]));
    __m128 a51_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[281]));
    c51_1 = _mm_add_ps(c51_1, _mm_mul_ps(a51_1, b51));
    _mm_store_sd((double*)&C[(l_n*36)+2], _mm_castps_pd(c51_1));
    __m128 c51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+8]));
    __m128 a51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[283]));
    c51_3 = _mm_add_ps(c51_3, _mm_mul_ps(a51_3, b51));
    _mm_store_sd((double*)&C[(l_n*36)+8], _mm_castps_pd(c51_3));
    __m128 c51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+18]));
    __m128 a51_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[285]));
    c51_5 = _mm_add_ps(c51_5, _mm_mul_ps(a51_5, b51));
    _mm_store_sd((double*)&C[(l_n*36)+18], _mm_castps_pd(c51_5));
    __m128 c51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+33]));
    __m128 a51_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[287]));
    c51_7 = _mm_add_ps(c51_7, _mm_mul_ps(a51_7, b51));
    _mm_store_sd((double*)&C[(l_n*36)+33], _mm_castps_pd(c51_7));
#else
    C[(l_n*36)+0] += A[280] * B[(l_n*56)+51];
    C[(l_n*36)+2] += A[281] * B[(l_n*56)+51];
    C[(l_n*36)+3] += A[282] * B[(l_n*56)+51];
    C[(l_n*36)+8] += A[283] * B[(l_n*56)+51];
    C[(l_n*36)+9] += A[284] * B[(l_n*56)+51];
    C[(l_n*36)+18] += A[285] * B[(l_n*56)+51];
    C[(l_n*36)+19] += A[286] * B[(l_n*56)+51];
    C[(l_n*36)+33] += A[287] * B[(l_n*56)+51];
    C[(l_n*36)+34] += A[288] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a53_0 = _mm_load_ss(&A[289]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*36)+0], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a53_1 = _mm_load_ss(&A[290]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*36)+3], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a53_2 = _mm_load_ss(&A[291]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*36)+9], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a53_3 = _mm_load_ss(&A[292]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*36)+19], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a53_4 = _mm_load_ss(&A[293]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*36)+34], c53_4);
#else
    C[(l_n*36)+0] += A[289] * B[(l_n*56)+53];
    C[(l_n*36)+3] += A[290] * B[(l_n*56)+53];
    C[(l_n*36)+9] += A[291] * B[(l_n*56)+53];
    C[(l_n*36)+19] += A[292] * B[(l_n*56)+53];
    C[(l_n*36)+34] += A[293] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 5292;
#endif
}

void ssparse_starMatrix_m35_n9_k9_ldA36_ldBna7_ldC36_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 35; l_m++) {
    C[0+l_m] += A[216+l_m] * B[0];
    C[0+l_m] += A[252+l_m] * B[1];
    C[0+l_m] += A[288+l_m] * B[2];
    C[36+l_m] += A[216+l_m] * B[3];
    C[36+l_m] += A[252+l_m] * B[4];
    C[36+l_m] += A[288+l_m] * B[5];
    C[72+l_m] += A[216+l_m] * B[6];
    C[72+l_m] += A[252+l_m] * B[7];
    C[72+l_m] += A[288+l_m] * B[8];
    C[108+l_m] += A[216+l_m] * B[9];
    C[108+l_m] += A[252+l_m] * B[10];
    C[144+l_m] += A[252+l_m] * B[11];
    C[144+l_m] += A[288+l_m] * B[12];
    C[180+l_m] += A[216+l_m] * B[13];
    C[180+l_m] += A[288+l_m] * B[14];
    C[216+l_m] += A[0+l_m] * B[15];
    C[216+l_m] += A[108+l_m] * B[16];
    C[216+l_m] += A[180+l_m] * B[17];
    C[252+l_m] += A[36+l_m] * B[18];
    C[252+l_m] += A[108+l_m] * B[19];
    C[252+l_m] += A[144+l_m] * B[20];
    C[288+l_m] += A[72+l_m] * B[21];
    C[288+l_m] += A[144+l_m] * B[22];
    C[288+l_m] += A[180+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1680;
#endif
}

void ssparse_kXiDivMT_m20_n9_k35_ldAna7_ldB36_ldC20_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 20; l_m++) {
      C[(l_n*20)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*20)+0], c1_0);
#else
    C[(l_n*20)+0] += A[0] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*20)+1], c4_0);
#else
    C[(l_n*20)+1] += A[1] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*20)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*20)+0] += A[2] * B[(l_n*36)+5];
    C[(l_n*20)+2] += A[3] * B[(l_n*36)+5];
    C[(l_n*20)+3] += A[4] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*20)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*20)+3], c7_1);
#else
    C[(l_n*20)+0] += A[5] * B[(l_n*36)+7];
    C[(l_n*20)+3] += A[6] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*20)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*20)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*20)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*20)+0] += A[7] * B[(l_n*36)+10];
    C[(l_n*20)+2] += A[8] * B[(l_n*36)+10];
    C[(l_n*20)+3] += A[9] * B[(l_n*36)+10];
    C[(l_n*20)+4] += A[10] * B[(l_n*36)+10];
    C[(l_n*20)+6] += A[11] * B[(l_n*36)+10];
    C[(l_n*20)+8] += A[12] * B[(l_n*36)+10];
    C[(l_n*20)+9] += A[13] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*20)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*20)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*20)+7], c11_2);
#else
    C[(l_n*20)+1] += A[14] * B[(l_n*36)+11];
    C[(l_n*20)+5] += A[15] * B[(l_n*36)+11];
    C[(l_n*20)+7] += A[16] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*20)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*20)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*20)+0] += A[17] * B[(l_n*36)+12];
    C[(l_n*20)+2] += A[18] * B[(l_n*36)+12];
    C[(l_n*20)+3] += A[19] * B[(l_n*36)+12];
    C[(l_n*20)+6] += A[20] * B[(l_n*36)+12];
    C[(l_n*20)+8] += A[21] * B[(l_n*36)+12];
    C[(l_n*20)+9] += A[22] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*20)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*20)+7], c14_1);
#else
    C[(l_n*20)+1] += A[23] * B[(l_n*36)+14];
    C[(l_n*20)+7] += A[24] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*20)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*20)+0] += A[25] * B[(l_n*36)+15];
    C[(l_n*20)+2] += A[26] * B[(l_n*36)+15];
    C[(l_n*20)+3] += A[27] * B[(l_n*36)+15];
    C[(l_n*20)+8] += A[28] * B[(l_n*36)+15];
    C[(l_n*20)+9] += A[29] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*20)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*20)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*20)+9], c17_2);
#else
    C[(l_n*20)+0] += A[30] * B[(l_n*36)+17];
    C[(l_n*20)+3] += A[31] * B[(l_n*36)+17];
    C[(l_n*20)+9] += A[32] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a20_0 = _mm_load_ss(&A[33]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*20)+1], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a20_1 = _mm_load_ss(&A[34]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*20)+5], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a20_2 = _mm_load_ss(&A[35]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*20)+7], c20_2);
    __m128 c20_3 = _mm_load_ss(&C[(l_n*20)+10]);
    __m128 a20_3 = _mm_load_ss(&A[36]);
    c20_3 = _mm_add_ss(c20_3, _mm_mul_ss(a20_3, b20));
    _mm_store_ss(&C[(l_n*20)+10], c20_3);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a20_4 = _mm_load_ss(&A[37]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*20)+12], c20_4);
    __m128 c20_5 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a20_5 = _mm_load_ss(&A[38]);
    c20_5 = _mm_add_ss(c20_5, _mm_mul_ss(a20_5, b20));
    _mm_store_ss(&C[(l_n*20)+15], c20_5);
    __m128 c20_6 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a20_6 = _mm_load_ss(&A[39]);
    c20_6 = _mm_add_ss(c20_6, _mm_mul_ss(a20_6, b20));
    _mm_store_ss(&C[(l_n*20)+17], c20_6);
#else
    C[(l_n*20)+1] += A[33] * B[(l_n*36)+20];
    C[(l_n*20)+5] += A[34] * B[(l_n*36)+20];
    C[(l_n*20)+7] += A[35] * B[(l_n*36)+20];
    C[(l_n*20)+10] += A[36] * B[(l_n*36)+20];
    C[(l_n*20)+12] += A[37] * B[(l_n*36)+20];
    C[(l_n*20)+15] += A[38] * B[(l_n*36)+20];
    C[(l_n*20)+17] += A[39] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a21_0 = _mm_load_ss(&A[40]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*20)+0], c21_0);
    __m128 c21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a21_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c21_1 = _mm_add_ps(c21_1, _mm_mul_ps(a21_1, b21));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c21_1));
    __m128 c21_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a21_3 = _mm_load_ss(&A[43]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*20)+4], c21_3);
    __m128 c21_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a21_4 = _mm_load_ss(&A[44]);
    c21_4 = _mm_add_ss(c21_4, _mm_mul_ss(a21_4, b21));
    _mm_store_ss(&C[(l_n*20)+6], c21_4);
    __m128 c21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a21_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[45]));
    c21_5 = _mm_add_ps(c21_5, _mm_mul_ps(a21_5, b21));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c21_5));
    __m128 c21_7 = _mm_load_ss(&C[(l_n*20)+11]);
    __m128 a21_7 = _mm_load_ss(&A[47]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*20)+11], c21_7);
    __m128 c21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+13]));
    __m128 a21_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[48]));
    c21_8 = _mm_add_ps(c21_8, _mm_mul_ps(a21_8, b21));
    _mm_store_sd((double*)&C[(l_n*20)+13], _mm_castps_pd(c21_8));
    __m128 c21_10 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a21_10 = _mm_load_ss(&A[50]);
    c21_10 = _mm_add_ss(c21_10, _mm_mul_ss(a21_10, b21));
    _mm_store_ss(&C[(l_n*20)+16], c21_10);
    __m128 c21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a21_11 = _mm_castpd_ps(_mm_load_sd((const double*)&A[51]));
    c21_11 = _mm_add_ps(c21_11, _mm_mul_ps(a21_11, b21));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c21_11));
#else
    C[(l_n*20)+0] += A[40] * B[(l_n*36)+21];
    C[(l_n*20)+2] += A[41] * B[(l_n*36)+21];
    C[(l_n*20)+3] += A[42] * B[(l_n*36)+21];
    C[(l_n*20)+4] += A[43] * B[(l_n*36)+21];
    C[(l_n*20)+6] += A[44] * B[(l_n*36)+21];
    C[(l_n*20)+8] += A[45] * B[(l_n*36)+21];
    C[(l_n*20)+9] += A[46] * B[(l_n*36)+21];
    C[(l_n*20)+11] += A[47] * B[(l_n*36)+21];
    C[(l_n*20)+13] += A[48] * B[(l_n*36)+21];
    C[(l_n*20)+14] += A[49] * B[(l_n*36)+21];
    C[(l_n*20)+16] += A[50] * B[(l_n*36)+21];
    C[(l_n*20)+18] += A[51] * B[(l_n*36)+21];
    C[(l_n*20)+19] += A[52] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a22_0 = _mm_load_ss(&A[53]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*20)+1], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a22_1 = _mm_load_ss(&A[54]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*20)+5], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a22_2 = _mm_load_ss(&A[55]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*20)+7], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a22_3 = _mm_load_ss(&A[56]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*20)+12], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a22_4 = _mm_load_ss(&A[57]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*20)+15], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a22_5 = _mm_load_ss(&A[58]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*20)+17], c22_5);
#else
    C[(l_n*20)+1] += A[53] * B[(l_n*36)+22];
    C[(l_n*20)+5] += A[54] * B[(l_n*36)+22];
    C[(l_n*20)+7] += A[55] * B[(l_n*36)+22];
    C[(l_n*20)+12] += A[56] * B[(l_n*36)+22];
    C[(l_n*20)+15] += A[57] * B[(l_n*36)+22];
    C[(l_n*20)+17] += A[58] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a23_0 = _mm_load_ss(&A[59]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*20)+0], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[60]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a23_3 = _mm_load_ss(&A[62]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*20)+6], c23_3);
    __m128 c23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a23_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c23_4 = _mm_add_ps(c23_4, _mm_mul_ps(a23_4, b23));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c23_4));
    __m128 c23_6 = _mm_load_ss(&C[(l_n*20)+13]);
    __m128 a23_6 = _mm_load_ss(&A[65]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*20)+13], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a23_7 = _mm_load_ss(&A[66]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*20)+16], c23_7);
    __m128 c23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a23_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c23_8 = _mm_add_ps(c23_8, _mm_mul_ps(a23_8, b23));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c23_8));
#else
    C[(l_n*20)+0] += A[59] * B[(l_n*36)+23];
    C[(l_n*20)+2] += A[60] * B[(l_n*36)+23];
    C[(l_n*20)+3] += A[61] * B[(l_n*36)+23];
    C[(l_n*20)+6] += A[62] * B[(l_n*36)+23];
    C[(l_n*20)+8] += A[63] * B[(l_n*36)+23];
    C[(l_n*20)+9] += A[64] * B[(l_n*36)+23];
    C[(l_n*20)+13] += A[65] * B[(l_n*36)+23];
    C[(l_n*20)+16] += A[66] * B[(l_n*36)+23];
    C[(l_n*20)+18] += A[67] * B[(l_n*36)+23];
    C[(l_n*20)+19] += A[68] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a25_0 = _mm_load_ss(&A[69]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*20)+0], c25_0);
    __m128 c25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a25_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c25_1 = _mm_add_ps(c25_1, _mm_mul_ps(a25_1, b25));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c25_1));
    __m128 c25_3 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a25_3 = _mm_load_ss(&A[72]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*20)+4], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a25_4 = _mm_load_ss(&A[73]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*20)+6], c25_4);
    __m128 c25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a25_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c25_5 = _mm_add_ps(c25_5, _mm_mul_ps(a25_5, b25));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c25_5));
    __m128 c25_7 = _mm_load_ss(&C[(l_n*20)+14]);
    __m128 a25_7 = _mm_load_ss(&A[76]);
    c25_7 = _mm_add_ss(c25_7, _mm_mul_ss(a25_7, b25));
    _mm_store_ss(&C[(l_n*20)+14], c25_7);
    __m128 c25_8 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a25_8 = _mm_load_ss(&A[77]);
    c25_8 = _mm_add_ss(c25_8, _mm_mul_ss(a25_8, b25));
    _mm_store_ss(&C[(l_n*20)+16], c25_8);
    __m128 c25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a25_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c25_9 = _mm_add_ps(c25_9, _mm_mul_ps(a25_9, b25));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c25_9));
#else
    C[(l_n*20)+0] += A[69] * B[(l_n*36)+25];
    C[(l_n*20)+2] += A[70] * B[(l_n*36)+25];
    C[(l_n*20)+3] += A[71] * B[(l_n*36)+25];
    C[(l_n*20)+4] += A[72] * B[(l_n*36)+25];
    C[(l_n*20)+6] += A[73] * B[(l_n*36)+25];
    C[(l_n*20)+8] += A[74] * B[(l_n*36)+25];
    C[(l_n*20)+9] += A[75] * B[(l_n*36)+25];
    C[(l_n*20)+14] += A[76] * B[(l_n*36)+25];
    C[(l_n*20)+16] += A[77] * B[(l_n*36)+25];
    C[(l_n*20)+18] += A[78] * B[(l_n*36)+25];
    C[(l_n*20)+19] += A[79] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a26_0 = _mm_load_ss(&A[80]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*20)+1], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a26_1 = _mm_load_ss(&A[81]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*20)+5], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a26_2 = _mm_load_ss(&A[82]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*20)+7], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a26_3 = _mm_load_ss(&A[83]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*20)+15], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a26_4 = _mm_load_ss(&A[84]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*20)+17], c26_4);
#else
    C[(l_n*20)+1] += A[80] * B[(l_n*36)+26];
    C[(l_n*20)+5] += A[81] * B[(l_n*36)+26];
    C[(l_n*20)+7] += A[82] * B[(l_n*36)+26];
    C[(l_n*20)+15] += A[83] * B[(l_n*36)+26];
    C[(l_n*20)+17] += A[84] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a27_0 = _mm_load_ss(&A[85]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*20)+0], c27_0);
    __m128 c27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a27_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c27_1 = _mm_add_ps(c27_1, _mm_mul_ps(a27_1, b27));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c27_1));
    __m128 c27_3 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a27_3 = _mm_load_ss(&A[88]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*20)+6], c27_3);
    __m128 c27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a27_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[89]));
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c27_4));
    __m128 c27_6 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a27_6 = _mm_load_ss(&A[91]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*20)+16], c27_6);
    __m128 c27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a27_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[92]));
    c27_7 = _mm_add_ps(c27_7, _mm_mul_ps(a27_7, b27));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c27_7));
#else
    C[(l_n*20)+0] += A[85] * B[(l_n*36)+27];
    C[(l_n*20)+2] += A[86] * B[(l_n*36)+27];
    C[(l_n*20)+3] += A[87] * B[(l_n*36)+27];
    C[(l_n*20)+6] += A[88] * B[(l_n*36)+27];
    C[(l_n*20)+8] += A[89] * B[(l_n*36)+27];
    C[(l_n*20)+9] += A[90] * B[(l_n*36)+27];
    C[(l_n*20)+16] += A[91] * B[(l_n*36)+27];
    C[(l_n*20)+18] += A[92] * B[(l_n*36)+27];
    C[(l_n*20)+19] += A[93] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a29_0 = _mm_load_ss(&A[94]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*20)+1], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a29_1 = _mm_load_ss(&A[95]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*20)+7], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a29_2 = _mm_load_ss(&A[96]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*20)+17], c29_2);
#else
    C[(l_n*20)+1] += A[94] * B[(l_n*36)+29];
    C[(l_n*20)+7] += A[95] * B[(l_n*36)+29];
    C[(l_n*20)+17] += A[96] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a30_0 = _mm_load_ss(&A[97]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*20)+0], c30_0);
    __m128 c30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+2]));
    __m128 a30_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[98]));
    c30_1 = _mm_add_ps(c30_1, _mm_mul_ps(a30_1, b30));
    _mm_store_sd((double*)&C[(l_n*20)+2], _mm_castps_pd(c30_1));
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+8]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[100]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*20)+8], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*20)+18]));
    __m128 a30_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[102]));
    c30_5 = _mm_add_ps(c30_5, _mm_mul_ps(a30_5, b30));
    _mm_store_sd((double*)&C[(l_n*20)+18], _mm_castps_pd(c30_5));
#else
    C[(l_n*20)+0] += A[97] * B[(l_n*36)+30];
    C[(l_n*20)+2] += A[98] * B[(l_n*36)+30];
    C[(l_n*20)+3] += A[99] * B[(l_n*36)+30];
    C[(l_n*20)+8] += A[100] * B[(l_n*36)+30];
    C[(l_n*20)+9] += A[101] * B[(l_n*36)+30];
    C[(l_n*20)+18] += A[102] * B[(l_n*36)+30];
    C[(l_n*20)+19] += A[103] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a32_0 = _mm_load_ss(&A[104]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*20)+0], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a32_1 = _mm_load_ss(&A[105]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*20)+3], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a32_2 = _mm_load_ss(&A[106]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*20)+9], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a32_3 = _mm_load_ss(&A[107]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*20)+19], c32_3);
#else
    C[(l_n*20)+0] += A[104] * B[(l_n*36)+32];
    C[(l_n*20)+3] += A[105] * B[(l_n*36)+32];
    C[(l_n*20)+9] += A[106] * B[(l_n*36)+32];
    C[(l_n*20)+19] += A[107] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1944;
#endif
}

void ssparse_starMatrix_m20_n9_k9_ldA20_ldBna7_ldC20_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 20; l_m++) {
    C[0+l_m] += A[120+l_m] * B[0];
    C[0+l_m] += A[140+l_m] * B[1];
    C[0+l_m] += A[160+l_m] * B[2];
    C[20+l_m] += A[120+l_m] * B[3];
    C[20+l_m] += A[140+l_m] * B[4];
    C[20+l_m] += A[160+l_m] * B[5];
    C[40+l_m] += A[120+l_m] * B[6];
    C[40+l_m] += A[140+l_m] * B[7];
    C[40+l_m] += A[160+l_m] * B[8];
    C[60+l_m] += A[120+l_m] * B[9];
    C[60+l_m] += A[140+l_m] * B[10];
    C[80+l_m] += A[140+l_m] * B[11];
    C[80+l_m] += A[160+l_m] * B[12];
    C[100+l_m] += A[120+l_m] * B[13];
    C[100+l_m] += A[160+l_m] * B[14];
    C[120+l_m] += A[0+l_m] * B[15];
    C[120+l_m] += A[60+l_m] * B[16];
    C[120+l_m] += A[100+l_m] * B[17];
    C[140+l_m] += A[20+l_m] * B[18];
    C[140+l_m] += A[60+l_m] * B[19];
    C[140+l_m] += A[80+l_m] * B[20];
    C[160+l_m] += A[40+l_m] * B[21];
    C[160+l_m] += A[80+l_m] * B[22];
    C[160+l_m] += A[100+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 960;
#endif
}

void ssparse_kXiDivMT_m10_n9_k20_ldAna7_ldB20_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*20)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+0], c1_0);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*20)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*20)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+1], c4_0);
#else
    C[(l_n*12)+1] += A[1] * B[(l_n*20)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*20)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*12)+0] += A[2] * B[(l_n*20)+5];
    C[(l_n*12)+2] += A[3] * B[(l_n*20)+5];
    C[(l_n*12)+3] += A[4] * B[(l_n*20)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*20)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+3], c7_1);
#else
    C[(l_n*12)+0] += A[5] * B[(l_n*20)+7];
    C[(l_n*12)+3] += A[6] * B[(l_n*20)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*20)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a10_0 = _mm_load_ss(&A[7]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*12)+0], c10_0);
    __m128 c10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a10_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c10_1 = _mm_add_ps(c10_1, _mm_mul_ps(a10_1, b10));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c10_1));
    __m128 c10_3 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a10_3 = _mm_load_ss(&A[10]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*12)+4], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a10_4 = _mm_load_ss(&A[11]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*12)+6], c10_4);
    __m128 c10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a10_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c10_5 = _mm_add_ps(c10_5, _mm_mul_ps(a10_5, b10));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c10_5));
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*20)+10];
    C[(l_n*12)+2] += A[8] * B[(l_n*20)+10];
    C[(l_n*12)+3] += A[9] * B[(l_n*20)+10];
    C[(l_n*12)+4] += A[10] * B[(l_n*20)+10];
    C[(l_n*12)+6] += A[11] * B[(l_n*20)+10];
    C[(l_n*12)+8] += A[12] * B[(l_n*20)+10];
    C[(l_n*12)+9] += A[13] * B[(l_n*20)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*20)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a11_0 = _mm_load_ss(&A[14]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*12)+1], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a11_1 = _mm_load_ss(&A[15]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*12)+5], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a11_2 = _mm_load_ss(&A[16]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*12)+7], c11_2);
#else
    C[(l_n*12)+1] += A[14] * B[(l_n*20)+11];
    C[(l_n*12)+5] += A[15] * B[(l_n*20)+11];
    C[(l_n*12)+7] += A[16] * B[(l_n*20)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*20)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a12_0 = _mm_load_ss(&A[17]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*12)+0], c12_0);
    __m128 c12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a12_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c12_1 = _mm_add_ps(c12_1, _mm_mul_ps(a12_1, b12));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c12_1));
    __m128 c12_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a12_3 = _mm_load_ss(&A[20]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*12)+6], c12_3);
    __m128 c12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a12_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[21]));
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c12_4));
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*20)+12];
    C[(l_n*12)+2] += A[18] * B[(l_n*20)+12];
    C[(l_n*12)+3] += A[19] * B[(l_n*20)+12];
    C[(l_n*12)+6] += A[20] * B[(l_n*20)+12];
    C[(l_n*12)+8] += A[21] * B[(l_n*20)+12];
    C[(l_n*12)+9] += A[22] * B[(l_n*20)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*20)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a14_0 = _mm_load_ss(&A[23]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*12)+1], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a14_1 = _mm_load_ss(&A[24]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*12)+7], c14_1);
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*20)+14];
    C[(l_n*12)+7] += A[24] * B[(l_n*20)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*20)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a15_0 = _mm_load_ss(&A[25]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*12)+0], c15_0);
    __m128 c15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a15_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c15_1 = _mm_add_ps(c15_1, _mm_mul_ps(a15_1, b15));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c15_1));
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c15_3));
#else
    C[(l_n*12)+0] += A[25] * B[(l_n*20)+15];
    C[(l_n*12)+2] += A[26] * B[(l_n*20)+15];
    C[(l_n*12)+3] += A[27] * B[(l_n*20)+15];
    C[(l_n*12)+8] += A[28] * B[(l_n*20)+15];
    C[(l_n*12)+9] += A[29] * B[(l_n*20)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*20)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a17_0 = _mm_load_ss(&A[30]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*12)+0], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a17_1 = _mm_load_ss(&A[31]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*12)+3], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a17_2 = _mm_load_ss(&A[32]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*12)+9], c17_2);
#else
    C[(l_n*12)+0] += A[30] * B[(l_n*20)+17];
    C[(l_n*12)+3] += A[31] * B[(l_n*20)+17];
    C[(l_n*12)+9] += A[32] * B[(l_n*20)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 594;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna7_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_kXiDivMT_m4_n9_k10_ldAna7_ldB12_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 4; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*4)+1]);
    __m128 a4_0 = _mm_load_ss(&A[1]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*4)+1], c4_0);
#else
    C[(l_n*4)+1] += A[1] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a5_0 = _mm_load_ss(&A[2]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*4)+0], c5_0);
    __m128 c5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*4)+2]));
    __m128 a5_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c5_1 = _mm_add_ps(c5_1, _mm_mul_ps(a5_1, b5));
    _mm_store_sd((double*)&C[(l_n*4)+2], _mm_castps_pd(c5_1));
#else
    C[(l_n*4)+0] += A[2] * B[(l_n*12)+5];
    C[(l_n*4)+2] += A[3] * B[(l_n*12)+5];
    C[(l_n*4)+3] += A[4] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a7_0 = _mm_load_ss(&A[5]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*4)+0], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*4)+3]);
    __m128 a7_1 = _mm_load_ss(&A[6]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*4)+3], c7_1);
#else
    C[(l_n*4)+0] += A[5] * B[(l_n*12)+7];
    C[(l_n*4)+3] += A[6] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 126;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna7_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_kXiDivMT_m1_n9_k4_ldAna7_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
    for ( l_m = 0; l_m < 1; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*4)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*4)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*4)+0]);
    __m128 a1_0 = _mm_load_ss(&A[0]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*4)+0], c1_0);
#else
    C[(l_n*4)+0] += A[0] * B[(l_n*4)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna7_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_starMatrix_m84_n9_k9_ldA84_ldBna8_ldC84_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 84; l_m++) {
    C[0+l_m] += A[504+l_m] * B[0];
    C[0+l_m] += A[588+l_m] * B[1];
    C[0+l_m] += A[672+l_m] * B[2];
    C[84+l_m] += A[504+l_m] * B[3];
    C[84+l_m] += A[588+l_m] * B[4];
    C[84+l_m] += A[672+l_m] * B[5];
    C[168+l_m] += A[504+l_m] * B[6];
    C[168+l_m] += A[588+l_m] * B[7];
    C[168+l_m] += A[672+l_m] * B[8];
    C[252+l_m] += A[504+l_m] * B[9];
    C[252+l_m] += A[588+l_m] * B[10];
    C[336+l_m] += A[588+l_m] * B[11];
    C[336+l_m] += A[672+l_m] * B[12];
    C[420+l_m] += A[504+l_m] * B[13];
    C[420+l_m] += A[672+l_m] * B[14];
    C[504+l_m] += A[0+l_m] * B[15];
    C[504+l_m] += A[252+l_m] * B[16];
    C[504+l_m] += A[420+l_m] * B[17];
    C[588+l_m] += A[84+l_m] * B[18];
    C[588+l_m] += A[252+l_m] * B[19];
    C[588+l_m] += A[336+l_m] * B[20];
    C[672+l_m] += A[168+l_m] * B[21];
    C[672+l_m] += A[336+l_m] * B[22];
    C[672+l_m] += A[420+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 4032;
#endif
}

void ssparse_starMatrix_m56_n9_k9_ldA56_ldBna8_ldC56_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 56; l_m++) {
    C[0+l_m] += A[336+l_m] * B[0];
    C[0+l_m] += A[392+l_m] * B[1];
    C[0+l_m] += A[448+l_m] * B[2];
    C[56+l_m] += A[336+l_m] * B[3];
    C[56+l_m] += A[392+l_m] * B[4];
    C[56+l_m] += A[448+l_m] * B[5];
    C[112+l_m] += A[336+l_m] * B[6];
    C[112+l_m] += A[392+l_m] * B[7];
    C[112+l_m] += A[448+l_m] * B[8];
    C[168+l_m] += A[336+l_m] * B[9];
    C[168+l_m] += A[392+l_m] * B[10];
    C[224+l_m] += A[392+l_m] * B[11];
    C[224+l_m] += A[448+l_m] * B[12];
    C[280+l_m] += A[336+l_m] * B[13];
    C[280+l_m] += A[448+l_m] * B[14];
    C[336+l_m] += A[0+l_m] * B[15];
    C[336+l_m] += A[168+l_m] * B[16];
    C[336+l_m] += A[280+l_m] * B[17];
    C[392+l_m] += A[56+l_m] * B[18];
    C[392+l_m] += A[168+l_m] * B[19];
    C[392+l_m] += A[224+l_m] * B[20];
    C[448+l_m] += A[112+l_m] * B[21];
    C[448+l_m] += A[224+l_m] * B[22];
    C[448+l_m] += A[280+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 2688;
#endif
}

void ssparse_starMatrix_m35_n9_k9_ldA36_ldBna8_ldC36_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 35; l_m++) {
    C[0+l_m] += A[216+l_m] * B[0];
    C[0+l_m] += A[252+l_m] * B[1];
    C[0+l_m] += A[288+l_m] * B[2];
    C[36+l_m] += A[216+l_m] * B[3];
    C[36+l_m] += A[252+l_m] * B[4];
    C[36+l_m] += A[288+l_m] * B[5];
    C[72+l_m] += A[216+l_m] * B[6];
    C[72+l_m] += A[252+l_m] * B[7];
    C[72+l_m] += A[288+l_m] * B[8];
    C[108+l_m] += A[216+l_m] * B[9];
    C[108+l_m] += A[252+l_m] * B[10];
    C[144+l_m] += A[252+l_m] * B[11];
    C[144+l_m] += A[288+l_m] * B[12];
    C[180+l_m] += A[216+l_m] * B[13];
    C[180+l_m] += A[288+l_m] * B[14];
    C[216+l_m] += A[0+l_m] * B[15];
    C[216+l_m] += A[108+l_m] * B[16];
    C[216+l_m] += A[180+l_m] * B[17];
    C[252+l_m] += A[36+l_m] * B[18];
    C[252+l_m] += A[108+l_m] * B[19];
    C[252+l_m] += A[144+l_m] * B[20];
    C[288+l_m] += A[72+l_m] * B[21];
    C[288+l_m] += A[144+l_m] * B[22];
    C[288+l_m] += A[180+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1680;
#endif
}

void ssparse_starMatrix_m20_n9_k9_ldA20_ldBna8_ldC20_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 20; l_m++) {
    C[0+l_m] += A[120+l_m] * B[0];
    C[0+l_m] += A[140+l_m] * B[1];
    C[0+l_m] += A[160+l_m] * B[2];
    C[20+l_m] += A[120+l_m] * B[3];
    C[20+l_m] += A[140+l_m] * B[4];
    C[20+l_m] += A[160+l_m] * B[5];
    C[40+l_m] += A[120+l_m] * B[6];
    C[40+l_m] += A[140+l_m] * B[7];
    C[40+l_m] += A[160+l_m] * B[8];
    C[60+l_m] += A[120+l_m] * B[9];
    C[60+l_m] += A[140+l_m] * B[10];
    C[80+l_m] += A[140+l_m] * B[11];
    C[80+l_m] += A[160+l_m] * B[12];
    C[100+l_m] += A[120+l_m] * B[13];
    C[100+l_m] += A[160+l_m] * B[14];
    C[120+l_m] += A[0+l_m] * B[15];
    C[120+l_m] += A[60+l_m] * B[16];
    C[120+l_m] += A[100+l_m] * B[17];
    C[140+l_m] += A[20+l_m] * B[18];
    C[140+l_m] += A[60+l_m] * B[19];
    C[140+l_m] += A[80+l_m] * B[20];
    C[160+l_m] += A[40+l_m] * B[21];
    C[160+l_m] += A[80+l_m] * B[22];
    C[160+l_m] += A[100+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 960;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna8_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna8_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_starMatrix_m1_n9_k9_ldA4_ldBna8_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  for ( l_m = 0; l_m < 1; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 48;
#endif
}

void ssparse_kXiDivM_m4_n9_k4_ldAna2_ldB4_ldC4_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 4; l_m++) {
      C[(l_n*4)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*4)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*4)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*4)+1]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*4)+1], c0_0);
#else
    C[(l_n*4)+1] += A[0] * B[(l_n*4)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

#if defined(__SSE3__) || defined(__AVX__)
#else
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 18;
#endif
}

void ssparse_starMatrix_m4_n9_k9_ldA4_ldBna2_ldC4_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(4)
  #pragma vector aligned
  for ( l_m = 0; l_m < 4; l_m++) {
    C[0+l_m] += A[24+l_m] * B[0];
    C[0+l_m] += A[28+l_m] * B[1];
    C[0+l_m] += A[32+l_m] * B[2];
    C[4+l_m] += A[24+l_m] * B[3];
    C[4+l_m] += A[28+l_m] * B[4];
    C[4+l_m] += A[32+l_m] * B[5];
    C[8+l_m] += A[24+l_m] * B[6];
    C[8+l_m] += A[28+l_m] * B[7];
    C[8+l_m] += A[32+l_m] * B[8];
    C[12+l_m] += A[24+l_m] * B[9];
    C[12+l_m] += A[28+l_m] * B[10];
    C[16+l_m] += A[28+l_m] * B[11];
    C[16+l_m] += A[32+l_m] * B[12];
    C[20+l_m] += A[24+l_m] * B[13];
    C[20+l_m] += A[32+l_m] * B[14];
    C[24+l_m] += A[0+l_m] * B[15];
    C[24+l_m] += A[12+l_m] * B[16];
    C[24+l_m] += A[20+l_m] * B[17];
    C[28+l_m] += A[4+l_m] * B[18];
    C[28+l_m] += A[12+l_m] * B[19];
    C[28+l_m] += A[16+l_m] * B[20];
    C[32+l_m] += A[8+l_m] * B[21];
    C[32+l_m] += A[16+l_m] * B[22];
    C[32+l_m] += A[20+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 192;
#endif
}

void ssparse_starMatrix_m10_n9_k9_ldA12_ldBna3_ldC12_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 10; l_m++) {
    C[0+l_m] += A[72+l_m] * B[0];
    C[0+l_m] += A[84+l_m] * B[1];
    C[0+l_m] += A[96+l_m] * B[2];
    C[12+l_m] += A[72+l_m] * B[3];
    C[12+l_m] += A[84+l_m] * B[4];
    C[12+l_m] += A[96+l_m] * B[5];
    C[24+l_m] += A[72+l_m] * B[6];
    C[24+l_m] += A[84+l_m] * B[7];
    C[24+l_m] += A[96+l_m] * B[8];
    C[36+l_m] += A[72+l_m] * B[9];
    C[36+l_m] += A[84+l_m] * B[10];
    C[48+l_m] += A[84+l_m] * B[11];
    C[48+l_m] += A[96+l_m] * B[12];
    C[60+l_m] += A[72+l_m] * B[13];
    C[60+l_m] += A[96+l_m] * B[14];
    C[72+l_m] += A[0+l_m] * B[15];
    C[72+l_m] += A[36+l_m] * B[16];
    C[72+l_m] += A[60+l_m] * B[17];
    C[84+l_m] += A[12+l_m] * B[18];
    C[84+l_m] += A[36+l_m] * B[19];
    C[84+l_m] += A[48+l_m] * B[20];
    C[96+l_m] += A[24+l_m] * B[21];
    C[96+l_m] += A[48+l_m] * B[22];
    C[96+l_m] += A[60+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 480;
#endif
}

void ssparse_fM1DivM_m10_n9_k10_ldAna3_ldB12_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*12)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*12)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*12)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*12)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*12)+9], c0_2);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*12)+0];
    C[(l_n*12)+3] += A[1] * B[(l_n*12)+0];
    C[(l_n*12)+9] += A[2] * B[(l_n*12)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a1_0 = _mm_load_ss(&A[3]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a1_1 = _mm_load_ss(&A[4]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*12)+7], c1_1);
#else
    C[(l_n*12)+1] += A[3] * B[(l_n*12)+1];
    C[(l_n*12)+7] += A[4] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*12)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*12)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*12)+2]);
    __m128 a2_0 = _mm_load_ss(&A[5]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*12)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a2_1 = _mm_load_ss(&A[6]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*12)+8], c2_1);
#else
    C[(l_n*12)+2] += A[5] * B[(l_n*12)+2];
    C[(l_n*12)+8] += A[6] * B[(l_n*12)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*12)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*12)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a3_0 = _mm_load_ss(&A[7]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*12)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a3_1 = _mm_load_ss(&A[8]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*12)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a3_2 = _mm_load_ss(&A[9]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*12)+9], c3_2);
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*12)+3];
    C[(l_n*12)+3] += A[8] * B[(l_n*12)+3];
    C[(l_n*12)+9] += A[9] * B[(l_n*12)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a4_0 = _mm_load_ss(&A[10]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+4], c4_0);
#else
    C[(l_n*12)+4] += A[10] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a5_0 = _mm_load_ss(&A[11]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+5], c5_0);
#else
    C[(l_n*12)+5] += A[11] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*12)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*12)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a6_0 = _mm_load_ss(&A[12]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*12)+6], c6_0);
#else
    C[(l_n*12)+6] += A[12] * B[(l_n*12)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a7_0 = _mm_load_ss(&A[13]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a7_1 = _mm_load_ss(&A[14]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+7], c7_1);
#else
    C[(l_n*12)+1] += A[13] * B[(l_n*12)+7];
    C[(l_n*12)+7] += A[14] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*12)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*12)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*12)+2]);
    __m128 a8_0 = _mm_load_ss(&A[15]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*12)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a8_1 = _mm_load_ss(&A[16]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*12)+8], c8_1);
#else
    C[(l_n*12)+2] += A[15] * B[(l_n*12)+8];
    C[(l_n*12)+8] += A[16] * B[(l_n*12)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*12)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*12)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a9_0 = _mm_load_ss(&A[17]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*12)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a9_1 = _mm_load_ss(&A[18]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*12)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a9_2 = _mm_load_ss(&A[19]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*12)+9], c9_2);
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*12)+9];
    C[(l_n*12)+3] += A[18] * B[(l_n*12)+9];
    C[(l_n*12)+9] += A[19] * B[(l_n*12)+9];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 360;
#endif
}

void ssparse_fP113DivM_m10_n9_k10_ldAna3_ldB12_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*12)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*12)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*12)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*12)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*12)+9], c0_2);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*12)+0];
    C[(l_n*12)+3] += A[1] * B[(l_n*12)+0];
    C[(l_n*12)+9] += A[2] * B[(l_n*12)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a1_0 = _mm_load_ss(&A[3]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a1_1 = _mm_load_ss(&A[4]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*12)+7], c1_1);
#else
    C[(l_n*12)+1] += A[3] * B[(l_n*12)+1];
    C[(l_n*12)+7] += A[4] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*12)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*12)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*12)+2]);
    __m128 a2_0 = _mm_load_ss(&A[5]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*12)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a2_1 = _mm_load_ss(&A[6]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*12)+8], c2_1);
#else
    C[(l_n*12)+2] += A[5] * B[(l_n*12)+2];
    C[(l_n*12)+8] += A[6] * B[(l_n*12)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*12)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*12)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a3_0 = _mm_load_ss(&A[7]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*12)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a3_1 = _mm_load_ss(&A[8]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*12)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a3_2 = _mm_load_ss(&A[9]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*12)+9], c3_2);
#else
    C[(l_n*12)+0] += A[7] * B[(l_n*12)+3];
    C[(l_n*12)+3] += A[8] * B[(l_n*12)+3];
    C[(l_n*12)+9] += A[9] * B[(l_n*12)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a4_0 = _mm_load_ss(&A[10]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+4], c4_0);
#else
    C[(l_n*12)+4] += A[10] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a5_0 = _mm_load_ss(&A[11]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+5], c5_0);
#else
    C[(l_n*12)+5] += A[11] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*12)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*12)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a6_0 = _mm_load_ss(&A[12]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*12)+6], c6_0);
#else
    C[(l_n*12)+6] += A[12] * B[(l_n*12)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a7_0 = _mm_load_ss(&A[13]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a7_1 = _mm_load_ss(&A[14]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+7], c7_1);
#else
    C[(l_n*12)+1] += A[13] * B[(l_n*12)+7];
    C[(l_n*12)+7] += A[14] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*12)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*12)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*12)+2]);
    __m128 a8_0 = _mm_load_ss(&A[15]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*12)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a8_1 = _mm_load_ss(&A[16]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*12)+8], c8_1);
#else
    C[(l_n*12)+2] += A[15] * B[(l_n*12)+8];
    C[(l_n*12)+8] += A[16] * B[(l_n*12)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*12)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*12)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a9_0 = _mm_load_ss(&A[17]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*12)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a9_1 = _mm_load_ss(&A[18]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*12)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a9_2 = _mm_load_ss(&A[19]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*12)+9], c9_2);
#else
    C[(l_n*12)+0] += A[17] * B[(l_n*12)+9];
    C[(l_n*12)+3] += A[18] * B[(l_n*12)+9];
    C[(l_n*12)+9] += A[19] * B[(l_n*12)+9];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 360;
#endif
}

void ssparse_fP111DivM_m10_n9_k10_ldAna3_ldB12_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*12)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*12)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*12)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*12)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*12)+9], c0_2);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*12)+0];
    C[(l_n*12)+3] += A[1] * B[(l_n*12)+0];
    C[(l_n*12)+9] += A[2] * B[(l_n*12)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c1_0 = _mm_add_ps(c1_0, _mm_mul_ps(a1_0, b1));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c1_0));
    __m128 c1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[5]));
    c1_2 = _mm_add_ps(c1_2, _mm_mul_ps(a1_2, b1));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c1_2));
#else
    C[(l_n*12)+1] += A[3] * B[(l_n*12)+1];
    C[(l_n*12)+2] += A[4] * B[(l_n*12)+1];
    C[(l_n*12)+7] += A[5] * B[(l_n*12)+1];
    C[(l_n*12)+8] += A[6] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*12)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*12)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[7]));
    c2_0 = _mm_add_ps(c2_0, _mm_mul_ps(a2_0, b2));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c2_0));
    __m128 c2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[9]));
    c2_2 = _mm_add_ps(c2_2, _mm_mul_ps(a2_2, b2));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c2_2));
#else
    C[(l_n*12)+1] += A[7] * B[(l_n*12)+2];
    C[(l_n*12)+2] += A[8] * B[(l_n*12)+2];
    C[(l_n*12)+7] += A[9] * B[(l_n*12)+2];
    C[(l_n*12)+8] += A[10] * B[(l_n*12)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*12)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*12)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a3_0 = _mm_load_ss(&A[11]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*12)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a3_1 = _mm_load_ss(&A[12]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*12)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a3_2 = _mm_load_ss(&A[13]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*12)+9], c3_2);
#else
    C[(l_n*12)+0] += A[11] * B[(l_n*12)+3];
    C[(l_n*12)+3] += A[12] * B[(l_n*12)+3];
    C[(l_n*12)+9] += A[13] * B[(l_n*12)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[14]));
    c4_0 = _mm_add_ps(c4_0, _mm_mul_ps(a4_0, b4));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c4_0));
    __m128 c4_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a4_2 = _mm_load_ss(&A[16]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*12)+6], c4_2);
#else
    C[(l_n*12)+4] += A[14] * B[(l_n*12)+4];
    C[(l_n*12)+5] += A[15] * B[(l_n*12)+4];
    C[(l_n*12)+6] += A[16] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[17]));
    c5_0 = _mm_add_ps(c5_0, _mm_mul_ps(a5_0, b5));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c5_0));
    __m128 c5_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a5_2 = _mm_load_ss(&A[19]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*12)+6], c5_2);
#else
    C[(l_n*12)+4] += A[17] * B[(l_n*12)+5];
    C[(l_n*12)+5] += A[18] * B[(l_n*12)+5];
    C[(l_n*12)+6] += A[19] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*12)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*12)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[20]));
    c6_0 = _mm_add_ps(c6_0, _mm_mul_ps(a6_0, b6));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c6_0));
    __m128 c6_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a6_2 = _mm_load_ss(&A[22]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*12)+6], c6_2);
#else
    C[(l_n*12)+4] += A[20] * B[(l_n*12)+6];
    C[(l_n*12)+5] += A[21] * B[(l_n*12)+6];
    C[(l_n*12)+6] += A[22] * B[(l_n*12)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[23]));
    c7_0 = _mm_add_ps(c7_0, _mm_mul_ps(a7_0, b7));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c7_0));
    __m128 c7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[25]));
    c7_2 = _mm_add_ps(c7_2, _mm_mul_ps(a7_2, b7));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c7_2));
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*12)+7];
    C[(l_n*12)+2] += A[24] * B[(l_n*12)+7];
    C[(l_n*12)+7] += A[25] * B[(l_n*12)+7];
    C[(l_n*12)+8] += A[26] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*12)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*12)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[27]));
    c8_0 = _mm_add_ps(c8_0, _mm_mul_ps(a8_0, b8));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c8_0));
    __m128 c8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[29]));
    c8_2 = _mm_add_ps(c8_2, _mm_mul_ps(a8_2, b8));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c8_2));
#else
    C[(l_n*12)+1] += A[27] * B[(l_n*12)+8];
    C[(l_n*12)+2] += A[28] * B[(l_n*12)+8];
    C[(l_n*12)+7] += A[29] * B[(l_n*12)+8];
    C[(l_n*12)+8] += A[30] * B[(l_n*12)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*12)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*12)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a9_0 = _mm_load_ss(&A[31]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*12)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a9_1 = _mm_load_ss(&A[32]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*12)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a9_2 = _mm_load_ss(&A[33]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*12)+9], c9_2);
#else
    C[(l_n*12)+0] += A[31] * B[(l_n*12)+9];
    C[(l_n*12)+3] += A[32] * B[(l_n*12)+9];
    C[(l_n*12)+9] += A[33] * B[(l_n*12)+9];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 612;
#endif
}

void ssparse_fP112DivM_m10_n9_k10_ldAna3_ldB12_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*12)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*12)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*12)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*12)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*12)+9], c0_2);
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*12)+0];
    C[(l_n*12)+3] += A[1] * B[(l_n*12)+0];
    C[(l_n*12)+9] += A[2] * B[(l_n*12)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[3]));
    c1_0 = _mm_add_ps(c1_0, _mm_mul_ps(a1_0, b1));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c1_0));
    __m128 c1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[5]));
    c1_2 = _mm_add_ps(c1_2, _mm_mul_ps(a1_2, b1));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c1_2));
#else
    C[(l_n*12)+1] += A[3] * B[(l_n*12)+1];
    C[(l_n*12)+2] += A[4] * B[(l_n*12)+1];
    C[(l_n*12)+7] += A[5] * B[(l_n*12)+1];
    C[(l_n*12)+8] += A[6] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*12)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*12)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[7]));
    c2_0 = _mm_add_ps(c2_0, _mm_mul_ps(a2_0, b2));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c2_0));
    __m128 c2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[9]));
    c2_2 = _mm_add_ps(c2_2, _mm_mul_ps(a2_2, b2));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c2_2));
#else
    C[(l_n*12)+1] += A[7] * B[(l_n*12)+2];
    C[(l_n*12)+2] += A[8] * B[(l_n*12)+2];
    C[(l_n*12)+7] += A[9] * B[(l_n*12)+2];
    C[(l_n*12)+8] += A[10] * B[(l_n*12)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*12)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*12)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a3_0 = _mm_load_ss(&A[11]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*12)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a3_1 = _mm_load_ss(&A[12]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*12)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a3_2 = _mm_load_ss(&A[13]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*12)+9], c3_2);
#else
    C[(l_n*12)+0] += A[11] * B[(l_n*12)+3];
    C[(l_n*12)+3] += A[12] * B[(l_n*12)+3];
    C[(l_n*12)+9] += A[13] * B[(l_n*12)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[14]));
    c4_0 = _mm_add_ps(c4_0, _mm_mul_ps(a4_0, b4));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c4_0));
    __m128 c4_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a4_2 = _mm_load_ss(&A[16]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*12)+6], c4_2);
#else
    C[(l_n*12)+4] += A[14] * B[(l_n*12)+4];
    C[(l_n*12)+5] += A[15] * B[(l_n*12)+4];
    C[(l_n*12)+6] += A[16] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[17]));
    c5_0 = _mm_add_ps(c5_0, _mm_mul_ps(a5_0, b5));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c5_0));
    __m128 c5_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a5_2 = _mm_load_ss(&A[19]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*12)+6], c5_2);
#else
    C[(l_n*12)+4] += A[17] * B[(l_n*12)+5];
    C[(l_n*12)+5] += A[18] * B[(l_n*12)+5];
    C[(l_n*12)+6] += A[19] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*12)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*12)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+4]));
    __m128 a6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[20]));
    c6_0 = _mm_add_ps(c6_0, _mm_mul_ps(a6_0, b6));
    _mm_store_sd((double*)&C[(l_n*12)+4], _mm_castps_pd(c6_0));
    __m128 c6_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a6_2 = _mm_load_ss(&A[22]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*12)+6], c6_2);
#else
    C[(l_n*12)+4] += A[20] * B[(l_n*12)+6];
    C[(l_n*12)+5] += A[21] * B[(l_n*12)+6];
    C[(l_n*12)+6] += A[22] * B[(l_n*12)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[23]));
    c7_0 = _mm_add_ps(c7_0, _mm_mul_ps(a7_0, b7));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c7_0));
    __m128 c7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[25]));
    c7_2 = _mm_add_ps(c7_2, _mm_mul_ps(a7_2, b7));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c7_2));
#else
    C[(l_n*12)+1] += A[23] * B[(l_n*12)+7];
    C[(l_n*12)+2] += A[24] * B[(l_n*12)+7];
    C[(l_n*12)+7] += A[25] * B[(l_n*12)+7];
    C[(l_n*12)+8] += A[26] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*12)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*12)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+1]));
    __m128 a8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[27]));
    c8_0 = _mm_add_ps(c8_0, _mm_mul_ps(a8_0, b8));
    _mm_store_sd((double*)&C[(l_n*12)+1], _mm_castps_pd(c8_0));
    __m128 c8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+7]));
    __m128 a8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[29]));
    c8_2 = _mm_add_ps(c8_2, _mm_mul_ps(a8_2, b8));
    _mm_store_sd((double*)&C[(l_n*12)+7], _mm_castps_pd(c8_2));
#else
    C[(l_n*12)+1] += A[27] * B[(l_n*12)+8];
    C[(l_n*12)+2] += A[28] * B[(l_n*12)+8];
    C[(l_n*12)+7] += A[29] * B[(l_n*12)+8];
    C[(l_n*12)+8] += A[30] * B[(l_n*12)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*12)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*12)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a9_0 = _mm_load_ss(&A[31]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*12)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a9_1 = _mm_load_ss(&A[32]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*12)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a9_2 = _mm_load_ss(&A[33]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*12)+9], c9_2);
#else
    C[(l_n*12)+0] += A[31] * B[(l_n*12)+9];
    C[(l_n*12)+3] += A[32] * B[(l_n*12)+9];
    C[(l_n*12)+9] += A[33] * B[(l_n*12)+9];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 612;
#endif
}

void ssparse_fP222DivM_m10_n9_k10_ldAna3_ldB12_ldC12_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 10; l_m++) {
      C[(l_n*12)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*12)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*12)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*12)+0], c0_0);
    __m128 c0_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a0_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[1]));
    c0_1 = _mm_add_ps(c0_1, _mm_mul_ps(a0_1, b0));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c0_1));
    __m128 c0_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*12)+6], c0_3);
    __m128 c0_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a0_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[4]));
    c0_4 = _mm_add_ps(c0_4, _mm_mul_ps(a0_4, b0));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c0_4));
#else
    C[(l_n*12)+0] += A[0] * B[(l_n*12)+0];
    C[(l_n*12)+2] += A[1] * B[(l_n*12)+0];
    C[(l_n*12)+3] += A[2] * B[(l_n*12)+0];
    C[(l_n*12)+6] += A[3] * B[(l_n*12)+0];
    C[(l_n*12)+8] += A[4] * B[(l_n*12)+0];
    C[(l_n*12)+9] += A[5] * B[(l_n*12)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*12)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*12)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a1_0 = _mm_load_ss(&A[6]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*12)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a1_1 = _mm_load_ss(&A[7]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*12)+5], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a1_2 = _mm_load_ss(&A[8]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*12)+7], c1_2);
#else
    C[(l_n*12)+1] += A[6] * B[(l_n*12)+1];
    C[(l_n*12)+5] += A[7] * B[(l_n*12)+1];
    C[(l_n*12)+7] += A[8] * B[(l_n*12)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*12)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*12)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a2_0 = _mm_load_ss(&A[9]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*12)+0], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*12)+2]);
    __m128 a2_1 = _mm_load_ss(&A[10]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*12)+2], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a2_2 = _mm_load_ss(&A[11]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*12)+6], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a2_3 = _mm_load_ss(&A[12]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*12)+8], c2_3);
#else
    C[(l_n*12)+0] += A[9] * B[(l_n*12)+2];
    C[(l_n*12)+2] += A[10] * B[(l_n*12)+2];
    C[(l_n*12)+6] += A[11] * B[(l_n*12)+2];
    C[(l_n*12)+8] += A[12] * B[(l_n*12)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*12)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*12)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a3_0 = _mm_load_ss(&A[13]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*12)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a3_1 = _mm_load_ss(&A[14]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*12)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a3_2 = _mm_load_ss(&A[15]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*12)+6], c3_2);
    __m128 c3_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+8]));
    __m128 a3_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[16]));
    c3_3 = _mm_add_ps(c3_3, _mm_mul_ps(a3_3, b3));
    _mm_store_sd((double*)&C[(l_n*12)+8], _mm_castps_pd(c3_3));
#else
    C[(l_n*12)+0] += A[13] * B[(l_n*12)+3];
    C[(l_n*12)+3] += A[14] * B[(l_n*12)+3];
    C[(l_n*12)+6] += A[15] * B[(l_n*12)+3];
    C[(l_n*12)+8] += A[16] * B[(l_n*12)+3];
    C[(l_n*12)+9] += A[17] * B[(l_n*12)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*12)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*12)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*12)+4]);
    __m128 a4_0 = _mm_load_ss(&A[18]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*12)+4], c4_0);
#else
    C[(l_n*12)+4] += A[18] * B[(l_n*12)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*12)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*12)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a5_0 = _mm_load_ss(&A[19]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*12)+1], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*12)+5]);
    __m128 a5_1 = _mm_load_ss(&A[20]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*12)+5], c5_1);
#else
    C[(l_n*12)+1] += A[19] * B[(l_n*12)+5];
    C[(l_n*12)+5] += A[20] * B[(l_n*12)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*12)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*12)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a6_0 = _mm_load_ss(&A[21]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*12)+0], c6_0);
    __m128 c6_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a6_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[22]));
    c6_1 = _mm_add_ps(c6_1, _mm_mul_ps(a6_1, b6));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c6_1));
    __m128 c6_3 = _mm_load_ss(&C[(l_n*12)+6]);
    __m128 a6_3 = _mm_load_ss(&A[24]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*12)+6], c6_3);
#else
    C[(l_n*12)+0] += A[21] * B[(l_n*12)+6];
    C[(l_n*12)+2] += A[22] * B[(l_n*12)+6];
    C[(l_n*12)+3] += A[23] * B[(l_n*12)+6];
    C[(l_n*12)+6] += A[24] * B[(l_n*12)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*12)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*12)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*12)+1]);
    __m128 a7_0 = _mm_load_ss(&A[25]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*12)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*12)+7]);
    __m128 a7_1 = _mm_load_ss(&A[26]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*12)+7], c7_1);
#else
    C[(l_n*12)+1] += A[25] * B[(l_n*12)+7];
    C[(l_n*12)+7] += A[26] * B[(l_n*12)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*12)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*12)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a8_0 = _mm_load_ss(&A[27]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*12)+0], c8_0);
    __m128 c8_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*12)+2]));
    __m128 a8_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[28]));
    c8_1 = _mm_add_ps(c8_1, _mm_mul_ps(a8_1, b8));
    _mm_store_sd((double*)&C[(l_n*12)+2], _mm_castps_pd(c8_1));
    __m128 c8_3 = _mm_load_ss(&C[(l_n*12)+8]);
    __m128 a8_3 = _mm_load_ss(&A[30]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*12)+8], c8_3);
#else
    C[(l_n*12)+0] += A[27] * B[(l_n*12)+8];
    C[(l_n*12)+2] += A[28] * B[(l_n*12)+8];
    C[(l_n*12)+3] += A[29] * B[(l_n*12)+8];
    C[(l_n*12)+8] += A[30] * B[(l_n*12)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*12)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*12)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*12)+0]);
    __m128 a9_0 = _mm_load_ss(&A[31]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*12)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*12)+3]);
    __m128 a9_1 = _mm_load_ss(&A[32]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*12)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*12)+9]);
    __m128 a9_2 = _mm_load_ss(&A[33]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*12)+9], c9_2);
#else
    C[(l_n*12)+0] += A[31] * B[(l_n*12)+9];
    C[(l_n*12)+3] += A[32] * B[(l_n*12)+9];
    C[(l_n*12)+9] += A[33] * B[(l_n*12)+9];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 612;
#endif
}

void ssparse_starMatrix_m20_n9_k9_ldA20_ldBna4_ldC20_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 20; l_m++) {
    C[0+l_m] += A[120+l_m] * B[0];
    C[0+l_m] += A[140+l_m] * B[1];
    C[0+l_m] += A[160+l_m] * B[2];
    C[20+l_m] += A[120+l_m] * B[3];
    C[20+l_m] += A[140+l_m] * B[4];
    C[20+l_m] += A[160+l_m] * B[5];
    C[40+l_m] += A[120+l_m] * B[6];
    C[40+l_m] += A[140+l_m] * B[7];
    C[40+l_m] += A[160+l_m] * B[8];
    C[60+l_m] += A[120+l_m] * B[9];
    C[60+l_m] += A[140+l_m] * B[10];
    C[80+l_m] += A[140+l_m] * B[11];
    C[80+l_m] += A[160+l_m] * B[12];
    C[100+l_m] += A[120+l_m] * B[13];
    C[100+l_m] += A[160+l_m] * B[14];
    C[120+l_m] += A[0+l_m] * B[15];
    C[120+l_m] += A[60+l_m] * B[16];
    C[120+l_m] += A[100+l_m] * B[17];
    C[140+l_m] += A[20+l_m] * B[18];
    C[140+l_m] += A[60+l_m] * B[19];
    C[140+l_m] += A[80+l_m] * B[20];
    C[160+l_m] += A[40+l_m] * B[21];
    C[160+l_m] += A[80+l_m] * B[22];
    C[160+l_m] += A[100+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 960;
#endif
}

void ssparse_fM1DivM_m20_n9_k20_ldAna4_ldB20_ldC20_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 20; l_m++) {
      C[(l_n*20)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*20)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*20)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*20)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*20)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*20)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*20)+19], c0_3);
#else
    C[(l_n*20)+0] += A[0] * B[(l_n*20)+0];
    C[(l_n*20)+3] += A[1] * B[(l_n*20)+0];
    C[(l_n*20)+9] += A[2] * B[(l_n*20)+0];
    C[(l_n*20)+19] += A[3] * B[(l_n*20)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*20)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*20)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a1_0 = _mm_load_ss(&A[4]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*20)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a1_1 = _mm_load_ss(&A[5]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*20)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a1_2 = _mm_load_ss(&A[6]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*20)+17], c1_2);
#else
    C[(l_n*20)+1] += A[4] * B[(l_n*20)+1];
    C[(l_n*20)+7] += A[5] * B[(l_n*20)+1];
    C[(l_n*20)+17] += A[6] * B[(l_n*20)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*20)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*20)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*20)+2]);
    __m128 a2_0 = _mm_load_ss(&A[7]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*20)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*20)+8]);
    __m128 a2_1 = _mm_load_ss(&A[8]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*20)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*20)+18]);
    __m128 a2_2 = _mm_load_ss(&A[9]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*20)+18], c2_2);
#else
    C[(l_n*20)+2] += A[7] * B[(l_n*20)+2];
    C[(l_n*20)+8] += A[8] * B[(l_n*20)+2];
    C[(l_n*20)+18] += A[9] * B[(l_n*20)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*20)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*20)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a3_0 = _mm_load_ss(&A[10]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*20)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a3_1 = _mm_load_ss(&A[11]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*20)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a3_2 = _mm_load_ss(&A[12]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*20)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a3_3 = _mm_load_ss(&A[13]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*20)+19], c3_3);
#else
    C[(l_n*20)+0] += A[10] * B[(l_n*20)+3];
    C[(l_n*20)+3] += A[11] * B[(l_n*20)+3];
    C[(l_n*20)+9] += A[12] * B[(l_n*20)+3];
    C[(l_n*20)+19] += A[13] * B[(l_n*20)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*20)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*20)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a4_0 = _mm_load_ss(&A[14]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*20)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*20)+14]);
    __m128 a4_1 = _mm_load_ss(&A[15]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*20)+14], c4_1);
#else
    C[(l_n*20)+4] += A[14] * B[(l_n*20)+4];
    C[(l_n*20)+14] += A[15] * B[(l_n*20)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*20)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*20)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a5_0 = _mm_load_ss(&A[16]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*20)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a5_1 = _mm_load_ss(&A[17]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*20)+15], c5_1);
#else
    C[(l_n*20)+5] += A[16] * B[(l_n*20)+5];
    C[(l_n*20)+15] += A[17] * B[(l_n*20)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*20)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*20)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a6_0 = _mm_load_ss(&A[18]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*20)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a6_1 = _mm_load_ss(&A[19]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*20)+16], c6_1);
#else
    C[(l_n*20)+6] += A[18] * B[(l_n*20)+6];
    C[(l_n*20)+16] += A[19] * B[(l_n*20)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*20)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*20)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a7_0 = _mm_load_ss(&A[20]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*20)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a7_1 = _mm_load_ss(&A[21]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*20)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a7_2 = _mm_load_ss(&A[22]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*20)+17], c7_2);
#else
    C[(l_n*20)+1] += A[20] * B[(l_n*20)+7];
    C[(l_n*20)+7] += A[21] * B[(l_n*20)+7];
    C[(l_n*20)+17] += A[22] * B[(l_n*20)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*20)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*20)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*20)+2]);
    __m128 a8_0 = _mm_load_ss(&A[23]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*20)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*20)+8]);
    __m128 a8_1 = _mm_load_ss(&A[24]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*20)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*20)+18]);
    __m128 a8_2 = _mm_load_ss(&A[25]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*20)+18], c8_2);
#else
    C[(l_n*20)+2] += A[23] * B[(l_n*20)+8];
    C[(l_n*20)+8] += A[24] * B[(l_n*20)+8];
    C[(l_n*20)+18] += A[25] * B[(l_n*20)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*20)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*20)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a9_0 = _mm_load_ss(&A[26]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*20)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a9_1 = _mm_load_ss(&A[27]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*20)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a9_2 = _mm_load_ss(&A[28]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*20)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a9_3 = _mm_load_ss(&A[29]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*20)+19], c9_3);
#else
    C[(l_n*20)+0] += A[26] * B[(l_n*20)+9];
    C[(l_n*20)+3] += A[27] * B[(l_n*20)+9];
    C[(l_n*20)+9] += A[28] * B[(l_n*20)+9];
    C[(l_n*20)+19] += A[29] * B[(l_n*20)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*20)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*20)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*20)+10]);
    __m128 a10_0 = _mm_load_ss(&A[30]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*20)+10], c10_0);
#else
    C[(l_n*20)+10] += A[30] * B[(l_n*20)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*20)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*20)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*20)+11]);
    __m128 a11_0 = _mm_load_ss(&A[31]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*20)+11], c11_0);
#else
    C[(l_n*20)+11] += A[31] * B[(l_n*20)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*20)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*20)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*20)+12]);
    __m128 a12_0 = _mm_load_ss(&A[32]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*20)+12], c12_0);
#else
    C[(l_n*20)+12] += A[32] * B[(l_n*20)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*20)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*20)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*20)+13]);
    __m128 a13_0 = _mm_load_ss(&A[33]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*20)+13], c13_0);
#else
    C[(l_n*20)+13] += A[33] * B[(l_n*20)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*20)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*20)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*20)+4]);
    __m128 a14_0 = _mm_load_ss(&A[34]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*20)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*20)+14]);
    __m128 a14_1 = _mm_load_ss(&A[35]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*20)+14], c14_1);
#else
    C[(l_n*20)+4] += A[34] * B[(l_n*20)+14];
    C[(l_n*20)+14] += A[35] * B[(l_n*20)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*20)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*20)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*20)+5]);
    __m128 a15_0 = _mm_load_ss(&A[36]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*20)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*20)+15]);
    __m128 a15_1 = _mm_load_ss(&A[37]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*20)+15], c15_1);
#else
    C[(l_n*20)+5] += A[36] * B[(l_n*20)+15];
    C[(l_n*20)+15] += A[37] * B[(l_n*20)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*20)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*20)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*20)+6]);
    __m128 a16_0 = _mm_load_ss(&A[38]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*20)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*20)+16]);
    __m128 a16_1 = _mm_load_ss(&A[39]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*20)+16], c16_1);
#else
    C[(l_n*20)+6] += A[38] * B[(l_n*20)+16];
    C[(l_n*20)+16] += A[39] * B[(l_n*20)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*20)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*20)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*20)+1]);
    __m128 a17_0 = _mm_load_ss(&A[40]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*20)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*20)+7]);
    __m128 a17_1 = _mm_load_ss(&A[41]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*20)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*20)+17]);
    __m128 a17_2 = _mm_load_ss(&A[42]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*20)+17], c17_2);
#else
    C[(l_n*20)+1] += A[40] * B[(l_n*20)+17];
    C[(l_n*20)+7] += A[41] * B[(l_n*20)+17];
    C[(l_n*20)+17] += A[42] * B[(l_n*20)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*20)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*20)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*20)+2]);
    __m128 a18_0 = _mm_load_ss(&A[43]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*20)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*20)+8]);
    __m128 a18_1 = _mm_load_ss(&A[44]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*20)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*20)+18]);
    __m128 a18_2 = _mm_load_ss(&A[45]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*20)+18], c18_2);
#else
    C[(l_n*20)+2] += A[43] * B[(l_n*20)+18];
    C[(l_n*20)+8] += A[44] * B[(l_n*20)+18];
    C[(l_n*20)+18] += A[45] * B[(l_n*20)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*20)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*20)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*20)+0]);
    __m128 a19_0 = _mm_load_ss(&A[46]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*20)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*20)+3]);
    __m128 a19_1 = _mm_load_ss(&A[47]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*20)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*20)+9]);
    __m128 a19_2 = _mm_load_ss(&A[48]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*20)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*20)+19]);
    __m128 a19_3 = _mm_load_ss(&A[49]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*20)+19], c19_3);
#else
    C[(l_n*20)+0] += A[46] * B[(l_n*20)+19];
    C[(l_n*20)+3] += A[47] * B[(l_n*20)+19];
    C[(l_n*20)+9] += A[48] * B[(l_n*20)+19];
    C[(l_n*20)+19] += A[49] * B[(l_n*20)+19];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 900;
#endif
}

void ssparse_starMatrix_m35_n9_k9_ldA36_ldBna5_ldC36_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 35; l_m++) {
    C[0+l_m] += A[216+l_m] * B[0];
    C[0+l_m] += A[252+l_m] * B[1];
    C[0+l_m] += A[288+l_m] * B[2];
    C[36+l_m] += A[216+l_m] * B[3];
    C[36+l_m] += A[252+l_m] * B[4];
    C[36+l_m] += A[288+l_m] * B[5];
    C[72+l_m] += A[216+l_m] * B[6];
    C[72+l_m] += A[252+l_m] * B[7];
    C[72+l_m] += A[288+l_m] * B[8];
    C[108+l_m] += A[216+l_m] * B[9];
    C[108+l_m] += A[252+l_m] * B[10];
    C[144+l_m] += A[252+l_m] * B[11];
    C[144+l_m] += A[288+l_m] * B[12];
    C[180+l_m] += A[216+l_m] * B[13];
    C[180+l_m] += A[288+l_m] * B[14];
    C[216+l_m] += A[0+l_m] * B[15];
    C[216+l_m] += A[108+l_m] * B[16];
    C[216+l_m] += A[180+l_m] * B[17];
    C[252+l_m] += A[36+l_m] * B[18];
    C[252+l_m] += A[108+l_m] * B[19];
    C[252+l_m] += A[144+l_m] * B[20];
    C[288+l_m] += A[72+l_m] * B[21];
    C[288+l_m] += A[144+l_m] * B[22];
    C[288+l_m] += A[180+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1680;
#endif
}

void ssparse_fM1DivM_m35_n9_k35_ldAna5_ldB36_ldC36_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 35; l_m++) {
      C[(l_n*36)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*36)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*36)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*36)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*36)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*36)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*36)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*36)+34], c0_4);
#else
    C[(l_n*36)+0] += A[0] * B[(l_n*36)+0];
    C[(l_n*36)+3] += A[1] * B[(l_n*36)+0];
    C[(l_n*36)+9] += A[2] * B[(l_n*36)+0];
    C[(l_n*36)+19] += A[3] * B[(l_n*36)+0];
    C[(l_n*36)+34] += A[4] * B[(l_n*36)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a1_0 = _mm_load_ss(&A[5]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*36)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a1_1 = _mm_load_ss(&A[6]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*36)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a1_2 = _mm_load_ss(&A[7]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*36)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a1_3 = _mm_load_ss(&A[8]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*36)+32], c1_3);
#else
    C[(l_n*36)+1] += A[5] * B[(l_n*36)+1];
    C[(l_n*36)+7] += A[6] * B[(l_n*36)+1];
    C[(l_n*36)+17] += A[7] * B[(l_n*36)+1];
    C[(l_n*36)+32] += A[8] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*36)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*36)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a2_0 = _mm_load_ss(&A[9]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*36)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a2_1 = _mm_load_ss(&A[10]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*36)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a2_2 = _mm_load_ss(&A[11]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*36)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a2_3 = _mm_load_ss(&A[12]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*36)+33], c2_3);
#else
    C[(l_n*36)+2] += A[9] * B[(l_n*36)+2];
    C[(l_n*36)+8] += A[10] * B[(l_n*36)+2];
    C[(l_n*36)+18] += A[11] * B[(l_n*36)+2];
    C[(l_n*36)+33] += A[12] * B[(l_n*36)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*36)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*36)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a3_0 = _mm_load_ss(&A[13]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*36)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a3_1 = _mm_load_ss(&A[14]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*36)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a3_2 = _mm_load_ss(&A[15]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*36)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a3_3 = _mm_load_ss(&A[16]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*36)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a3_4 = _mm_load_ss(&A[17]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*36)+34], c3_4);
#else
    C[(l_n*36)+0] += A[13] * B[(l_n*36)+3];
    C[(l_n*36)+3] += A[14] * B[(l_n*36)+3];
    C[(l_n*36)+9] += A[15] * B[(l_n*36)+3];
    C[(l_n*36)+19] += A[16] * B[(l_n*36)+3];
    C[(l_n*36)+34] += A[17] * B[(l_n*36)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a4_0 = _mm_load_ss(&A[18]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*36)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a4_1 = _mm_load_ss(&A[19]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*36)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a4_2 = _mm_load_ss(&A[20]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*36)+29], c4_2);
#else
    C[(l_n*36)+4] += A[18] * B[(l_n*36)+4];
    C[(l_n*36)+14] += A[19] * B[(l_n*36)+4];
    C[(l_n*36)+29] += A[20] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a5_0 = _mm_load_ss(&A[21]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*36)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a5_1 = _mm_load_ss(&A[22]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*36)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a5_2 = _mm_load_ss(&A[23]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*36)+30], c5_2);
#else
    C[(l_n*36)+5] += A[21] * B[(l_n*36)+5];
    C[(l_n*36)+15] += A[22] * B[(l_n*36)+5];
    C[(l_n*36)+30] += A[23] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*36)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*36)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a6_0 = _mm_load_ss(&A[24]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*36)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a6_1 = _mm_load_ss(&A[25]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*36)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a6_2 = _mm_load_ss(&A[26]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*36)+31], c6_2);
#else
    C[(l_n*36)+6] += A[24] * B[(l_n*36)+6];
    C[(l_n*36)+16] += A[25] * B[(l_n*36)+6];
    C[(l_n*36)+31] += A[26] * B[(l_n*36)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a7_0 = _mm_load_ss(&A[27]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*36)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a7_1 = _mm_load_ss(&A[28]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*36)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a7_2 = _mm_load_ss(&A[29]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*36)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a7_3 = _mm_load_ss(&A[30]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*36)+32], c7_3);
#else
    C[(l_n*36)+1] += A[27] * B[(l_n*36)+7];
    C[(l_n*36)+7] += A[28] * B[(l_n*36)+7];
    C[(l_n*36)+17] += A[29] * B[(l_n*36)+7];
    C[(l_n*36)+32] += A[30] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*36)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*36)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a8_0 = _mm_load_ss(&A[31]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*36)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a8_1 = _mm_load_ss(&A[32]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*36)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a8_2 = _mm_load_ss(&A[33]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*36)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a8_3 = _mm_load_ss(&A[34]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*36)+33], c8_3);
#else
    C[(l_n*36)+2] += A[31] * B[(l_n*36)+8];
    C[(l_n*36)+8] += A[32] * B[(l_n*36)+8];
    C[(l_n*36)+18] += A[33] * B[(l_n*36)+8];
    C[(l_n*36)+33] += A[34] * B[(l_n*36)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*36)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*36)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a9_0 = _mm_load_ss(&A[35]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*36)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a9_1 = _mm_load_ss(&A[36]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*36)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a9_2 = _mm_load_ss(&A[37]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*36)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a9_3 = _mm_load_ss(&A[38]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*36)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a9_4 = _mm_load_ss(&A[39]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*36)+34], c9_4);
#else
    C[(l_n*36)+0] += A[35] * B[(l_n*36)+9];
    C[(l_n*36)+3] += A[36] * B[(l_n*36)+9];
    C[(l_n*36)+9] += A[37] * B[(l_n*36)+9];
    C[(l_n*36)+19] += A[38] * B[(l_n*36)+9];
    C[(l_n*36)+34] += A[39] * B[(l_n*36)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a10_0 = _mm_load_ss(&A[40]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*36)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a10_1 = _mm_load_ss(&A[41]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*36)+25], c10_1);
#else
    C[(l_n*36)+10] += A[40] * B[(l_n*36)+10];
    C[(l_n*36)+25] += A[41] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a11_0 = _mm_load_ss(&A[42]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*36)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a11_1 = _mm_load_ss(&A[43]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*36)+26], c11_1);
#else
    C[(l_n*36)+11] += A[42] * B[(l_n*36)+11];
    C[(l_n*36)+26] += A[43] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a12_0 = _mm_load_ss(&A[44]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*36)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a12_1 = _mm_load_ss(&A[45]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*36)+27], c12_1);
#else
    C[(l_n*36)+12] += A[44] * B[(l_n*36)+12];
    C[(l_n*36)+27] += A[45] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*36)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*36)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a13_0 = _mm_load_ss(&A[46]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*36)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a13_1 = _mm_load_ss(&A[47]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*36)+28], c13_1);
#else
    C[(l_n*36)+13] += A[46] * B[(l_n*36)+13];
    C[(l_n*36)+28] += A[47] * B[(l_n*36)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a14_0 = _mm_load_ss(&A[48]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*36)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a14_1 = _mm_load_ss(&A[49]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*36)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a14_2 = _mm_load_ss(&A[50]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*36)+29], c14_2);
#else
    C[(l_n*36)+4] += A[48] * B[(l_n*36)+14];
    C[(l_n*36)+14] += A[49] * B[(l_n*36)+14];
    C[(l_n*36)+29] += A[50] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a15_0 = _mm_load_ss(&A[51]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*36)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a15_1 = _mm_load_ss(&A[52]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*36)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a15_2 = _mm_load_ss(&A[53]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*36)+30], c15_2);
#else
    C[(l_n*36)+5] += A[51] * B[(l_n*36)+15];
    C[(l_n*36)+15] += A[52] * B[(l_n*36)+15];
    C[(l_n*36)+30] += A[53] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*36)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*36)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a16_0 = _mm_load_ss(&A[54]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*36)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a16_1 = _mm_load_ss(&A[55]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*36)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a16_2 = _mm_load_ss(&A[56]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*36)+31], c16_2);
#else
    C[(l_n*36)+6] += A[54] * B[(l_n*36)+16];
    C[(l_n*36)+16] += A[55] * B[(l_n*36)+16];
    C[(l_n*36)+31] += A[56] * B[(l_n*36)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a17_0 = _mm_load_ss(&A[57]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*36)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a17_1 = _mm_load_ss(&A[58]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*36)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a17_2 = _mm_load_ss(&A[59]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*36)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a17_3 = _mm_load_ss(&A[60]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*36)+32], c17_3);
#else
    C[(l_n*36)+1] += A[57] * B[(l_n*36)+17];
    C[(l_n*36)+7] += A[58] * B[(l_n*36)+17];
    C[(l_n*36)+17] += A[59] * B[(l_n*36)+17];
    C[(l_n*36)+32] += A[60] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*36)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*36)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a18_0 = _mm_load_ss(&A[61]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*36)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a18_1 = _mm_load_ss(&A[62]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*36)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a18_2 = _mm_load_ss(&A[63]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*36)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a18_3 = _mm_load_ss(&A[64]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*36)+33], c18_3);
#else
    C[(l_n*36)+2] += A[61] * B[(l_n*36)+18];
    C[(l_n*36)+8] += A[62] * B[(l_n*36)+18];
    C[(l_n*36)+18] += A[63] * B[(l_n*36)+18];
    C[(l_n*36)+33] += A[64] * B[(l_n*36)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*36)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*36)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a19_0 = _mm_load_ss(&A[65]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*36)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a19_1 = _mm_load_ss(&A[66]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*36)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a19_2 = _mm_load_ss(&A[67]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*36)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a19_3 = _mm_load_ss(&A[68]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*36)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a19_4 = _mm_load_ss(&A[69]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*36)+34], c19_4);
#else
    C[(l_n*36)+0] += A[65] * B[(l_n*36)+19];
    C[(l_n*36)+3] += A[66] * B[(l_n*36)+19];
    C[(l_n*36)+9] += A[67] * B[(l_n*36)+19];
    C[(l_n*36)+19] += A[68] * B[(l_n*36)+19];
    C[(l_n*36)+34] += A[69] * B[(l_n*36)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*36)+20]);
    __m128 a20_0 = _mm_load_ss(&A[70]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*36)+20], c20_0);
#else
    C[(l_n*36)+20] += A[70] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*36)+21]);
    __m128 a21_0 = _mm_load_ss(&A[71]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*36)+21], c21_0);
#else
    C[(l_n*36)+21] += A[71] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a22_0 = _mm_load_ss(&A[72]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*36)+22], c22_0);
#else
    C[(l_n*36)+22] += A[72] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a23_0 = _mm_load_ss(&A[73]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*36)+23], c23_0);
#else
    C[(l_n*36)+23] += A[73] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*36)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*36)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a24_0 = _mm_load_ss(&A[74]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*36)+24], c24_0);
#else
    C[(l_n*36)+24] += A[74] * B[(l_n*36)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a25_0 = _mm_load_ss(&A[75]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*36)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a25_1 = _mm_load_ss(&A[76]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*36)+25], c25_1);
#else
    C[(l_n*36)+10] += A[75] * B[(l_n*36)+25];
    C[(l_n*36)+25] += A[76] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a26_0 = _mm_load_ss(&A[77]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*36)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a26_1 = _mm_load_ss(&A[78]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*36)+26], c26_1);
#else
    C[(l_n*36)+11] += A[77] * B[(l_n*36)+26];
    C[(l_n*36)+26] += A[78] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a27_0 = _mm_load_ss(&A[79]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*36)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a27_1 = _mm_load_ss(&A[80]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*36)+27], c27_1);
#else
    C[(l_n*36)+12] += A[79] * B[(l_n*36)+27];
    C[(l_n*36)+27] += A[80] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*36)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*36)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a28_0 = _mm_load_ss(&A[81]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*36)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a28_1 = _mm_load_ss(&A[82]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*36)+28], c28_1);
#else
    C[(l_n*36)+13] += A[81] * B[(l_n*36)+28];
    C[(l_n*36)+28] += A[82] * B[(l_n*36)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a29_0 = _mm_load_ss(&A[83]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*36)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a29_1 = _mm_load_ss(&A[84]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*36)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a29_2 = _mm_load_ss(&A[85]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*36)+29], c29_2);
#else
    C[(l_n*36)+4] += A[83] * B[(l_n*36)+29];
    C[(l_n*36)+14] += A[84] * B[(l_n*36)+29];
    C[(l_n*36)+29] += A[85] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a30_0 = _mm_load_ss(&A[86]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*36)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a30_1 = _mm_load_ss(&A[87]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*36)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a30_2 = _mm_load_ss(&A[88]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*36)+30], c30_2);
#else
    C[(l_n*36)+5] += A[86] * B[(l_n*36)+30];
    C[(l_n*36)+15] += A[87] * B[(l_n*36)+30];
    C[(l_n*36)+30] += A[88] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*36)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*36)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a31_0 = _mm_load_ss(&A[89]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*36)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a31_1 = _mm_load_ss(&A[90]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*36)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a31_2 = _mm_load_ss(&A[91]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*36)+31], c31_2);
#else
    C[(l_n*36)+6] += A[89] * B[(l_n*36)+31];
    C[(l_n*36)+16] += A[90] * B[(l_n*36)+31];
    C[(l_n*36)+31] += A[91] * B[(l_n*36)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a32_0 = _mm_load_ss(&A[92]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*36)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a32_1 = _mm_load_ss(&A[93]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*36)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a32_2 = _mm_load_ss(&A[94]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*36)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a32_3 = _mm_load_ss(&A[95]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*36)+32], c32_3);
#else
    C[(l_n*36)+1] += A[92] * B[(l_n*36)+32];
    C[(l_n*36)+7] += A[93] * B[(l_n*36)+32];
    C[(l_n*36)+17] += A[94] * B[(l_n*36)+32];
    C[(l_n*36)+32] += A[95] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*36)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*36)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a33_0 = _mm_load_ss(&A[96]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*36)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a33_1 = _mm_load_ss(&A[97]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*36)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a33_2 = _mm_load_ss(&A[98]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*36)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a33_3 = _mm_load_ss(&A[99]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*36)+33], c33_3);
#else
    C[(l_n*36)+2] += A[96] * B[(l_n*36)+33];
    C[(l_n*36)+8] += A[97] * B[(l_n*36)+33];
    C[(l_n*36)+18] += A[98] * B[(l_n*36)+33];
    C[(l_n*36)+33] += A[99] * B[(l_n*36)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*36)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*36)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a34_0 = _mm_load_ss(&A[100]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*36)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a34_1 = _mm_load_ss(&A[101]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*36)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a34_2 = _mm_load_ss(&A[102]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*36)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a34_3 = _mm_load_ss(&A[103]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*36)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a34_4 = _mm_load_ss(&A[104]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*36)+34], c34_4);
#else
    C[(l_n*36)+0] += A[100] * B[(l_n*36)+34];
    C[(l_n*36)+3] += A[101] * B[(l_n*36)+34];
    C[(l_n*36)+9] += A[102] * B[(l_n*36)+34];
    C[(l_n*36)+19] += A[103] * B[(l_n*36)+34];
    C[(l_n*36)+34] += A[104] * B[(l_n*36)+34];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1890;
#endif
}

void ssparse_fP113DivM_m35_n9_k35_ldAna5_ldB36_ldC36_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 35; l_m++) {
      C[(l_n*36)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*36)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*36)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*36)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*36)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*36)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*36)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*36)+34], c0_4);
#else
    C[(l_n*36)+0] += A[0] * B[(l_n*36)+0];
    C[(l_n*36)+3] += A[1] * B[(l_n*36)+0];
    C[(l_n*36)+9] += A[2] * B[(l_n*36)+0];
    C[(l_n*36)+19] += A[3] * B[(l_n*36)+0];
    C[(l_n*36)+34] += A[4] * B[(l_n*36)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a1_0 = _mm_load_ss(&A[5]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*36)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a1_1 = _mm_load_ss(&A[6]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*36)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a1_2 = _mm_load_ss(&A[7]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*36)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a1_3 = _mm_load_ss(&A[8]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*36)+32], c1_3);
#else
    C[(l_n*36)+1] += A[5] * B[(l_n*36)+1];
    C[(l_n*36)+7] += A[6] * B[(l_n*36)+1];
    C[(l_n*36)+17] += A[7] * B[(l_n*36)+1];
    C[(l_n*36)+32] += A[8] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*36)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*36)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a2_0 = _mm_load_ss(&A[9]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*36)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a2_1 = _mm_load_ss(&A[10]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*36)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a2_2 = _mm_load_ss(&A[11]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*36)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a2_3 = _mm_load_ss(&A[12]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*36)+33], c2_3);
#else
    C[(l_n*36)+2] += A[9] * B[(l_n*36)+2];
    C[(l_n*36)+8] += A[10] * B[(l_n*36)+2];
    C[(l_n*36)+18] += A[11] * B[(l_n*36)+2];
    C[(l_n*36)+33] += A[12] * B[(l_n*36)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*36)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*36)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a3_0 = _mm_load_ss(&A[13]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*36)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a3_1 = _mm_load_ss(&A[14]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*36)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a3_2 = _mm_load_ss(&A[15]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*36)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a3_3 = _mm_load_ss(&A[16]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*36)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a3_4 = _mm_load_ss(&A[17]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*36)+34], c3_4);
#else
    C[(l_n*36)+0] += A[13] * B[(l_n*36)+3];
    C[(l_n*36)+3] += A[14] * B[(l_n*36)+3];
    C[(l_n*36)+9] += A[15] * B[(l_n*36)+3];
    C[(l_n*36)+19] += A[16] * B[(l_n*36)+3];
    C[(l_n*36)+34] += A[17] * B[(l_n*36)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a4_0 = _mm_load_ss(&A[18]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*36)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a4_1 = _mm_load_ss(&A[19]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*36)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a4_2 = _mm_load_ss(&A[20]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*36)+29], c4_2);
#else
    C[(l_n*36)+4] += A[18] * B[(l_n*36)+4];
    C[(l_n*36)+14] += A[19] * B[(l_n*36)+4];
    C[(l_n*36)+29] += A[20] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a5_0 = _mm_load_ss(&A[21]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*36)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a5_1 = _mm_load_ss(&A[22]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*36)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a5_2 = _mm_load_ss(&A[23]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*36)+30], c5_2);
#else
    C[(l_n*36)+5] += A[21] * B[(l_n*36)+5];
    C[(l_n*36)+15] += A[22] * B[(l_n*36)+5];
    C[(l_n*36)+30] += A[23] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*36)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*36)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a6_0 = _mm_load_ss(&A[24]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*36)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a6_1 = _mm_load_ss(&A[25]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*36)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a6_2 = _mm_load_ss(&A[26]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*36)+31], c6_2);
#else
    C[(l_n*36)+6] += A[24] * B[(l_n*36)+6];
    C[(l_n*36)+16] += A[25] * B[(l_n*36)+6];
    C[(l_n*36)+31] += A[26] * B[(l_n*36)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a7_0 = _mm_load_ss(&A[27]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*36)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a7_1 = _mm_load_ss(&A[28]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*36)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a7_2 = _mm_load_ss(&A[29]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*36)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a7_3 = _mm_load_ss(&A[30]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*36)+32], c7_3);
#else
    C[(l_n*36)+1] += A[27] * B[(l_n*36)+7];
    C[(l_n*36)+7] += A[28] * B[(l_n*36)+7];
    C[(l_n*36)+17] += A[29] * B[(l_n*36)+7];
    C[(l_n*36)+32] += A[30] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*36)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*36)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a8_0 = _mm_load_ss(&A[31]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*36)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a8_1 = _mm_load_ss(&A[32]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*36)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a8_2 = _mm_load_ss(&A[33]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*36)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a8_3 = _mm_load_ss(&A[34]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*36)+33], c8_3);
#else
    C[(l_n*36)+2] += A[31] * B[(l_n*36)+8];
    C[(l_n*36)+8] += A[32] * B[(l_n*36)+8];
    C[(l_n*36)+18] += A[33] * B[(l_n*36)+8];
    C[(l_n*36)+33] += A[34] * B[(l_n*36)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*36)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*36)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a9_0 = _mm_load_ss(&A[35]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*36)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a9_1 = _mm_load_ss(&A[36]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*36)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a9_2 = _mm_load_ss(&A[37]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*36)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a9_3 = _mm_load_ss(&A[38]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*36)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a9_4 = _mm_load_ss(&A[39]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*36)+34], c9_4);
#else
    C[(l_n*36)+0] += A[35] * B[(l_n*36)+9];
    C[(l_n*36)+3] += A[36] * B[(l_n*36)+9];
    C[(l_n*36)+9] += A[37] * B[(l_n*36)+9];
    C[(l_n*36)+19] += A[38] * B[(l_n*36)+9];
    C[(l_n*36)+34] += A[39] * B[(l_n*36)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a10_0 = _mm_load_ss(&A[40]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*36)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a10_1 = _mm_load_ss(&A[41]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*36)+25], c10_1);
#else
    C[(l_n*36)+10] += A[40] * B[(l_n*36)+10];
    C[(l_n*36)+25] += A[41] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a11_0 = _mm_load_ss(&A[42]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*36)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a11_1 = _mm_load_ss(&A[43]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*36)+26], c11_1);
#else
    C[(l_n*36)+11] += A[42] * B[(l_n*36)+11];
    C[(l_n*36)+26] += A[43] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a12_0 = _mm_load_ss(&A[44]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*36)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a12_1 = _mm_load_ss(&A[45]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*36)+27], c12_1);
#else
    C[(l_n*36)+12] += A[44] * B[(l_n*36)+12];
    C[(l_n*36)+27] += A[45] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*36)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*36)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a13_0 = _mm_load_ss(&A[46]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*36)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a13_1 = _mm_load_ss(&A[47]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*36)+28], c13_1);
#else
    C[(l_n*36)+13] += A[46] * B[(l_n*36)+13];
    C[(l_n*36)+28] += A[47] * B[(l_n*36)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a14_0 = _mm_load_ss(&A[48]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*36)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a14_1 = _mm_load_ss(&A[49]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*36)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a14_2 = _mm_load_ss(&A[50]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*36)+29], c14_2);
#else
    C[(l_n*36)+4] += A[48] * B[(l_n*36)+14];
    C[(l_n*36)+14] += A[49] * B[(l_n*36)+14];
    C[(l_n*36)+29] += A[50] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a15_0 = _mm_load_ss(&A[51]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*36)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a15_1 = _mm_load_ss(&A[52]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*36)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a15_2 = _mm_load_ss(&A[53]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*36)+30], c15_2);
#else
    C[(l_n*36)+5] += A[51] * B[(l_n*36)+15];
    C[(l_n*36)+15] += A[52] * B[(l_n*36)+15];
    C[(l_n*36)+30] += A[53] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*36)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*36)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a16_0 = _mm_load_ss(&A[54]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*36)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a16_1 = _mm_load_ss(&A[55]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*36)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a16_2 = _mm_load_ss(&A[56]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*36)+31], c16_2);
#else
    C[(l_n*36)+6] += A[54] * B[(l_n*36)+16];
    C[(l_n*36)+16] += A[55] * B[(l_n*36)+16];
    C[(l_n*36)+31] += A[56] * B[(l_n*36)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a17_0 = _mm_load_ss(&A[57]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*36)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a17_1 = _mm_load_ss(&A[58]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*36)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a17_2 = _mm_load_ss(&A[59]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*36)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a17_3 = _mm_load_ss(&A[60]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*36)+32], c17_3);
#else
    C[(l_n*36)+1] += A[57] * B[(l_n*36)+17];
    C[(l_n*36)+7] += A[58] * B[(l_n*36)+17];
    C[(l_n*36)+17] += A[59] * B[(l_n*36)+17];
    C[(l_n*36)+32] += A[60] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*36)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*36)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a18_0 = _mm_load_ss(&A[61]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*36)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a18_1 = _mm_load_ss(&A[62]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*36)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a18_2 = _mm_load_ss(&A[63]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*36)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a18_3 = _mm_load_ss(&A[64]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*36)+33], c18_3);
#else
    C[(l_n*36)+2] += A[61] * B[(l_n*36)+18];
    C[(l_n*36)+8] += A[62] * B[(l_n*36)+18];
    C[(l_n*36)+18] += A[63] * B[(l_n*36)+18];
    C[(l_n*36)+33] += A[64] * B[(l_n*36)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*36)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*36)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a19_0 = _mm_load_ss(&A[65]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*36)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a19_1 = _mm_load_ss(&A[66]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*36)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a19_2 = _mm_load_ss(&A[67]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*36)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a19_3 = _mm_load_ss(&A[68]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*36)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a19_4 = _mm_load_ss(&A[69]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*36)+34], c19_4);
#else
    C[(l_n*36)+0] += A[65] * B[(l_n*36)+19];
    C[(l_n*36)+3] += A[66] * B[(l_n*36)+19];
    C[(l_n*36)+9] += A[67] * B[(l_n*36)+19];
    C[(l_n*36)+19] += A[68] * B[(l_n*36)+19];
    C[(l_n*36)+34] += A[69] * B[(l_n*36)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*36)+20]);
    __m128 a20_0 = _mm_load_ss(&A[70]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*36)+20], c20_0);
#else
    C[(l_n*36)+20] += A[70] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*36)+21]);
    __m128 a21_0 = _mm_load_ss(&A[71]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*36)+21], c21_0);
#else
    C[(l_n*36)+21] += A[71] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a22_0 = _mm_load_ss(&A[72]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*36)+22], c22_0);
#else
    C[(l_n*36)+22] += A[72] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*36)+23]);
    __m128 a23_0 = _mm_load_ss(&A[73]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*36)+23], c23_0);
#else
    C[(l_n*36)+23] += A[73] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*36)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*36)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a24_0 = _mm_load_ss(&A[74]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*36)+24], c24_0);
#else
    C[(l_n*36)+24] += A[74] * B[(l_n*36)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*36)+10]);
    __m128 a25_0 = _mm_load_ss(&A[75]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*36)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*36)+25]);
    __m128 a25_1 = _mm_load_ss(&A[76]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*36)+25], c25_1);
#else
    C[(l_n*36)+10] += A[75] * B[(l_n*36)+25];
    C[(l_n*36)+25] += A[76] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*36)+11]);
    __m128 a26_0 = _mm_load_ss(&A[77]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*36)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*36)+26]);
    __m128 a26_1 = _mm_load_ss(&A[78]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*36)+26], c26_1);
#else
    C[(l_n*36)+11] += A[77] * B[(l_n*36)+26];
    C[(l_n*36)+26] += A[78] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*36)+12]);
    __m128 a27_0 = _mm_load_ss(&A[79]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*36)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*36)+27]);
    __m128 a27_1 = _mm_load_ss(&A[80]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*36)+27], c27_1);
#else
    C[(l_n*36)+12] += A[79] * B[(l_n*36)+27];
    C[(l_n*36)+27] += A[80] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*36)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*36)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*36)+13]);
    __m128 a28_0 = _mm_load_ss(&A[81]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*36)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*36)+28]);
    __m128 a28_1 = _mm_load_ss(&A[82]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*36)+28], c28_1);
#else
    C[(l_n*36)+13] += A[81] * B[(l_n*36)+28];
    C[(l_n*36)+28] += A[82] * B[(l_n*36)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*36)+4]);
    __m128 a29_0 = _mm_load_ss(&A[83]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*36)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*36)+14]);
    __m128 a29_1 = _mm_load_ss(&A[84]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*36)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*36)+29]);
    __m128 a29_2 = _mm_load_ss(&A[85]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*36)+29], c29_2);
#else
    C[(l_n*36)+4] += A[83] * B[(l_n*36)+29];
    C[(l_n*36)+14] += A[84] * B[(l_n*36)+29];
    C[(l_n*36)+29] += A[85] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*36)+5]);
    __m128 a30_0 = _mm_load_ss(&A[86]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*36)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*36)+15]);
    __m128 a30_1 = _mm_load_ss(&A[87]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*36)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*36)+30]);
    __m128 a30_2 = _mm_load_ss(&A[88]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*36)+30], c30_2);
#else
    C[(l_n*36)+5] += A[86] * B[(l_n*36)+30];
    C[(l_n*36)+15] += A[87] * B[(l_n*36)+30];
    C[(l_n*36)+30] += A[88] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*36)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*36)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a31_0 = _mm_load_ss(&A[89]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*36)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a31_1 = _mm_load_ss(&A[90]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*36)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a31_2 = _mm_load_ss(&A[91]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*36)+31], c31_2);
#else
    C[(l_n*36)+6] += A[89] * B[(l_n*36)+31];
    C[(l_n*36)+16] += A[90] * B[(l_n*36)+31];
    C[(l_n*36)+31] += A[91] * B[(l_n*36)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*36)+1]);
    __m128 a32_0 = _mm_load_ss(&A[92]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*36)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*36)+7]);
    __m128 a32_1 = _mm_load_ss(&A[93]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*36)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*36)+17]);
    __m128 a32_2 = _mm_load_ss(&A[94]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*36)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*36)+32]);
    __m128 a32_3 = _mm_load_ss(&A[95]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*36)+32], c32_3);
#else
    C[(l_n*36)+1] += A[92] * B[(l_n*36)+32];
    C[(l_n*36)+7] += A[93] * B[(l_n*36)+32];
    C[(l_n*36)+17] += A[94] * B[(l_n*36)+32];
    C[(l_n*36)+32] += A[95] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*36)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*36)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*36)+2]);
    __m128 a33_0 = _mm_load_ss(&A[96]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*36)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*36)+8]);
    __m128 a33_1 = _mm_load_ss(&A[97]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*36)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*36)+18]);
    __m128 a33_2 = _mm_load_ss(&A[98]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*36)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*36)+33]);
    __m128 a33_3 = _mm_load_ss(&A[99]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*36)+33], c33_3);
#else
    C[(l_n*36)+2] += A[96] * B[(l_n*36)+33];
    C[(l_n*36)+8] += A[97] * B[(l_n*36)+33];
    C[(l_n*36)+18] += A[98] * B[(l_n*36)+33];
    C[(l_n*36)+33] += A[99] * B[(l_n*36)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*36)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*36)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a34_0 = _mm_load_ss(&A[100]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*36)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a34_1 = _mm_load_ss(&A[101]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*36)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a34_2 = _mm_load_ss(&A[102]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*36)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a34_3 = _mm_load_ss(&A[103]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*36)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a34_4 = _mm_load_ss(&A[104]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*36)+34], c34_4);
#else
    C[(l_n*36)+0] += A[100] * B[(l_n*36)+34];
    C[(l_n*36)+3] += A[101] * B[(l_n*36)+34];
    C[(l_n*36)+9] += A[102] * B[(l_n*36)+34];
    C[(l_n*36)+19] += A[103] * B[(l_n*36)+34];
    C[(l_n*36)+34] += A[104] * B[(l_n*36)+34];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 1890;
#endif
}

void ssparse_fP111DivM_m35_n9_k35_ldAna5_ldB36_ldC36_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 35; l_m++) {
      C[(l_n*36)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*36)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*36)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*36)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*36)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*36)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*36)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*36)+34], c0_4);
#else
    C[(l_n*36)+0] += A[0] * B[(l_n*36)+0];
    C[(l_n*36)+3] += A[1] * B[(l_n*36)+0];
    C[(l_n*36)+9] += A[2] * B[(l_n*36)+0];
    C[(l_n*36)+19] += A[3] * B[(l_n*36)+0];
    C[(l_n*36)+34] += A[4] * B[(l_n*36)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*36)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*36)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[5]));
    c1_0 = _mm_add_ps(c1_0, _mm_mul_ps(a1_0, b1));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c1_0));
    __m128 c1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[7]));
    c1_2 = _mm_add_ps(c1_2, _mm_mul_ps(a1_2, b1));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c1_2));
    __m128 c1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[9]));
    c1_4 = _mm_add_ps(c1_4, _mm_mul_ps(a1_4, b1));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c1_4));
    __m128 c1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[11]));
    c1_6 = _mm_add_ps(c1_6, _mm_mul_ps(a1_6, b1));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c1_6));
#else
    C[(l_n*36)+1] += A[5] * B[(l_n*36)+1];
    C[(l_n*36)+2] += A[6] * B[(l_n*36)+1];
    C[(l_n*36)+7] += A[7] * B[(l_n*36)+1];
    C[(l_n*36)+8] += A[8] * B[(l_n*36)+1];
    C[(l_n*36)+17] += A[9] * B[(l_n*36)+1];
    C[(l_n*36)+18] += A[10] * B[(l_n*36)+1];
    C[(l_n*36)+32] += A[11] * B[(l_n*36)+1];
    C[(l_n*36)+33] += A[12] * B[(l_n*36)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*36)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*36)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[13]));
    c2_0 = _mm_add_ps(c2_0, _mm_mul_ps(a2_0, b2));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c2_0));
    __m128 c2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[15]));
    c2_2 = _mm_add_ps(c2_2, _mm_mul_ps(a2_2, b2));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c2_2));
    __m128 c2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[17]));
    c2_4 = _mm_add_ps(c2_4, _mm_mul_ps(a2_4, b2));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c2_4));
    __m128 c2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[19]));
    c2_6 = _mm_add_ps(c2_6, _mm_mul_ps(a2_6, b2));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c2_6));
#else
    C[(l_n*36)+1] += A[13] * B[(l_n*36)+2];
    C[(l_n*36)+2] += A[14] * B[(l_n*36)+2];
    C[(l_n*36)+7] += A[15] * B[(l_n*36)+2];
    C[(l_n*36)+8] += A[16] * B[(l_n*36)+2];
    C[(l_n*36)+17] += A[17] * B[(l_n*36)+2];
    C[(l_n*36)+18] += A[18] * B[(l_n*36)+2];
    C[(l_n*36)+32] += A[19] * B[(l_n*36)+2];
    C[(l_n*36)+33] += A[20] * B[(l_n*36)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*36)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*36)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a3_0 = _mm_load_ss(&A[21]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*36)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a3_1 = _mm_load_ss(&A[22]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*36)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a3_2 = _mm_load_ss(&A[23]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*36)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a3_3 = _mm_load_ss(&A[24]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*36)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a3_4 = _mm_load_ss(&A[25]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*36)+34], c3_4);
#else
    C[(l_n*36)+0] += A[21] * B[(l_n*36)+3];
    C[(l_n*36)+3] += A[22] * B[(l_n*36)+3];
    C[(l_n*36)+9] += A[23] * B[(l_n*36)+3];
    C[(l_n*36)+19] += A[24] * B[(l_n*36)+3];
    C[(l_n*36)+34] += A[25] * B[(l_n*36)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*36)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*36)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[26]));
    c4_0 = _mm_add_ps(c4_0, _mm_mul_ps(a4_0, b4));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c4_0));
    __m128 c4_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a4_2 = _mm_load_ss(&A[28]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*36)+6], c4_2);
    __m128 c4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[29]));
    c4_3 = _mm_add_ps(c4_3, _mm_mul_ps(a4_3, b4));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c4_3));
    __m128 c4_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a4_5 = _mm_load_ss(&A[31]);
    c4_5 = _mm_add_ss(c4_5, _mm_mul_ss(a4_5, b4));
    _mm_store_ss(&C[(l_n*36)+16], c4_5);
    __m128 c4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[32]));
    c4_6 = _mm_add_ps(c4_6, _mm_mul_ps(a4_6, b4));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c4_6));
    __m128 c4_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a4_8 = _mm_load_ss(&A[34]);
    c4_8 = _mm_add_ss(c4_8, _mm_mul_ss(a4_8, b4));
    _mm_store_ss(&C[(l_n*36)+31], c4_8);
#else
    C[(l_n*36)+4] += A[26] * B[(l_n*36)+4];
    C[(l_n*36)+5] += A[27] * B[(l_n*36)+4];
    C[(l_n*36)+6] += A[28] * B[(l_n*36)+4];
    C[(l_n*36)+14] += A[29] * B[(l_n*36)+4];
    C[(l_n*36)+15] += A[30] * B[(l_n*36)+4];
    C[(l_n*36)+16] += A[31] * B[(l_n*36)+4];
    C[(l_n*36)+29] += A[32] * B[(l_n*36)+4];
    C[(l_n*36)+30] += A[33] * B[(l_n*36)+4];
    C[(l_n*36)+31] += A[34] * B[(l_n*36)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*36)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*36)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[35]));
    c5_0 = _mm_add_ps(c5_0, _mm_mul_ps(a5_0, b5));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c5_0));
    __m128 c5_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a5_2 = _mm_load_ss(&A[37]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*36)+6], c5_2);
    __m128 c5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[38]));
    c5_3 = _mm_add_ps(c5_3, _mm_mul_ps(a5_3, b5));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c5_3));
    __m128 c5_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a5_5 = _mm_load_ss(&A[40]);
    c5_5 = _mm_add_ss(c5_5, _mm_mul_ss(a5_5, b5));
    _mm_store_ss(&C[(l_n*36)+16], c5_5);
    __m128 c5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c5_6 = _mm_add_ps(c5_6, _mm_mul_ps(a5_6, b5));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c5_6));
    __m128 c5_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a5_8 = _mm_load_ss(&A[43]);
    c5_8 = _mm_add_ss(c5_8, _mm_mul_ss(a5_8, b5));
    _mm_store_ss(&C[(l_n*36)+31], c5_8);
#else
    C[(l_n*36)+4] += A[35] * B[(l_n*36)+5];
    C[(l_n*36)+5] += A[36] * B[(l_n*36)+5];
    C[(l_n*36)+6] += A[37] * B[(l_n*36)+5];
    C[(l_n*36)+14] += A[38] * B[(l_n*36)+5];
    C[(l_n*36)+15] += A[39] * B[(l_n*36)+5];
    C[(l_n*36)+16] += A[40] * B[(l_n*36)+5];
    C[(l_n*36)+29] += A[41] * B[(l_n*36)+5];
    C[(l_n*36)+30] += A[42] * B[(l_n*36)+5];
    C[(l_n*36)+31] += A[43] * B[(l_n*36)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*36)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*36)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[44]));
    c6_0 = _mm_add_ps(c6_0, _mm_mul_ps(a6_0, b6));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c6_0));
    __m128 c6_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a6_2 = _mm_load_ss(&A[46]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*36)+6], c6_2);
    __m128 c6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[47]));
    c6_3 = _mm_add_ps(c6_3, _mm_mul_ps(a6_3, b6));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c6_3));
    __m128 c6_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a6_5 = _mm_load_ss(&A[49]);
    c6_5 = _mm_add_ss(c6_5, _mm_mul_ss(a6_5, b6));
    _mm_store_ss(&C[(l_n*36)+16], c6_5);
    __m128 c6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[50]));
    c6_6 = _mm_add_ps(c6_6, _mm_mul_ps(a6_6, b6));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c6_6));
    __m128 c6_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a6_8 = _mm_load_ss(&A[52]);
    c6_8 = _mm_add_ss(c6_8, _mm_mul_ss(a6_8, b6));
    _mm_store_ss(&C[(l_n*36)+31], c6_8);
#else
    C[(l_n*36)+4] += A[44] * B[(l_n*36)+6];
    C[(l_n*36)+5] += A[45] * B[(l_n*36)+6];
    C[(l_n*36)+6] += A[46] * B[(l_n*36)+6];
    C[(l_n*36)+14] += A[47] * B[(l_n*36)+6];
    C[(l_n*36)+15] += A[48] * B[(l_n*36)+6];
    C[(l_n*36)+16] += A[49] * B[(l_n*36)+6];
    C[(l_n*36)+29] += A[50] * B[(l_n*36)+6];
    C[(l_n*36)+30] += A[51] * B[(l_n*36)+6];
    C[(l_n*36)+31] += A[52] * B[(l_n*36)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*36)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*36)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[53]));
    c7_0 = _mm_add_ps(c7_0, _mm_mul_ps(a7_0, b7));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c7_0));
    __m128 c7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[55]));
    c7_2 = _mm_add_ps(c7_2, _mm_mul_ps(a7_2, b7));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c7_2));
    __m128 c7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[57]));
    c7_4 = _mm_add_ps(c7_4, _mm_mul_ps(a7_4, b7));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c7_4));
    __m128 c7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[59]));
    c7_6 = _mm_add_ps(c7_6, _mm_mul_ps(a7_6, b7));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c7_6));
#else
    C[(l_n*36)+1] += A[53] * B[(l_n*36)+7];
    C[(l_n*36)+2] += A[54] * B[(l_n*36)+7];
    C[(l_n*36)+7] += A[55] * B[(l_n*36)+7];
    C[(l_n*36)+8] += A[56] * B[(l_n*36)+7];
    C[(l_n*36)+17] += A[57] * B[(l_n*36)+7];
    C[(l_n*36)+18] += A[58] * B[(l_n*36)+7];
    C[(l_n*36)+32] += A[59] * B[(l_n*36)+7];
    C[(l_n*36)+33] += A[60] * B[(l_n*36)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*36)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*36)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[61]));
    c8_0 = _mm_add_ps(c8_0, _mm_mul_ps(a8_0, b8));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c8_0));
    __m128 c8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[63]));
    c8_2 = _mm_add_ps(c8_2, _mm_mul_ps(a8_2, b8));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c8_2));
    __m128 c8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[65]));
    c8_4 = _mm_add_ps(c8_4, _mm_mul_ps(a8_4, b8));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c8_4));
    __m128 c8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[67]));
    c8_6 = _mm_add_ps(c8_6, _mm_mul_ps(a8_6, b8));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c8_6));
#else
    C[(l_n*36)+1] += A[61] * B[(l_n*36)+8];
    C[(l_n*36)+2] += A[62] * B[(l_n*36)+8];
    C[(l_n*36)+7] += A[63] * B[(l_n*36)+8];
    C[(l_n*36)+8] += A[64] * B[(l_n*36)+8];
    C[(l_n*36)+17] += A[65] * B[(l_n*36)+8];
    C[(l_n*36)+18] += A[66] * B[(l_n*36)+8];
    C[(l_n*36)+32] += A[67] * B[(l_n*36)+8];
    C[(l_n*36)+33] += A[68] * B[(l_n*36)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*36)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*36)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a9_0 = _mm_load_ss(&A[69]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*36)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a9_1 = _mm_load_ss(&A[70]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*36)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a9_2 = _mm_load_ss(&A[71]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*36)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a9_3 = _mm_load_ss(&A[72]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*36)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a9_4 = _mm_load_ss(&A[73]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*36)+34], c9_4);
#else
    C[(l_n*36)+0] += A[69] * B[(l_n*36)+9];
    C[(l_n*36)+3] += A[70] * B[(l_n*36)+9];
    C[(l_n*36)+9] += A[71] * B[(l_n*36)+9];
    C[(l_n*36)+19] += A[72] * B[(l_n*36)+9];
    C[(l_n*36)+34] += A[73] * B[(l_n*36)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*36)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*36)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a10_0 = _mm_loadu_ps(&A[74]);
    c10_0 = _mm_add_ps(c10_0, _mm_mul_ps(a10_0, b10));
    _mm_storeu_ps(&C[(l_n*36)+10], c10_0);
    __m128 c10_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a10_4 = _mm_loadu_ps(&A[78]);
    c10_4 = _mm_add_ps(c10_4, _mm_mul_ps(a10_4, b10));
    _mm_storeu_ps(&C[(l_n*36)+25], c10_4);
#else
    C[(l_n*36)+10] += A[74] * B[(l_n*36)+10];
    C[(l_n*36)+11] += A[75] * B[(l_n*36)+10];
    C[(l_n*36)+12] += A[76] * B[(l_n*36)+10];
    C[(l_n*36)+13] += A[77] * B[(l_n*36)+10];
    C[(l_n*36)+25] += A[78] * B[(l_n*36)+10];
    C[(l_n*36)+26] += A[79] * B[(l_n*36)+10];
    C[(l_n*36)+27] += A[80] * B[(l_n*36)+10];
    C[(l_n*36)+28] += A[81] * B[(l_n*36)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*36)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*36)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a11_0 = _mm_loadu_ps(&A[82]);
    c11_0 = _mm_add_ps(c11_0, _mm_mul_ps(a11_0, b11));
    _mm_storeu_ps(&C[(l_n*36)+10], c11_0);
    __m128 c11_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a11_4 = _mm_loadu_ps(&A[86]);
    c11_4 = _mm_add_ps(c11_4, _mm_mul_ps(a11_4, b11));
    _mm_storeu_ps(&C[(l_n*36)+25], c11_4);
#else
    C[(l_n*36)+10] += A[82] * B[(l_n*36)+11];
    C[(l_n*36)+11] += A[83] * B[(l_n*36)+11];
    C[(l_n*36)+12] += A[84] * B[(l_n*36)+11];
    C[(l_n*36)+13] += A[85] * B[(l_n*36)+11];
    C[(l_n*36)+25] += A[86] * B[(l_n*36)+11];
    C[(l_n*36)+26] += A[87] * B[(l_n*36)+11];
    C[(l_n*36)+27] += A[88] * B[(l_n*36)+11];
    C[(l_n*36)+28] += A[89] * B[(l_n*36)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*36)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*36)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a12_0 = _mm_loadu_ps(&A[90]);
    c12_0 = _mm_add_ps(c12_0, _mm_mul_ps(a12_0, b12));
    _mm_storeu_ps(&C[(l_n*36)+10], c12_0);
    __m128 c12_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a12_4 = _mm_loadu_ps(&A[94]);
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_storeu_ps(&C[(l_n*36)+25], c12_4);
#else
    C[(l_n*36)+10] += A[90] * B[(l_n*36)+12];
    C[(l_n*36)+11] += A[91] * B[(l_n*36)+12];
    C[(l_n*36)+12] += A[92] * B[(l_n*36)+12];
    C[(l_n*36)+13] += A[93] * B[(l_n*36)+12];
    C[(l_n*36)+25] += A[94] * B[(l_n*36)+12];
    C[(l_n*36)+26] += A[95] * B[(l_n*36)+12];
    C[(l_n*36)+27] += A[96] * B[(l_n*36)+12];
    C[(l_n*36)+28] += A[97] * B[(l_n*36)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*36)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*36)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a13_0 = _mm_loadu_ps(&A[98]);
    c13_0 = _mm_add_ps(c13_0, _mm_mul_ps(a13_0, b13));
    _mm_storeu_ps(&C[(l_n*36)+10], c13_0);
    __m128 c13_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a13_4 = _mm_loadu_ps(&A[102]);
    c13_4 = _mm_add_ps(c13_4, _mm_mul_ps(a13_4, b13));
    _mm_storeu_ps(&C[(l_n*36)+25], c13_4);
#else
    C[(l_n*36)+10] += A[98] * B[(l_n*36)+13];
    C[(l_n*36)+11] += A[99] * B[(l_n*36)+13];
    C[(l_n*36)+12] += A[100] * B[(l_n*36)+13];
    C[(l_n*36)+13] += A[101] * B[(l_n*36)+13];
    C[(l_n*36)+25] += A[102] * B[(l_n*36)+13];
    C[(l_n*36)+26] += A[103] * B[(l_n*36)+13];
    C[(l_n*36)+27] += A[104] * B[(l_n*36)+13];
    C[(l_n*36)+28] += A[105] * B[(l_n*36)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*36)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*36)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[106]));
    c14_0 = _mm_add_ps(c14_0, _mm_mul_ps(a14_0, b14));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c14_0));
    __m128 c14_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a14_2 = _mm_load_ss(&A[108]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*36)+6], c14_2);
    __m128 c14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[109]));
    c14_3 = _mm_add_ps(c14_3, _mm_mul_ps(a14_3, b14));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c14_3));
    __m128 c14_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a14_5 = _mm_load_ss(&A[111]);
    c14_5 = _mm_add_ss(c14_5, _mm_mul_ss(a14_5, b14));
    _mm_store_ss(&C[(l_n*36)+16], c14_5);
    __m128 c14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[112]));
    c14_6 = _mm_add_ps(c14_6, _mm_mul_ps(a14_6, b14));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c14_6));
    __m128 c14_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a14_8 = _mm_load_ss(&A[114]);
    c14_8 = _mm_add_ss(c14_8, _mm_mul_ss(a14_8, b14));
    _mm_store_ss(&C[(l_n*36)+31], c14_8);
#else
    C[(l_n*36)+4] += A[106] * B[(l_n*36)+14];
    C[(l_n*36)+5] += A[107] * B[(l_n*36)+14];
    C[(l_n*36)+6] += A[108] * B[(l_n*36)+14];
    C[(l_n*36)+14] += A[109] * B[(l_n*36)+14];
    C[(l_n*36)+15] += A[110] * B[(l_n*36)+14];
    C[(l_n*36)+16] += A[111] * B[(l_n*36)+14];
    C[(l_n*36)+29] += A[112] * B[(l_n*36)+14];
    C[(l_n*36)+30] += A[113] * B[(l_n*36)+14];
    C[(l_n*36)+31] += A[114] * B[(l_n*36)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*36)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*36)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[115]));
    c15_0 = _mm_add_ps(c15_0, _mm_mul_ps(a15_0, b15));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c15_0));
    __m128 c15_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a15_2 = _mm_load_ss(&A[117]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*36)+6], c15_2);
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[118]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c15_3));
    __m128 c15_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a15_5 = _mm_load_ss(&A[120]);
    c15_5 = _mm_add_ss(c15_5, _mm_mul_ss(a15_5, b15));
    _mm_store_ss(&C[(l_n*36)+16], c15_5);
    __m128 c15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[121]));
    c15_6 = _mm_add_ps(c15_6, _mm_mul_ps(a15_6, b15));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c15_6));
    __m128 c15_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a15_8 = _mm_load_ss(&A[123]);
    c15_8 = _mm_add_ss(c15_8, _mm_mul_ss(a15_8, b15));
    _mm_store_ss(&C[(l_n*36)+31], c15_8);
#else
    C[(l_n*36)+4] += A[115] * B[(l_n*36)+15];
    C[(l_n*36)+5] += A[116] * B[(l_n*36)+15];
    C[(l_n*36)+6] += A[117] * B[(l_n*36)+15];
    C[(l_n*36)+14] += A[118] * B[(l_n*36)+15];
    C[(l_n*36)+15] += A[119] * B[(l_n*36)+15];
    C[(l_n*36)+16] += A[120] * B[(l_n*36)+15];
    C[(l_n*36)+29] += A[121] * B[(l_n*36)+15];
    C[(l_n*36)+30] += A[122] * B[(l_n*36)+15];
    C[(l_n*36)+31] += A[123] * B[(l_n*36)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*36)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*36)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[124]));
    c16_0 = _mm_add_ps(c16_0, _mm_mul_ps(a16_0, b16));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c16_0));
    __m128 c16_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a16_2 = _mm_load_ss(&A[126]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*36)+6], c16_2);
    __m128 c16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[127]));
    c16_3 = _mm_add_ps(c16_3, _mm_mul_ps(a16_3, b16));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c16_3));
    __m128 c16_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a16_5 = _mm_load_ss(&A[129]);
    c16_5 = _mm_add_ss(c16_5, _mm_mul_ss(a16_5, b16));
    _mm_store_ss(&C[(l_n*36)+16], c16_5);
    __m128 c16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[130]));
    c16_6 = _mm_add_ps(c16_6, _mm_mul_ps(a16_6, b16));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c16_6));
    __m128 c16_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a16_8 = _mm_load_ss(&A[132]);
    c16_8 = _mm_add_ss(c16_8, _mm_mul_ss(a16_8, b16));
    _mm_store_ss(&C[(l_n*36)+31], c16_8);
#else
    C[(l_n*36)+4] += A[124] * B[(l_n*36)+16];
    C[(l_n*36)+5] += A[125] * B[(l_n*36)+16];
    C[(l_n*36)+6] += A[126] * B[(l_n*36)+16];
    C[(l_n*36)+14] += A[127] * B[(l_n*36)+16];
    C[(l_n*36)+15] += A[128] * B[(l_n*36)+16];
    C[(l_n*36)+16] += A[129] * B[(l_n*36)+16];
    C[(l_n*36)+29] += A[130] * B[(l_n*36)+16];
    C[(l_n*36)+30] += A[131] * B[(l_n*36)+16];
    C[(l_n*36)+31] += A[132] * B[(l_n*36)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*36)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*36)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[133]));
    c17_0 = _mm_add_ps(c17_0, _mm_mul_ps(a17_0, b17));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c17_0));
    __m128 c17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[135]));
    c17_2 = _mm_add_ps(c17_2, _mm_mul_ps(a17_2, b17));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c17_2));
    __m128 c17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[137]));
    c17_4 = _mm_add_ps(c17_4, _mm_mul_ps(a17_4, b17));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c17_4));
    __m128 c17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[139]));
    c17_6 = _mm_add_ps(c17_6, _mm_mul_ps(a17_6, b17));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c17_6));
#else
    C[(l_n*36)+1] += A[133] * B[(l_n*36)+17];
    C[(l_n*36)+2] += A[134] * B[(l_n*36)+17];
    C[(l_n*36)+7] += A[135] * B[(l_n*36)+17];
    C[(l_n*36)+8] += A[136] * B[(l_n*36)+17];
    C[(l_n*36)+17] += A[137] * B[(l_n*36)+17];
    C[(l_n*36)+18] += A[138] * B[(l_n*36)+17];
    C[(l_n*36)+32] += A[139] * B[(l_n*36)+17];
    C[(l_n*36)+33] += A[140] * B[(l_n*36)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*36)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*36)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[141]));
    c18_0 = _mm_add_ps(c18_0, _mm_mul_ps(a18_0, b18));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c18_0));
    __m128 c18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[143]));
    c18_2 = _mm_add_ps(c18_2, _mm_mul_ps(a18_2, b18));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c18_2));
    __m128 c18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[145]));
    c18_4 = _mm_add_ps(c18_4, _mm_mul_ps(a18_4, b18));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c18_4));
    __m128 c18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[147]));
    c18_6 = _mm_add_ps(c18_6, _mm_mul_ps(a18_6, b18));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c18_6));
#else
    C[(l_n*36)+1] += A[141] * B[(l_n*36)+18];
    C[(l_n*36)+2] += A[142] * B[(l_n*36)+18];
    C[(l_n*36)+7] += A[143] * B[(l_n*36)+18];
    C[(l_n*36)+8] += A[144] * B[(l_n*36)+18];
    C[(l_n*36)+17] += A[145] * B[(l_n*36)+18];
    C[(l_n*36)+18] += A[146] * B[(l_n*36)+18];
    C[(l_n*36)+32] += A[147] * B[(l_n*36)+18];
    C[(l_n*36)+33] += A[148] * B[(l_n*36)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*36)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*36)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a19_0 = _mm_load_ss(&A[149]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*36)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a19_1 = _mm_load_ss(&A[150]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*36)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a19_2 = _mm_load_ss(&A[151]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*36)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a19_3 = _mm_load_ss(&A[152]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*36)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a19_4 = _mm_load_ss(&A[153]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*36)+34], c19_4);
#else
    C[(l_n*36)+0] += A[149] * B[(l_n*36)+19];
    C[(l_n*36)+3] += A[150] * B[(l_n*36)+19];
    C[(l_n*36)+9] += A[151] * B[(l_n*36)+19];
    C[(l_n*36)+19] += A[152] * B[(l_n*36)+19];
    C[(l_n*36)+34] += A[153] * B[(l_n*36)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*36)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*36)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_loadu_ps(&C[(l_n*36)+20]);
    __m128 a20_0 = _mm_loadu_ps(&A[154]);
    c20_0 = _mm_add_ps(c20_0, _mm_mul_ps(a20_0, b20));
    _mm_storeu_ps(&C[(l_n*36)+20], c20_0);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a20_4 = _mm_load_ss(&A[158]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*36)+24], c20_4);
#else
    C[(l_n*36)+20] += A[154] * B[(l_n*36)+20];
    C[(l_n*36)+21] += A[155] * B[(l_n*36)+20];
    C[(l_n*36)+22] += A[156] * B[(l_n*36)+20];
    C[(l_n*36)+23] += A[157] * B[(l_n*36)+20];
    C[(l_n*36)+24] += A[158] * B[(l_n*36)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*36)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*36)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+20]));
    __m128 a21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[159]));
    c21_0 = _mm_add_ps(c21_0, _mm_mul_ps(a21_0, b21));
    _mm_store_sd((double*)&C[(l_n*36)+20], _mm_castps_pd(c21_0));
    __m128 c21_2 = _mm_load_ss(&C[(l_n*36)+22]);
    __m128 a21_2 = _mm_load_ss(&A[161]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*36)+22], c21_2);
    __m128 c21_3 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a21_3 = _mm_load_ss(&A[162]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*36)+24], c21_3);
#else
    C[(l_n*36)+20] += A[159] * B[(l_n*36)+21];
    C[(l_n*36)+21] += A[160] * B[(l_n*36)+21];
    C[(l_n*36)+22] += A[161] * B[(l_n*36)+21];
    C[(l_n*36)+24] += A[162] * B[(l_n*36)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*36)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*36)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_loadu_ps(&C[(l_n*36)+20]);
    __m128 a22_0 = _mm_loadu_ps(&A[163]);
    c22_0 = _mm_add_ps(c22_0, _mm_mul_ps(a22_0, b22));
    _mm_storeu_ps(&C[(l_n*36)+20], c22_0);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a22_4 = _mm_load_ss(&A[167]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*36)+24], c22_4);
#else
    C[(l_n*36)+20] += A[163] * B[(l_n*36)+22];
    C[(l_n*36)+21] += A[164] * B[(l_n*36)+22];
    C[(l_n*36)+22] += A[165] * B[(l_n*36)+22];
    C[(l_n*36)+23] += A[166] * B[(l_n*36)+22];
    C[(l_n*36)+24] += A[167] * B[(l_n*36)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*36)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*36)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*36)+20]);
    __m128 a23_0 = _mm_load_ss(&A[168]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*36)+20], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+22]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[169]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*36)+22], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a23_3 = _mm_load_ss(&A[171]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*36)+24], c23_3);
#else
    C[(l_n*36)+20] += A[168] * B[(l_n*36)+23];
    C[(l_n*36)+22] += A[169] * B[(l_n*36)+23];
    C[(l_n*36)+23] += A[170] * B[(l_n*36)+23];
    C[(l_n*36)+24] += A[171] * B[(l_n*36)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*36)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*36)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_loadu_ps(&C[(l_n*36)+20]);
    __m128 a24_0 = _mm_loadu_ps(&A[172]);
    c24_0 = _mm_add_ps(c24_0, _mm_mul_ps(a24_0, b24));
    _mm_storeu_ps(&C[(l_n*36)+20], c24_0);
    __m128 c24_4 = _mm_load_ss(&C[(l_n*36)+24]);
    __m128 a24_4 = _mm_load_ss(&A[176]);
    c24_4 = _mm_add_ss(c24_4, _mm_mul_ss(a24_4, b24));
    _mm_store_ss(&C[(l_n*36)+24], c24_4);
#else
    C[(l_n*36)+20] += A[172] * B[(l_n*36)+24];
    C[(l_n*36)+21] += A[173] * B[(l_n*36)+24];
    C[(l_n*36)+22] += A[174] * B[(l_n*36)+24];
    C[(l_n*36)+23] += A[175] * B[(l_n*36)+24];
    C[(l_n*36)+24] += A[176] * B[(l_n*36)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*36)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*36)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a25_0 = _mm_loadu_ps(&A[177]);
    c25_0 = _mm_add_ps(c25_0, _mm_mul_ps(a25_0, b25));
    _mm_storeu_ps(&C[(l_n*36)+10], c25_0);
    __m128 c25_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a25_4 = _mm_loadu_ps(&A[181]);
    c25_4 = _mm_add_ps(c25_4, _mm_mul_ps(a25_4, b25));
    _mm_storeu_ps(&C[(l_n*36)+25], c25_4);
#else
    C[(l_n*36)+10] += A[177] * B[(l_n*36)+25];
    C[(l_n*36)+11] += A[178] * B[(l_n*36)+25];
    C[(l_n*36)+12] += A[179] * B[(l_n*36)+25];
    C[(l_n*36)+13] += A[180] * B[(l_n*36)+25];
    C[(l_n*36)+25] += A[181] * B[(l_n*36)+25];
    C[(l_n*36)+26] += A[182] * B[(l_n*36)+25];
    C[(l_n*36)+27] += A[183] * B[(l_n*36)+25];
    C[(l_n*36)+28] += A[184] * B[(l_n*36)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*36)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*36)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a26_0 = _mm_loadu_ps(&A[185]);
    c26_0 = _mm_add_ps(c26_0, _mm_mul_ps(a26_0, b26));
    _mm_storeu_ps(&C[(l_n*36)+10], c26_0);
    __m128 c26_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a26_4 = _mm_loadu_ps(&A[189]);
    c26_4 = _mm_add_ps(c26_4, _mm_mul_ps(a26_4, b26));
    _mm_storeu_ps(&C[(l_n*36)+25], c26_4);
#else
    C[(l_n*36)+10] += A[185] * B[(l_n*36)+26];
    C[(l_n*36)+11] += A[186] * B[(l_n*36)+26];
    C[(l_n*36)+12] += A[187] * B[(l_n*36)+26];
    C[(l_n*36)+13] += A[188] * B[(l_n*36)+26];
    C[(l_n*36)+25] += A[189] * B[(l_n*36)+26];
    C[(l_n*36)+26] += A[190] * B[(l_n*36)+26];
    C[(l_n*36)+27] += A[191] * B[(l_n*36)+26];
    C[(l_n*36)+28] += A[192] * B[(l_n*36)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*36)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*36)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a27_0 = _mm_loadu_ps(&A[193]);
    c27_0 = _mm_add_ps(c27_0, _mm_mul_ps(a27_0, b27));
    _mm_storeu_ps(&C[(l_n*36)+10], c27_0);
    __m128 c27_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a27_4 = _mm_loadu_ps(&A[197]);
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_storeu_ps(&C[(l_n*36)+25], c27_4);
#else
    C[(l_n*36)+10] += A[193] * B[(l_n*36)+27];
    C[(l_n*36)+11] += A[194] * B[(l_n*36)+27];
    C[(l_n*36)+12] += A[195] * B[(l_n*36)+27];
    C[(l_n*36)+13] += A[196] * B[(l_n*36)+27];
    C[(l_n*36)+25] += A[197] * B[(l_n*36)+27];
    C[(l_n*36)+26] += A[198] * B[(l_n*36)+27];
    C[(l_n*36)+27] += A[199] * B[(l_n*36)+27];
    C[(l_n*36)+28] += A[200] * B[(l_n*36)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*36)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*36)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_loadu_ps(&C[(l_n*36)+10]);
    __m128 a28_0 = _mm_loadu_ps(&A[201]);
    c28_0 = _mm_add_ps(c28_0, _mm_mul_ps(a28_0, b28));
    _mm_storeu_ps(&C[(l_n*36)+10], c28_0);
    __m128 c28_4 = _mm_loadu_ps(&C[(l_n*36)+25]);
    __m128 a28_4 = _mm_loadu_ps(&A[205]);
    c28_4 = _mm_add_ps(c28_4, _mm_mul_ps(a28_4, b28));
    _mm_storeu_ps(&C[(l_n*36)+25], c28_4);
#else
    C[(l_n*36)+10] += A[201] * B[(l_n*36)+28];
    C[(l_n*36)+11] += A[202] * B[(l_n*36)+28];
    C[(l_n*36)+12] += A[203] * B[(l_n*36)+28];
    C[(l_n*36)+13] += A[204] * B[(l_n*36)+28];
    C[(l_n*36)+25] += A[205] * B[(l_n*36)+28];
    C[(l_n*36)+26] += A[206] * B[(l_n*36)+28];
    C[(l_n*36)+27] += A[207] * B[(l_n*36)+28];
    C[(l_n*36)+28] += A[208] * B[(l_n*36)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*36)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*36)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[209]));
    c29_0 = _mm_add_ps(c29_0, _mm_mul_ps(a29_0, b29));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c29_0));
    __m128 c29_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a29_2 = _mm_load_ss(&A[211]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*36)+6], c29_2);
    __m128 c29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[212]));
    c29_3 = _mm_add_ps(c29_3, _mm_mul_ps(a29_3, b29));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c29_3));
    __m128 c29_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a29_5 = _mm_load_ss(&A[214]);
    c29_5 = _mm_add_ss(c29_5, _mm_mul_ss(a29_5, b29));
    _mm_store_ss(&C[(l_n*36)+16], c29_5);
    __m128 c29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[215]));
    c29_6 = _mm_add_ps(c29_6, _mm_mul_ps(a29_6, b29));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c29_6));
    __m128 c29_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a29_8 = _mm_load_ss(&A[217]);
    c29_8 = _mm_add_ss(c29_8, _mm_mul_ss(a29_8, b29));
    _mm_store_ss(&C[(l_n*36)+31], c29_8);
#else
    C[(l_n*36)+4] += A[209] * B[(l_n*36)+29];
    C[(l_n*36)+5] += A[210] * B[(l_n*36)+29];
    C[(l_n*36)+6] += A[211] * B[(l_n*36)+29];
    C[(l_n*36)+14] += A[212] * B[(l_n*36)+29];
    C[(l_n*36)+15] += A[213] * B[(l_n*36)+29];
    C[(l_n*36)+16] += A[214] * B[(l_n*36)+29];
    C[(l_n*36)+29] += A[215] * B[(l_n*36)+29];
    C[(l_n*36)+30] += A[216] * B[(l_n*36)+29];
    C[(l_n*36)+31] += A[217] * B[(l_n*36)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*36)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*36)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[218]));
    c30_0 = _mm_add_ps(c30_0, _mm_mul_ps(a30_0, b30));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c30_0));
    __m128 c30_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a30_2 = _mm_load_ss(&A[220]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*36)+6], c30_2);
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[221]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a30_5 = _mm_load_ss(&A[223]);
    c30_5 = _mm_add_ss(c30_5, _mm_mul_ss(a30_5, b30));
    _mm_store_ss(&C[(l_n*36)+16], c30_5);
    __m128 c30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[224]));
    c30_6 = _mm_add_ps(c30_6, _mm_mul_ps(a30_6, b30));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c30_6));
    __m128 c30_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a30_8 = _mm_load_ss(&A[226]);
    c30_8 = _mm_add_ss(c30_8, _mm_mul_ss(a30_8, b30));
    _mm_store_ss(&C[(l_n*36)+31], c30_8);
#else
    C[(l_n*36)+4] += A[218] * B[(l_n*36)+30];
    C[(l_n*36)+5] += A[219] * B[(l_n*36)+30];
    C[(l_n*36)+6] += A[220] * B[(l_n*36)+30];
    C[(l_n*36)+14] += A[221] * B[(l_n*36)+30];
    C[(l_n*36)+15] += A[222] * B[(l_n*36)+30];
    C[(l_n*36)+16] += A[223] * B[(l_n*36)+30];
    C[(l_n*36)+29] += A[224] * B[(l_n*36)+30];
    C[(l_n*36)+30] += A[225] * B[(l_n*36)+30];
    C[(l_n*36)+31] += A[226] * B[(l_n*36)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*36)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*36)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+4]));
    __m128 a31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[227]));
    c31_0 = _mm_add_ps(c31_0, _mm_mul_ps(a31_0, b31));
    _mm_store_sd((double*)&C[(l_n*36)+4], _mm_castps_pd(c31_0));
    __m128 c31_2 = _mm_load_ss(&C[(l_n*36)+6]);
    __m128 a31_2 = _mm_load_ss(&A[229]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*36)+6], c31_2);
    __m128 c31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+14]));
    __m128 a31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[230]));
    c31_3 = _mm_add_ps(c31_3, _mm_mul_ps(a31_3, b31));
    _mm_store_sd((double*)&C[(l_n*36)+14], _mm_castps_pd(c31_3));
    __m128 c31_5 = _mm_load_ss(&C[(l_n*36)+16]);
    __m128 a31_5 = _mm_load_ss(&A[232]);
    c31_5 = _mm_add_ss(c31_5, _mm_mul_ss(a31_5, b31));
    _mm_store_ss(&C[(l_n*36)+16], c31_5);
    __m128 c31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+29]));
    __m128 a31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[233]));
    c31_6 = _mm_add_ps(c31_6, _mm_mul_ps(a31_6, b31));
    _mm_store_sd((double*)&C[(l_n*36)+29], _mm_castps_pd(c31_6));
    __m128 c31_8 = _mm_load_ss(&C[(l_n*36)+31]);
    __m128 a31_8 = _mm_load_ss(&A[235]);
    c31_8 = _mm_add_ss(c31_8, _mm_mul_ss(a31_8, b31));
    _mm_store_ss(&C[(l_n*36)+31], c31_8);
#else
    C[(l_n*36)+4] += A[227] * B[(l_n*36)+31];
    C[(l_n*36)+5] += A[228] * B[(l_n*36)+31];
    C[(l_n*36)+6] += A[229] * B[(l_n*36)+31];
    C[(l_n*36)+14] += A[230] * B[(l_n*36)+31];
    C[(l_n*36)+15] += A[231] * B[(l_n*36)+31];
    C[(l_n*36)+16] += A[232] * B[(l_n*36)+31];
    C[(l_n*36)+29] += A[233] * B[(l_n*36)+31];
    C[(l_n*36)+30] += A[234] * B[(l_n*36)+31];
    C[(l_n*36)+31] += A[235] * B[(l_n*36)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*36)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*36)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[236]));
    c32_0 = _mm_add_ps(c32_0, _mm_mul_ps(a32_0, b32));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c32_0));
    __m128 c32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[238]));
    c32_2 = _mm_add_ps(c32_2, _mm_mul_ps(a32_2, b32));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c32_2));
    __m128 c32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[240]));
    c32_4 = _mm_add_ps(c32_4, _mm_mul_ps(a32_4, b32));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c32_4));
    __m128 c32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[242]));
    c32_6 = _mm_add_ps(c32_6, _mm_mul_ps(a32_6, b32));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c32_6));
#else
    C[(l_n*36)+1] += A[236] * B[(l_n*36)+32];
    C[(l_n*36)+2] += A[237] * B[(l_n*36)+32];
    C[(l_n*36)+7] += A[238] * B[(l_n*36)+32];
    C[(l_n*36)+8] += A[239] * B[(l_n*36)+32];
    C[(l_n*36)+17] += A[240] * B[(l_n*36)+32];
    C[(l_n*36)+18] += A[241] * B[(l_n*36)+32];
    C[(l_n*36)+32] += A[242] * B[(l_n*36)+32];
    C[(l_n*36)+33] += A[243] * B[(l_n*36)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*36)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*36)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+1]));
    __m128 a33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[244]));
    c33_0 = _mm_add_ps(c33_0, _mm_mul_ps(a33_0, b33));
    _mm_store_sd((double*)&C[(l_n*36)+1], _mm_castps_pd(c33_0));
    __m128 c33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+7]));
    __m128 a33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[246]));
    c33_2 = _mm_add_ps(c33_2, _mm_mul_ps(a33_2, b33));
    _mm_store_sd((double*)&C[(l_n*36)+7], _mm_castps_pd(c33_2));
    __m128 c33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+17]));
    __m128 a33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[248]));
    c33_4 = _mm_add_ps(c33_4, _mm_mul_ps(a33_4, b33));
    _mm_store_sd((double*)&C[(l_n*36)+17], _mm_castps_pd(c33_4));
    __m128 c33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*36)+32]));
    __m128 a33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[250]));
    c33_6 = _mm_add_ps(c33_6, _mm_mul_ps(a33_6, b33));
    _mm_store_sd((double*)&C[(l_n*36)+32], _mm_castps_pd(c33_6));
#else
    C[(l_n*36)+1] += A[244] * B[(l_n*36)+33];
    C[(l_n*36)+2] += A[245] * B[(l_n*36)+33];
    C[(l_n*36)+7] += A[246] * B[(l_n*36)+33];
    C[(l_n*36)+8] += A[247] * B[(l_n*36)+33];
    C[(l_n*36)+17] += A[248] * B[(l_n*36)+33];
    C[(l_n*36)+18] += A[249] * B[(l_n*36)+33];
    C[(l_n*36)+32] += A[250] * B[(l_n*36)+33];
    C[(l_n*36)+33] += A[251] * B[(l_n*36)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*36)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*36)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*36)+0]);
    __m128 a34_0 = _mm_load_ss(&A[252]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*36)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*36)+3]);
    __m128 a34_1 = _mm_load_ss(&A[253]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*36)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*36)+9]);
    __m128 a34_2 = _mm_load_ss(&A[254]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*36)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*36)+19]);
    __m128 a34_3 = _mm_load_ss(&A[255]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*36)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*36)+34]);
    __m128 a34_4 = _mm_load_ss(&A[256]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*36)+34], c34_4);
#else
    C[(l_n*36)+0] += A[252] * B[(l_n*36)+34];
    C[(l_n*36)+3] += A[253] * B[(l_n*36)+34];
    C[(l_n*36)+9] += A[254] * B[(l_n*36)+34];
    C[(l_n*36)+19] += A[255] * B[(l_n*36)+34];
    C[(l_n*36)+34] += A[256] * B[(l_n*36)+34];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 4626;
#endif
}

void ssparse_starMatrix_m56_n9_k9_ldA56_ldBna6_ldC56_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 56; l_m++) {
    C[0+l_m] += A[336+l_m] * B[0];
    C[0+l_m] += A[392+l_m] * B[1];
    C[0+l_m] += A[448+l_m] * B[2];
    C[56+l_m] += A[336+l_m] * B[3];
    C[56+l_m] += A[392+l_m] * B[4];
    C[56+l_m] += A[448+l_m] * B[5];
    C[112+l_m] += A[336+l_m] * B[6];
    C[112+l_m] += A[392+l_m] * B[7];
    C[112+l_m] += A[448+l_m] * B[8];
    C[168+l_m] += A[336+l_m] * B[9];
    C[168+l_m] += A[392+l_m] * B[10];
    C[224+l_m] += A[392+l_m] * B[11];
    C[224+l_m] += A[448+l_m] * B[12];
    C[280+l_m] += A[336+l_m] * B[13];
    C[280+l_m] += A[448+l_m] * B[14];
    C[336+l_m] += A[0+l_m] * B[15];
    C[336+l_m] += A[168+l_m] * B[16];
    C[336+l_m] += A[280+l_m] * B[17];
    C[392+l_m] += A[56+l_m] * B[18];
    C[392+l_m] += A[168+l_m] * B[19];
    C[392+l_m] += A[224+l_m] * B[20];
    C[448+l_m] += A[112+l_m] * B[21];
    C[448+l_m] += A[224+l_m] * B[22];
    C[448+l_m] += A[280+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 2688;
#endif
}

void ssparse_fM1DivM_m56_n9_k56_ldAna6_ldB56_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*56)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*56)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*56)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*56)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*56)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*56)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*56)+55], c0_5);
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*56)+0];
    C[(l_n*56)+3] += A[1] * B[(l_n*56)+0];
    C[(l_n*56)+9] += A[2] * B[(l_n*56)+0];
    C[(l_n*56)+19] += A[3] * B[(l_n*56)+0];
    C[(l_n*56)+34] += A[4] * B[(l_n*56)+0];
    C[(l_n*56)+55] += A[5] * B[(l_n*56)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a1_0 = _mm_load_ss(&A[6]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*56)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a1_1 = _mm_load_ss(&A[7]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*56)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a1_2 = _mm_load_ss(&A[8]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*56)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a1_3 = _mm_load_ss(&A[9]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*56)+32], c1_3);
    __m128 c1_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a1_4 = _mm_load_ss(&A[10]);
    c1_4 = _mm_add_ss(c1_4, _mm_mul_ss(a1_4, b1));
    _mm_store_ss(&C[(l_n*56)+53], c1_4);
#else
    C[(l_n*56)+1] += A[6] * B[(l_n*56)+1];
    C[(l_n*56)+7] += A[7] * B[(l_n*56)+1];
    C[(l_n*56)+17] += A[8] * B[(l_n*56)+1];
    C[(l_n*56)+32] += A[9] * B[(l_n*56)+1];
    C[(l_n*56)+53] += A[10] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*56)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a2_0 = _mm_load_ss(&A[11]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*56)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a2_1 = _mm_load_ss(&A[12]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*56)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a2_2 = _mm_load_ss(&A[13]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*56)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a2_3 = _mm_load_ss(&A[14]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*56)+33], c2_3);
    __m128 c2_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a2_4 = _mm_load_ss(&A[15]);
    c2_4 = _mm_add_ss(c2_4, _mm_mul_ss(a2_4, b2));
    _mm_store_ss(&C[(l_n*56)+54], c2_4);
#else
    C[(l_n*56)+2] += A[11] * B[(l_n*56)+2];
    C[(l_n*56)+8] += A[12] * B[(l_n*56)+2];
    C[(l_n*56)+18] += A[13] * B[(l_n*56)+2];
    C[(l_n*56)+33] += A[14] * B[(l_n*56)+2];
    C[(l_n*56)+54] += A[15] * B[(l_n*56)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*56)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a3_0 = _mm_load_ss(&A[16]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*56)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a3_1 = _mm_load_ss(&A[17]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*56)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a3_2 = _mm_load_ss(&A[18]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*56)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a3_3 = _mm_load_ss(&A[19]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*56)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a3_4 = _mm_load_ss(&A[20]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*56)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a3_5 = _mm_load_ss(&A[21]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*56)+55], c3_5);
#else
    C[(l_n*56)+0] += A[16] * B[(l_n*56)+3];
    C[(l_n*56)+3] += A[17] * B[(l_n*56)+3];
    C[(l_n*56)+9] += A[18] * B[(l_n*56)+3];
    C[(l_n*56)+19] += A[19] * B[(l_n*56)+3];
    C[(l_n*56)+34] += A[20] * B[(l_n*56)+3];
    C[(l_n*56)+55] += A[21] * B[(l_n*56)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a4_0 = _mm_load_ss(&A[22]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*56)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a4_1 = _mm_load_ss(&A[23]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*56)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a4_2 = _mm_load_ss(&A[24]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*56)+29], c4_2);
    __m128 c4_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a4_3 = _mm_load_ss(&A[25]);
    c4_3 = _mm_add_ss(c4_3, _mm_mul_ss(a4_3, b4));
    _mm_store_ss(&C[(l_n*56)+50], c4_3);
#else
    C[(l_n*56)+4] += A[22] * B[(l_n*56)+4];
    C[(l_n*56)+14] += A[23] * B[(l_n*56)+4];
    C[(l_n*56)+29] += A[24] * B[(l_n*56)+4];
    C[(l_n*56)+50] += A[25] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a5_0 = _mm_load_ss(&A[26]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*56)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a5_1 = _mm_load_ss(&A[27]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*56)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a5_2 = _mm_load_ss(&A[28]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*56)+30], c5_2);
    __m128 c5_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a5_3 = _mm_load_ss(&A[29]);
    c5_3 = _mm_add_ss(c5_3, _mm_mul_ss(a5_3, b5));
    _mm_store_ss(&C[(l_n*56)+51], c5_3);
#else
    C[(l_n*56)+5] += A[26] * B[(l_n*56)+5];
    C[(l_n*56)+15] += A[27] * B[(l_n*56)+5];
    C[(l_n*56)+30] += A[28] * B[(l_n*56)+5];
    C[(l_n*56)+51] += A[29] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*56)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a6_0 = _mm_load_ss(&A[30]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*56)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a6_1 = _mm_load_ss(&A[31]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*56)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a6_2 = _mm_load_ss(&A[32]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*56)+31], c6_2);
    __m128 c6_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a6_3 = _mm_load_ss(&A[33]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*56)+52], c6_3);
#else
    C[(l_n*56)+6] += A[30] * B[(l_n*56)+6];
    C[(l_n*56)+16] += A[31] * B[(l_n*56)+6];
    C[(l_n*56)+31] += A[32] * B[(l_n*56)+6];
    C[(l_n*56)+52] += A[33] * B[(l_n*56)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a7_0 = _mm_load_ss(&A[34]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*56)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a7_1 = _mm_load_ss(&A[35]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*56)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a7_2 = _mm_load_ss(&A[36]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*56)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a7_3 = _mm_load_ss(&A[37]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*56)+32], c7_3);
    __m128 c7_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a7_4 = _mm_load_ss(&A[38]);
    c7_4 = _mm_add_ss(c7_4, _mm_mul_ss(a7_4, b7));
    _mm_store_ss(&C[(l_n*56)+53], c7_4);
#else
    C[(l_n*56)+1] += A[34] * B[(l_n*56)+7];
    C[(l_n*56)+7] += A[35] * B[(l_n*56)+7];
    C[(l_n*56)+17] += A[36] * B[(l_n*56)+7];
    C[(l_n*56)+32] += A[37] * B[(l_n*56)+7];
    C[(l_n*56)+53] += A[38] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*56)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a8_0 = _mm_load_ss(&A[39]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*56)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a8_1 = _mm_load_ss(&A[40]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*56)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a8_2 = _mm_load_ss(&A[41]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*56)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a8_3 = _mm_load_ss(&A[42]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*56)+33], c8_3);
    __m128 c8_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a8_4 = _mm_load_ss(&A[43]);
    c8_4 = _mm_add_ss(c8_4, _mm_mul_ss(a8_4, b8));
    _mm_store_ss(&C[(l_n*56)+54], c8_4);
#else
    C[(l_n*56)+2] += A[39] * B[(l_n*56)+8];
    C[(l_n*56)+8] += A[40] * B[(l_n*56)+8];
    C[(l_n*56)+18] += A[41] * B[(l_n*56)+8];
    C[(l_n*56)+33] += A[42] * B[(l_n*56)+8];
    C[(l_n*56)+54] += A[43] * B[(l_n*56)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*56)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a9_0 = _mm_load_ss(&A[44]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*56)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a9_1 = _mm_load_ss(&A[45]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*56)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a9_2 = _mm_load_ss(&A[46]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*56)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a9_3 = _mm_load_ss(&A[47]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*56)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a9_4 = _mm_load_ss(&A[48]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*56)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a9_5 = _mm_load_ss(&A[49]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*56)+55], c9_5);
#else
    C[(l_n*56)+0] += A[44] * B[(l_n*56)+9];
    C[(l_n*56)+3] += A[45] * B[(l_n*56)+9];
    C[(l_n*56)+9] += A[46] * B[(l_n*56)+9];
    C[(l_n*56)+19] += A[47] * B[(l_n*56)+9];
    C[(l_n*56)+34] += A[48] * B[(l_n*56)+9];
    C[(l_n*56)+55] += A[49] * B[(l_n*56)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a10_0 = _mm_load_ss(&A[50]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*56)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a10_1 = _mm_load_ss(&A[51]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*56)+25], c10_1);
    __m128 c10_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a10_2 = _mm_load_ss(&A[52]);
    c10_2 = _mm_add_ss(c10_2, _mm_mul_ss(a10_2, b10));
    _mm_store_ss(&C[(l_n*56)+46], c10_2);
#else
    C[(l_n*56)+10] += A[50] * B[(l_n*56)+10];
    C[(l_n*56)+25] += A[51] * B[(l_n*56)+10];
    C[(l_n*56)+46] += A[52] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a11_0 = _mm_load_ss(&A[53]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*56)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a11_1 = _mm_load_ss(&A[54]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*56)+26], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a11_2 = _mm_load_ss(&A[55]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*56)+47], c11_2);
#else
    C[(l_n*56)+11] += A[53] * B[(l_n*56)+11];
    C[(l_n*56)+26] += A[54] * B[(l_n*56)+11];
    C[(l_n*56)+47] += A[55] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a12_0 = _mm_load_ss(&A[56]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*56)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a12_1 = _mm_load_ss(&A[57]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*56)+27], c12_1);
    __m128 c12_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a12_2 = _mm_load_ss(&A[58]);
    c12_2 = _mm_add_ss(c12_2, _mm_mul_ss(a12_2, b12));
    _mm_store_ss(&C[(l_n*56)+48], c12_2);
#else
    C[(l_n*56)+12] += A[56] * B[(l_n*56)+12];
    C[(l_n*56)+27] += A[57] * B[(l_n*56)+12];
    C[(l_n*56)+48] += A[58] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*56)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a13_0 = _mm_load_ss(&A[59]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*56)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a13_1 = _mm_load_ss(&A[60]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*56)+28], c13_1);
    __m128 c13_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a13_2 = _mm_load_ss(&A[61]);
    c13_2 = _mm_add_ss(c13_2, _mm_mul_ss(a13_2, b13));
    _mm_store_ss(&C[(l_n*56)+49], c13_2);
#else
    C[(l_n*56)+13] += A[59] * B[(l_n*56)+13];
    C[(l_n*56)+28] += A[60] * B[(l_n*56)+13];
    C[(l_n*56)+49] += A[61] * B[(l_n*56)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a14_0 = _mm_load_ss(&A[62]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*56)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a14_1 = _mm_load_ss(&A[63]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*56)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a14_2 = _mm_load_ss(&A[64]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*56)+29], c14_2);
    __m128 c14_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a14_3 = _mm_load_ss(&A[65]);
    c14_3 = _mm_add_ss(c14_3, _mm_mul_ss(a14_3, b14));
    _mm_store_ss(&C[(l_n*56)+50], c14_3);
#else
    C[(l_n*56)+4] += A[62] * B[(l_n*56)+14];
    C[(l_n*56)+14] += A[63] * B[(l_n*56)+14];
    C[(l_n*56)+29] += A[64] * B[(l_n*56)+14];
    C[(l_n*56)+50] += A[65] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a15_0 = _mm_load_ss(&A[66]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*56)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a15_1 = _mm_load_ss(&A[67]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*56)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a15_2 = _mm_load_ss(&A[68]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*56)+30], c15_2);
    __m128 c15_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a15_3 = _mm_load_ss(&A[69]);
    c15_3 = _mm_add_ss(c15_3, _mm_mul_ss(a15_3, b15));
    _mm_store_ss(&C[(l_n*56)+51], c15_3);
#else
    C[(l_n*56)+5] += A[66] * B[(l_n*56)+15];
    C[(l_n*56)+15] += A[67] * B[(l_n*56)+15];
    C[(l_n*56)+30] += A[68] * B[(l_n*56)+15];
    C[(l_n*56)+51] += A[69] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*56)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a16_0 = _mm_load_ss(&A[70]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*56)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a16_1 = _mm_load_ss(&A[71]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*56)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a16_2 = _mm_load_ss(&A[72]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*56)+31], c16_2);
    __m128 c16_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a16_3 = _mm_load_ss(&A[73]);
    c16_3 = _mm_add_ss(c16_3, _mm_mul_ss(a16_3, b16));
    _mm_store_ss(&C[(l_n*56)+52], c16_3);
#else
    C[(l_n*56)+6] += A[70] * B[(l_n*56)+16];
    C[(l_n*56)+16] += A[71] * B[(l_n*56)+16];
    C[(l_n*56)+31] += A[72] * B[(l_n*56)+16];
    C[(l_n*56)+52] += A[73] * B[(l_n*56)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a17_0 = _mm_load_ss(&A[74]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*56)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a17_1 = _mm_load_ss(&A[75]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*56)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a17_2 = _mm_load_ss(&A[76]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*56)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a17_3 = _mm_load_ss(&A[77]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*56)+32], c17_3);
    __m128 c17_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a17_4 = _mm_load_ss(&A[78]);
    c17_4 = _mm_add_ss(c17_4, _mm_mul_ss(a17_4, b17));
    _mm_store_ss(&C[(l_n*56)+53], c17_4);
#else
    C[(l_n*56)+1] += A[74] * B[(l_n*56)+17];
    C[(l_n*56)+7] += A[75] * B[(l_n*56)+17];
    C[(l_n*56)+17] += A[76] * B[(l_n*56)+17];
    C[(l_n*56)+32] += A[77] * B[(l_n*56)+17];
    C[(l_n*56)+53] += A[78] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*56)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a18_0 = _mm_load_ss(&A[79]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*56)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a18_1 = _mm_load_ss(&A[80]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*56)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a18_2 = _mm_load_ss(&A[81]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*56)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a18_3 = _mm_load_ss(&A[82]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*56)+33], c18_3);
    __m128 c18_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a18_4 = _mm_load_ss(&A[83]);
    c18_4 = _mm_add_ss(c18_4, _mm_mul_ss(a18_4, b18));
    _mm_store_ss(&C[(l_n*56)+54], c18_4);
#else
    C[(l_n*56)+2] += A[79] * B[(l_n*56)+18];
    C[(l_n*56)+8] += A[80] * B[(l_n*56)+18];
    C[(l_n*56)+18] += A[81] * B[(l_n*56)+18];
    C[(l_n*56)+33] += A[82] * B[(l_n*56)+18];
    C[(l_n*56)+54] += A[83] * B[(l_n*56)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*56)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a19_0 = _mm_load_ss(&A[84]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*56)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a19_1 = _mm_load_ss(&A[85]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*56)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a19_2 = _mm_load_ss(&A[86]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*56)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a19_3 = _mm_load_ss(&A[87]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*56)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a19_4 = _mm_load_ss(&A[88]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*56)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a19_5 = _mm_load_ss(&A[89]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*56)+55], c19_5);
#else
    C[(l_n*56)+0] += A[84] * B[(l_n*56)+19];
    C[(l_n*56)+3] += A[85] * B[(l_n*56)+19];
    C[(l_n*56)+9] += A[86] * B[(l_n*56)+19];
    C[(l_n*56)+19] += A[87] * B[(l_n*56)+19];
    C[(l_n*56)+34] += A[88] * B[(l_n*56)+19];
    C[(l_n*56)+55] += A[89] * B[(l_n*56)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a20_0 = _mm_load_ss(&A[90]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*56)+20], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a20_1 = _mm_load_ss(&A[91]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*56)+41], c20_1);
#else
    C[(l_n*56)+20] += A[90] * B[(l_n*56)+20];
    C[(l_n*56)+41] += A[91] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a21_0 = _mm_load_ss(&A[92]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*56)+21], c21_0);
    __m128 c21_1 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a21_1 = _mm_load_ss(&A[93]);
    c21_1 = _mm_add_ss(c21_1, _mm_mul_ss(a21_1, b21));
    _mm_store_ss(&C[(l_n*56)+42], c21_1);
#else
    C[(l_n*56)+21] += A[92] * B[(l_n*56)+21];
    C[(l_n*56)+42] += A[93] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a22_0 = _mm_load_ss(&A[94]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*56)+22], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a22_1 = _mm_load_ss(&A[95]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*56)+43], c22_1);
#else
    C[(l_n*56)+22] += A[94] * B[(l_n*56)+22];
    C[(l_n*56)+43] += A[95] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a23_0 = _mm_load_ss(&A[96]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+23], c23_0);
    __m128 c23_1 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a23_1 = _mm_load_ss(&A[97]);
    c23_1 = _mm_add_ss(c23_1, _mm_mul_ss(a23_1, b23));
    _mm_store_ss(&C[(l_n*56)+44], c23_1);
#else
    C[(l_n*56)+23] += A[96] * B[(l_n*56)+23];
    C[(l_n*56)+44] += A[97] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*56)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a24_0 = _mm_load_ss(&A[98]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*56)+24], c24_0);
    __m128 c24_1 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a24_1 = _mm_load_ss(&A[99]);
    c24_1 = _mm_add_ss(c24_1, _mm_mul_ss(a24_1, b24));
    _mm_store_ss(&C[(l_n*56)+45], c24_1);
#else
    C[(l_n*56)+24] += A[98] * B[(l_n*56)+24];
    C[(l_n*56)+45] += A[99] * B[(l_n*56)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a25_0 = _mm_load_ss(&A[100]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*56)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a25_1 = _mm_load_ss(&A[101]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*56)+25], c25_1);
    __m128 c25_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a25_2 = _mm_load_ss(&A[102]);
    c25_2 = _mm_add_ss(c25_2, _mm_mul_ss(a25_2, b25));
    _mm_store_ss(&C[(l_n*56)+46], c25_2);
#else
    C[(l_n*56)+10] += A[100] * B[(l_n*56)+25];
    C[(l_n*56)+25] += A[101] * B[(l_n*56)+25];
    C[(l_n*56)+46] += A[102] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a26_0 = _mm_load_ss(&A[103]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*56)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a26_1 = _mm_load_ss(&A[104]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*56)+26], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a26_2 = _mm_load_ss(&A[105]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*56)+47], c26_2);
#else
    C[(l_n*56)+11] += A[103] * B[(l_n*56)+26];
    C[(l_n*56)+26] += A[104] * B[(l_n*56)+26];
    C[(l_n*56)+47] += A[105] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a27_0 = _mm_load_ss(&A[106]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*56)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a27_1 = _mm_load_ss(&A[107]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*56)+27], c27_1);
    __m128 c27_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a27_2 = _mm_load_ss(&A[108]);
    c27_2 = _mm_add_ss(c27_2, _mm_mul_ss(a27_2, b27));
    _mm_store_ss(&C[(l_n*56)+48], c27_2);
#else
    C[(l_n*56)+12] += A[106] * B[(l_n*56)+27];
    C[(l_n*56)+27] += A[107] * B[(l_n*56)+27];
    C[(l_n*56)+48] += A[108] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*56)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a28_0 = _mm_load_ss(&A[109]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*56)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a28_1 = _mm_load_ss(&A[110]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*56)+28], c28_1);
    __m128 c28_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a28_2 = _mm_load_ss(&A[111]);
    c28_2 = _mm_add_ss(c28_2, _mm_mul_ss(a28_2, b28));
    _mm_store_ss(&C[(l_n*56)+49], c28_2);
#else
    C[(l_n*56)+13] += A[109] * B[(l_n*56)+28];
    C[(l_n*56)+28] += A[110] * B[(l_n*56)+28];
    C[(l_n*56)+49] += A[111] * B[(l_n*56)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a29_0 = _mm_load_ss(&A[112]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*56)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a29_1 = _mm_load_ss(&A[113]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*56)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a29_2 = _mm_load_ss(&A[114]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+29], c29_2);
    __m128 c29_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a29_3 = _mm_load_ss(&A[115]);
    c29_3 = _mm_add_ss(c29_3, _mm_mul_ss(a29_3, b29));
    _mm_store_ss(&C[(l_n*56)+50], c29_3);
#else
    C[(l_n*56)+4] += A[112] * B[(l_n*56)+29];
    C[(l_n*56)+14] += A[113] * B[(l_n*56)+29];
    C[(l_n*56)+29] += A[114] * B[(l_n*56)+29];
    C[(l_n*56)+50] += A[115] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a30_0 = _mm_load_ss(&A[116]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*56)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a30_1 = _mm_load_ss(&A[117]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*56)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a30_2 = _mm_load_ss(&A[118]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*56)+30], c30_2);
    __m128 c30_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a30_3 = _mm_load_ss(&A[119]);
    c30_3 = _mm_add_ss(c30_3, _mm_mul_ss(a30_3, b30));
    _mm_store_ss(&C[(l_n*56)+51], c30_3);
#else
    C[(l_n*56)+5] += A[116] * B[(l_n*56)+30];
    C[(l_n*56)+15] += A[117] * B[(l_n*56)+30];
    C[(l_n*56)+30] += A[118] * B[(l_n*56)+30];
    C[(l_n*56)+51] += A[119] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*56)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a31_0 = _mm_load_ss(&A[120]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*56)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a31_1 = _mm_load_ss(&A[121]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*56)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a31_2 = _mm_load_ss(&A[122]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*56)+31], c31_2);
    __m128 c31_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a31_3 = _mm_load_ss(&A[123]);
    c31_3 = _mm_add_ss(c31_3, _mm_mul_ss(a31_3, b31));
    _mm_store_ss(&C[(l_n*56)+52], c31_3);
#else
    C[(l_n*56)+6] += A[120] * B[(l_n*56)+31];
    C[(l_n*56)+16] += A[121] * B[(l_n*56)+31];
    C[(l_n*56)+31] += A[122] * B[(l_n*56)+31];
    C[(l_n*56)+52] += A[123] * B[(l_n*56)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a32_0 = _mm_load_ss(&A[124]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*56)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a32_1 = _mm_load_ss(&A[125]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*56)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a32_2 = _mm_load_ss(&A[126]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*56)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a32_3 = _mm_load_ss(&A[127]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*56)+32], c32_3);
    __m128 c32_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a32_4 = _mm_load_ss(&A[128]);
    c32_4 = _mm_add_ss(c32_4, _mm_mul_ss(a32_4, b32));
    _mm_store_ss(&C[(l_n*56)+53], c32_4);
#else
    C[(l_n*56)+1] += A[124] * B[(l_n*56)+32];
    C[(l_n*56)+7] += A[125] * B[(l_n*56)+32];
    C[(l_n*56)+17] += A[126] * B[(l_n*56)+32];
    C[(l_n*56)+32] += A[127] * B[(l_n*56)+32];
    C[(l_n*56)+53] += A[128] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*56)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a33_0 = _mm_load_ss(&A[129]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*56)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a33_1 = _mm_load_ss(&A[130]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*56)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a33_2 = _mm_load_ss(&A[131]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*56)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a33_3 = _mm_load_ss(&A[132]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*56)+33], c33_3);
    __m128 c33_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a33_4 = _mm_load_ss(&A[133]);
    c33_4 = _mm_add_ss(c33_4, _mm_mul_ss(a33_4, b33));
    _mm_store_ss(&C[(l_n*56)+54], c33_4);
#else
    C[(l_n*56)+2] += A[129] * B[(l_n*56)+33];
    C[(l_n*56)+8] += A[130] * B[(l_n*56)+33];
    C[(l_n*56)+18] += A[131] * B[(l_n*56)+33];
    C[(l_n*56)+33] += A[132] * B[(l_n*56)+33];
    C[(l_n*56)+54] += A[133] * B[(l_n*56)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*56)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a34_0 = _mm_load_ss(&A[134]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*56)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a34_1 = _mm_load_ss(&A[135]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*56)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a34_2 = _mm_load_ss(&A[136]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*56)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a34_3 = _mm_load_ss(&A[137]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*56)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a34_4 = _mm_load_ss(&A[138]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*56)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a34_5 = _mm_load_ss(&A[139]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*56)+55], c34_5);
#else
    C[(l_n*56)+0] += A[134] * B[(l_n*56)+34];
    C[(l_n*56)+3] += A[135] * B[(l_n*56)+34];
    C[(l_n*56)+9] += A[136] * B[(l_n*56)+34];
    C[(l_n*56)+19] += A[137] * B[(l_n*56)+34];
    C[(l_n*56)+34] += A[138] * B[(l_n*56)+34];
    C[(l_n*56)+55] += A[139] * B[(l_n*56)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*56)+35]);
    __m128 a35_0 = _mm_load_ss(&A[140]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*56)+35], c35_0);
#else
    C[(l_n*56)+35] += A[140] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*56)+36]);
    __m128 a36_0 = _mm_load_ss(&A[141]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*56)+36], c36_0);
#else
    C[(l_n*56)+36] += A[141] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a37_0 = _mm_load_ss(&A[142]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*56)+37], c37_0);
#else
    C[(l_n*56)+37] += A[142] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a38_0 = _mm_load_ss(&A[143]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*56)+38], c38_0);
#else
    C[(l_n*56)+38] += A[143] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a39_0 = _mm_load_ss(&A[144]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*56)+39], c39_0);
#else
    C[(l_n*56)+39] += A[144] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*56)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a40_0 = _mm_load_ss(&A[145]);
    c40_0 = _mm_add_ss(c40_0, _mm_mul_ss(a40_0, b40));
    _mm_store_ss(&C[(l_n*56)+40], c40_0);
#else
    C[(l_n*56)+40] += A[145] * B[(l_n*56)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a41_0 = _mm_load_ss(&A[146]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*56)+20], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a41_1 = _mm_load_ss(&A[147]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*56)+41], c41_1);
#else
    C[(l_n*56)+20] += A[146] * B[(l_n*56)+41];
    C[(l_n*56)+41] += A[147] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a42_0 = _mm_load_ss(&A[148]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*56)+21], c42_0);
    __m128 c42_1 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a42_1 = _mm_load_ss(&A[149]);
    c42_1 = _mm_add_ss(c42_1, _mm_mul_ss(a42_1, b42));
    _mm_store_ss(&C[(l_n*56)+42], c42_1);
#else
    C[(l_n*56)+21] += A[148] * B[(l_n*56)+42];
    C[(l_n*56)+42] += A[149] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a43_0 = _mm_load_ss(&A[150]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*56)+22], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a43_1 = _mm_load_ss(&A[151]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*56)+43], c43_1);
#else
    C[(l_n*56)+22] += A[150] * B[(l_n*56)+43];
    C[(l_n*56)+43] += A[151] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a44_0 = _mm_load_ss(&A[152]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+23], c44_0);
    __m128 c44_1 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a44_1 = _mm_load_ss(&A[153]);
    c44_1 = _mm_add_ss(c44_1, _mm_mul_ss(a44_1, b44));
    _mm_store_ss(&C[(l_n*56)+44], c44_1);
#else
    C[(l_n*56)+23] += A[152] * B[(l_n*56)+44];
    C[(l_n*56)+44] += A[153] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*56)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a45_0 = _mm_load_ss(&A[154]);
    c45_0 = _mm_add_ss(c45_0, _mm_mul_ss(a45_0, b45));
    _mm_store_ss(&C[(l_n*56)+24], c45_0);
    __m128 c45_1 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a45_1 = _mm_load_ss(&A[155]);
    c45_1 = _mm_add_ss(c45_1, _mm_mul_ss(a45_1, b45));
    _mm_store_ss(&C[(l_n*56)+45], c45_1);
#else
    C[(l_n*56)+24] += A[154] * B[(l_n*56)+45];
    C[(l_n*56)+45] += A[155] * B[(l_n*56)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a46_0 = _mm_load_ss(&A[156]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*56)+10], c46_0);
    __m128 c46_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a46_1 = _mm_load_ss(&A[157]);
    c46_1 = _mm_add_ss(c46_1, _mm_mul_ss(a46_1, b46));
    _mm_store_ss(&C[(l_n*56)+25], c46_1);
    __m128 c46_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a46_2 = _mm_load_ss(&A[158]);
    c46_2 = _mm_add_ss(c46_2, _mm_mul_ss(a46_2, b46));
    _mm_store_ss(&C[(l_n*56)+46], c46_2);
#else
    C[(l_n*56)+10] += A[156] * B[(l_n*56)+46];
    C[(l_n*56)+25] += A[157] * B[(l_n*56)+46];
    C[(l_n*56)+46] += A[158] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a47_0 = _mm_load_ss(&A[159]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*56)+11], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a47_1 = _mm_load_ss(&A[160]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*56)+26], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a47_2 = _mm_load_ss(&A[161]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*56)+47], c47_2);
#else
    C[(l_n*56)+11] += A[159] * B[(l_n*56)+47];
    C[(l_n*56)+26] += A[160] * B[(l_n*56)+47];
    C[(l_n*56)+47] += A[161] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a48_0 = _mm_load_ss(&A[162]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*56)+12], c48_0);
    __m128 c48_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a48_1 = _mm_load_ss(&A[163]);
    c48_1 = _mm_add_ss(c48_1, _mm_mul_ss(a48_1, b48));
    _mm_store_ss(&C[(l_n*56)+27], c48_1);
    __m128 c48_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a48_2 = _mm_load_ss(&A[164]);
    c48_2 = _mm_add_ss(c48_2, _mm_mul_ss(a48_2, b48));
    _mm_store_ss(&C[(l_n*56)+48], c48_2);
#else
    C[(l_n*56)+12] += A[162] * B[(l_n*56)+48];
    C[(l_n*56)+27] += A[163] * B[(l_n*56)+48];
    C[(l_n*56)+48] += A[164] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*56)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a49_0 = _mm_load_ss(&A[165]);
    c49_0 = _mm_add_ss(c49_0, _mm_mul_ss(a49_0, b49));
    _mm_store_ss(&C[(l_n*56)+13], c49_0);
    __m128 c49_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a49_1 = _mm_load_ss(&A[166]);
    c49_1 = _mm_add_ss(c49_1, _mm_mul_ss(a49_1, b49));
    _mm_store_ss(&C[(l_n*56)+28], c49_1);
    __m128 c49_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a49_2 = _mm_load_ss(&A[167]);
    c49_2 = _mm_add_ss(c49_2, _mm_mul_ss(a49_2, b49));
    _mm_store_ss(&C[(l_n*56)+49], c49_2);
#else
    C[(l_n*56)+13] += A[165] * B[(l_n*56)+49];
    C[(l_n*56)+28] += A[166] * B[(l_n*56)+49];
    C[(l_n*56)+49] += A[167] * B[(l_n*56)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a50_0 = _mm_load_ss(&A[168]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*56)+4], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a50_1 = _mm_load_ss(&A[169]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*56)+14], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a50_2 = _mm_load_ss(&A[170]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+29], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a50_3 = _mm_load_ss(&A[171]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*56)+50], c50_3);
#else
    C[(l_n*56)+4] += A[168] * B[(l_n*56)+50];
    C[(l_n*56)+14] += A[169] * B[(l_n*56)+50];
    C[(l_n*56)+29] += A[170] * B[(l_n*56)+50];
    C[(l_n*56)+50] += A[171] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a51_0 = _mm_load_ss(&A[172]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*56)+5], c51_0);
    __m128 c51_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a51_1 = _mm_load_ss(&A[173]);
    c51_1 = _mm_add_ss(c51_1, _mm_mul_ss(a51_1, b51));
    _mm_store_ss(&C[(l_n*56)+15], c51_1);
    __m128 c51_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a51_2 = _mm_load_ss(&A[174]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*56)+30], c51_2);
    __m128 c51_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a51_3 = _mm_load_ss(&A[175]);
    c51_3 = _mm_add_ss(c51_3, _mm_mul_ss(a51_3, b51));
    _mm_store_ss(&C[(l_n*56)+51], c51_3);
#else
    C[(l_n*56)+5] += A[172] * B[(l_n*56)+51];
    C[(l_n*56)+15] += A[173] * B[(l_n*56)+51];
    C[(l_n*56)+30] += A[174] * B[(l_n*56)+51];
    C[(l_n*56)+51] += A[175] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*56)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a52_0 = _mm_load_ss(&A[176]);
    c52_0 = _mm_add_ss(c52_0, _mm_mul_ss(a52_0, b52));
    _mm_store_ss(&C[(l_n*56)+6], c52_0);
    __m128 c52_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a52_1 = _mm_load_ss(&A[177]);
    c52_1 = _mm_add_ss(c52_1, _mm_mul_ss(a52_1, b52));
    _mm_store_ss(&C[(l_n*56)+16], c52_1);
    __m128 c52_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a52_2 = _mm_load_ss(&A[178]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*56)+31], c52_2);
    __m128 c52_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a52_3 = _mm_load_ss(&A[179]);
    c52_3 = _mm_add_ss(c52_3, _mm_mul_ss(a52_3, b52));
    _mm_store_ss(&C[(l_n*56)+52], c52_3);
#else
    C[(l_n*56)+6] += A[176] * B[(l_n*56)+52];
    C[(l_n*56)+16] += A[177] * B[(l_n*56)+52];
    C[(l_n*56)+31] += A[178] * B[(l_n*56)+52];
    C[(l_n*56)+52] += A[179] * B[(l_n*56)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a53_0 = _mm_load_ss(&A[180]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*56)+1], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a53_1 = _mm_load_ss(&A[181]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*56)+7], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a53_2 = _mm_load_ss(&A[182]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*56)+17], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a53_3 = _mm_load_ss(&A[183]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*56)+32], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a53_4 = _mm_load_ss(&A[184]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*56)+53], c53_4);
#else
    C[(l_n*56)+1] += A[180] * B[(l_n*56)+53];
    C[(l_n*56)+7] += A[181] * B[(l_n*56)+53];
    C[(l_n*56)+17] += A[182] * B[(l_n*56)+53];
    C[(l_n*56)+32] += A[183] * B[(l_n*56)+53];
    C[(l_n*56)+53] += A[184] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*56)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a54_0 = _mm_load_ss(&A[185]);
    c54_0 = _mm_add_ss(c54_0, _mm_mul_ss(a54_0, b54));
    _mm_store_ss(&C[(l_n*56)+2], c54_0);
    __m128 c54_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a54_1 = _mm_load_ss(&A[186]);
    c54_1 = _mm_add_ss(c54_1, _mm_mul_ss(a54_1, b54));
    _mm_store_ss(&C[(l_n*56)+8], c54_1);
    __m128 c54_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a54_2 = _mm_load_ss(&A[187]);
    c54_2 = _mm_add_ss(c54_2, _mm_mul_ss(a54_2, b54));
    _mm_store_ss(&C[(l_n*56)+18], c54_2);
    __m128 c54_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a54_3 = _mm_load_ss(&A[188]);
    c54_3 = _mm_add_ss(c54_3, _mm_mul_ss(a54_3, b54));
    _mm_store_ss(&C[(l_n*56)+33], c54_3);
    __m128 c54_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a54_4 = _mm_load_ss(&A[189]);
    c54_4 = _mm_add_ss(c54_4, _mm_mul_ss(a54_4, b54));
    _mm_store_ss(&C[(l_n*56)+54], c54_4);
#else
    C[(l_n*56)+2] += A[185] * B[(l_n*56)+54];
    C[(l_n*56)+8] += A[186] * B[(l_n*56)+54];
    C[(l_n*56)+18] += A[187] * B[(l_n*56)+54];
    C[(l_n*56)+33] += A[188] * B[(l_n*56)+54];
    C[(l_n*56)+54] += A[189] * B[(l_n*56)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*56)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a55_0 = _mm_load_ss(&A[190]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*56)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a55_1 = _mm_load_ss(&A[191]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*56)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a55_2 = _mm_load_ss(&A[192]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*56)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a55_3 = _mm_load_ss(&A[193]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*56)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a55_4 = _mm_load_ss(&A[194]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*56)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a55_5 = _mm_load_ss(&A[195]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*56)+55], c55_5);
#else
    C[(l_n*56)+0] += A[190] * B[(l_n*56)+55];
    C[(l_n*56)+3] += A[191] * B[(l_n*56)+55];
    C[(l_n*56)+9] += A[192] * B[(l_n*56)+55];
    C[(l_n*56)+19] += A[193] * B[(l_n*56)+55];
    C[(l_n*56)+34] += A[194] * B[(l_n*56)+55];
    C[(l_n*56)+55] += A[195] * B[(l_n*56)+55];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 3528;
#endif
}

void ssparse_fP113DivM_m56_n9_k56_ldAna6_ldB56_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*56)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*56)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*56)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*56)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*56)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*56)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*56)+55], c0_5);
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*56)+0];
    C[(l_n*56)+3] += A[1] * B[(l_n*56)+0];
    C[(l_n*56)+9] += A[2] * B[(l_n*56)+0];
    C[(l_n*56)+19] += A[3] * B[(l_n*56)+0];
    C[(l_n*56)+34] += A[4] * B[(l_n*56)+0];
    C[(l_n*56)+55] += A[5] * B[(l_n*56)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a1_0 = _mm_load_ss(&A[6]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*56)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a1_1 = _mm_load_ss(&A[7]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*56)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a1_2 = _mm_load_ss(&A[8]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*56)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a1_3 = _mm_load_ss(&A[9]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*56)+32], c1_3);
    __m128 c1_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a1_4 = _mm_load_ss(&A[10]);
    c1_4 = _mm_add_ss(c1_4, _mm_mul_ss(a1_4, b1));
    _mm_store_ss(&C[(l_n*56)+53], c1_4);
#else
    C[(l_n*56)+1] += A[6] * B[(l_n*56)+1];
    C[(l_n*56)+7] += A[7] * B[(l_n*56)+1];
    C[(l_n*56)+17] += A[8] * B[(l_n*56)+1];
    C[(l_n*56)+32] += A[9] * B[(l_n*56)+1];
    C[(l_n*56)+53] += A[10] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*56)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a2_0 = _mm_load_ss(&A[11]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*56)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a2_1 = _mm_load_ss(&A[12]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*56)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a2_2 = _mm_load_ss(&A[13]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*56)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a2_3 = _mm_load_ss(&A[14]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*56)+33], c2_3);
    __m128 c2_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a2_4 = _mm_load_ss(&A[15]);
    c2_4 = _mm_add_ss(c2_4, _mm_mul_ss(a2_4, b2));
    _mm_store_ss(&C[(l_n*56)+54], c2_4);
#else
    C[(l_n*56)+2] += A[11] * B[(l_n*56)+2];
    C[(l_n*56)+8] += A[12] * B[(l_n*56)+2];
    C[(l_n*56)+18] += A[13] * B[(l_n*56)+2];
    C[(l_n*56)+33] += A[14] * B[(l_n*56)+2];
    C[(l_n*56)+54] += A[15] * B[(l_n*56)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*56)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a3_0 = _mm_load_ss(&A[16]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*56)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a3_1 = _mm_load_ss(&A[17]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*56)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a3_2 = _mm_load_ss(&A[18]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*56)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a3_3 = _mm_load_ss(&A[19]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*56)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a3_4 = _mm_load_ss(&A[20]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*56)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a3_5 = _mm_load_ss(&A[21]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*56)+55], c3_5);
#else
    C[(l_n*56)+0] += A[16] * B[(l_n*56)+3];
    C[(l_n*56)+3] += A[17] * B[(l_n*56)+3];
    C[(l_n*56)+9] += A[18] * B[(l_n*56)+3];
    C[(l_n*56)+19] += A[19] * B[(l_n*56)+3];
    C[(l_n*56)+34] += A[20] * B[(l_n*56)+3];
    C[(l_n*56)+55] += A[21] * B[(l_n*56)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a4_0 = _mm_load_ss(&A[22]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*56)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a4_1 = _mm_load_ss(&A[23]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*56)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a4_2 = _mm_load_ss(&A[24]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*56)+29], c4_2);
    __m128 c4_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a4_3 = _mm_load_ss(&A[25]);
    c4_3 = _mm_add_ss(c4_3, _mm_mul_ss(a4_3, b4));
    _mm_store_ss(&C[(l_n*56)+50], c4_3);
#else
    C[(l_n*56)+4] += A[22] * B[(l_n*56)+4];
    C[(l_n*56)+14] += A[23] * B[(l_n*56)+4];
    C[(l_n*56)+29] += A[24] * B[(l_n*56)+4];
    C[(l_n*56)+50] += A[25] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a5_0 = _mm_load_ss(&A[26]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*56)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a5_1 = _mm_load_ss(&A[27]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*56)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a5_2 = _mm_load_ss(&A[28]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*56)+30], c5_2);
    __m128 c5_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a5_3 = _mm_load_ss(&A[29]);
    c5_3 = _mm_add_ss(c5_3, _mm_mul_ss(a5_3, b5));
    _mm_store_ss(&C[(l_n*56)+51], c5_3);
#else
    C[(l_n*56)+5] += A[26] * B[(l_n*56)+5];
    C[(l_n*56)+15] += A[27] * B[(l_n*56)+5];
    C[(l_n*56)+30] += A[28] * B[(l_n*56)+5];
    C[(l_n*56)+51] += A[29] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*56)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a6_0 = _mm_load_ss(&A[30]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*56)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a6_1 = _mm_load_ss(&A[31]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*56)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a6_2 = _mm_load_ss(&A[32]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*56)+31], c6_2);
    __m128 c6_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a6_3 = _mm_load_ss(&A[33]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*56)+52], c6_3);
#else
    C[(l_n*56)+6] += A[30] * B[(l_n*56)+6];
    C[(l_n*56)+16] += A[31] * B[(l_n*56)+6];
    C[(l_n*56)+31] += A[32] * B[(l_n*56)+6];
    C[(l_n*56)+52] += A[33] * B[(l_n*56)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a7_0 = _mm_load_ss(&A[34]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*56)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a7_1 = _mm_load_ss(&A[35]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*56)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a7_2 = _mm_load_ss(&A[36]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*56)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a7_3 = _mm_load_ss(&A[37]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*56)+32], c7_3);
    __m128 c7_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a7_4 = _mm_load_ss(&A[38]);
    c7_4 = _mm_add_ss(c7_4, _mm_mul_ss(a7_4, b7));
    _mm_store_ss(&C[(l_n*56)+53], c7_4);
#else
    C[(l_n*56)+1] += A[34] * B[(l_n*56)+7];
    C[(l_n*56)+7] += A[35] * B[(l_n*56)+7];
    C[(l_n*56)+17] += A[36] * B[(l_n*56)+7];
    C[(l_n*56)+32] += A[37] * B[(l_n*56)+7];
    C[(l_n*56)+53] += A[38] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*56)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a8_0 = _mm_load_ss(&A[39]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*56)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a8_1 = _mm_load_ss(&A[40]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*56)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a8_2 = _mm_load_ss(&A[41]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*56)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a8_3 = _mm_load_ss(&A[42]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*56)+33], c8_3);
    __m128 c8_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a8_4 = _mm_load_ss(&A[43]);
    c8_4 = _mm_add_ss(c8_4, _mm_mul_ss(a8_4, b8));
    _mm_store_ss(&C[(l_n*56)+54], c8_4);
#else
    C[(l_n*56)+2] += A[39] * B[(l_n*56)+8];
    C[(l_n*56)+8] += A[40] * B[(l_n*56)+8];
    C[(l_n*56)+18] += A[41] * B[(l_n*56)+8];
    C[(l_n*56)+33] += A[42] * B[(l_n*56)+8];
    C[(l_n*56)+54] += A[43] * B[(l_n*56)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*56)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a9_0 = _mm_load_ss(&A[44]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*56)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a9_1 = _mm_load_ss(&A[45]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*56)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a9_2 = _mm_load_ss(&A[46]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*56)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a9_3 = _mm_load_ss(&A[47]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*56)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a9_4 = _mm_load_ss(&A[48]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*56)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a9_5 = _mm_load_ss(&A[49]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*56)+55], c9_5);
#else
    C[(l_n*56)+0] += A[44] * B[(l_n*56)+9];
    C[(l_n*56)+3] += A[45] * B[(l_n*56)+9];
    C[(l_n*56)+9] += A[46] * B[(l_n*56)+9];
    C[(l_n*56)+19] += A[47] * B[(l_n*56)+9];
    C[(l_n*56)+34] += A[48] * B[(l_n*56)+9];
    C[(l_n*56)+55] += A[49] * B[(l_n*56)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a10_0 = _mm_load_ss(&A[50]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*56)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a10_1 = _mm_load_ss(&A[51]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*56)+25], c10_1);
    __m128 c10_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a10_2 = _mm_load_ss(&A[52]);
    c10_2 = _mm_add_ss(c10_2, _mm_mul_ss(a10_2, b10));
    _mm_store_ss(&C[(l_n*56)+46], c10_2);
#else
    C[(l_n*56)+10] += A[50] * B[(l_n*56)+10];
    C[(l_n*56)+25] += A[51] * B[(l_n*56)+10];
    C[(l_n*56)+46] += A[52] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a11_0 = _mm_load_ss(&A[53]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*56)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a11_1 = _mm_load_ss(&A[54]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*56)+26], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a11_2 = _mm_load_ss(&A[55]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*56)+47], c11_2);
#else
    C[(l_n*56)+11] += A[53] * B[(l_n*56)+11];
    C[(l_n*56)+26] += A[54] * B[(l_n*56)+11];
    C[(l_n*56)+47] += A[55] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a12_0 = _mm_load_ss(&A[56]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*56)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a12_1 = _mm_load_ss(&A[57]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*56)+27], c12_1);
    __m128 c12_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a12_2 = _mm_load_ss(&A[58]);
    c12_2 = _mm_add_ss(c12_2, _mm_mul_ss(a12_2, b12));
    _mm_store_ss(&C[(l_n*56)+48], c12_2);
#else
    C[(l_n*56)+12] += A[56] * B[(l_n*56)+12];
    C[(l_n*56)+27] += A[57] * B[(l_n*56)+12];
    C[(l_n*56)+48] += A[58] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*56)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a13_0 = _mm_load_ss(&A[59]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*56)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a13_1 = _mm_load_ss(&A[60]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*56)+28], c13_1);
    __m128 c13_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a13_2 = _mm_load_ss(&A[61]);
    c13_2 = _mm_add_ss(c13_2, _mm_mul_ss(a13_2, b13));
    _mm_store_ss(&C[(l_n*56)+49], c13_2);
#else
    C[(l_n*56)+13] += A[59] * B[(l_n*56)+13];
    C[(l_n*56)+28] += A[60] * B[(l_n*56)+13];
    C[(l_n*56)+49] += A[61] * B[(l_n*56)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a14_0 = _mm_load_ss(&A[62]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*56)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a14_1 = _mm_load_ss(&A[63]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*56)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a14_2 = _mm_load_ss(&A[64]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*56)+29], c14_2);
    __m128 c14_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a14_3 = _mm_load_ss(&A[65]);
    c14_3 = _mm_add_ss(c14_3, _mm_mul_ss(a14_3, b14));
    _mm_store_ss(&C[(l_n*56)+50], c14_3);
#else
    C[(l_n*56)+4] += A[62] * B[(l_n*56)+14];
    C[(l_n*56)+14] += A[63] * B[(l_n*56)+14];
    C[(l_n*56)+29] += A[64] * B[(l_n*56)+14];
    C[(l_n*56)+50] += A[65] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a15_0 = _mm_load_ss(&A[66]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*56)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a15_1 = _mm_load_ss(&A[67]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*56)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a15_2 = _mm_load_ss(&A[68]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*56)+30], c15_2);
    __m128 c15_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a15_3 = _mm_load_ss(&A[69]);
    c15_3 = _mm_add_ss(c15_3, _mm_mul_ss(a15_3, b15));
    _mm_store_ss(&C[(l_n*56)+51], c15_3);
#else
    C[(l_n*56)+5] += A[66] * B[(l_n*56)+15];
    C[(l_n*56)+15] += A[67] * B[(l_n*56)+15];
    C[(l_n*56)+30] += A[68] * B[(l_n*56)+15];
    C[(l_n*56)+51] += A[69] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*56)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a16_0 = _mm_load_ss(&A[70]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*56)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a16_1 = _mm_load_ss(&A[71]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*56)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a16_2 = _mm_load_ss(&A[72]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*56)+31], c16_2);
    __m128 c16_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a16_3 = _mm_load_ss(&A[73]);
    c16_3 = _mm_add_ss(c16_3, _mm_mul_ss(a16_3, b16));
    _mm_store_ss(&C[(l_n*56)+52], c16_3);
#else
    C[(l_n*56)+6] += A[70] * B[(l_n*56)+16];
    C[(l_n*56)+16] += A[71] * B[(l_n*56)+16];
    C[(l_n*56)+31] += A[72] * B[(l_n*56)+16];
    C[(l_n*56)+52] += A[73] * B[(l_n*56)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a17_0 = _mm_load_ss(&A[74]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*56)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a17_1 = _mm_load_ss(&A[75]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*56)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a17_2 = _mm_load_ss(&A[76]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*56)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a17_3 = _mm_load_ss(&A[77]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*56)+32], c17_3);
    __m128 c17_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a17_4 = _mm_load_ss(&A[78]);
    c17_4 = _mm_add_ss(c17_4, _mm_mul_ss(a17_4, b17));
    _mm_store_ss(&C[(l_n*56)+53], c17_4);
#else
    C[(l_n*56)+1] += A[74] * B[(l_n*56)+17];
    C[(l_n*56)+7] += A[75] * B[(l_n*56)+17];
    C[(l_n*56)+17] += A[76] * B[(l_n*56)+17];
    C[(l_n*56)+32] += A[77] * B[(l_n*56)+17];
    C[(l_n*56)+53] += A[78] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*56)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a18_0 = _mm_load_ss(&A[79]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*56)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a18_1 = _mm_load_ss(&A[80]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*56)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a18_2 = _mm_load_ss(&A[81]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*56)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a18_3 = _mm_load_ss(&A[82]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*56)+33], c18_3);
    __m128 c18_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a18_4 = _mm_load_ss(&A[83]);
    c18_4 = _mm_add_ss(c18_4, _mm_mul_ss(a18_4, b18));
    _mm_store_ss(&C[(l_n*56)+54], c18_4);
#else
    C[(l_n*56)+2] += A[79] * B[(l_n*56)+18];
    C[(l_n*56)+8] += A[80] * B[(l_n*56)+18];
    C[(l_n*56)+18] += A[81] * B[(l_n*56)+18];
    C[(l_n*56)+33] += A[82] * B[(l_n*56)+18];
    C[(l_n*56)+54] += A[83] * B[(l_n*56)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*56)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a19_0 = _mm_load_ss(&A[84]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*56)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a19_1 = _mm_load_ss(&A[85]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*56)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a19_2 = _mm_load_ss(&A[86]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*56)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a19_3 = _mm_load_ss(&A[87]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*56)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a19_4 = _mm_load_ss(&A[88]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*56)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a19_5 = _mm_load_ss(&A[89]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*56)+55], c19_5);
#else
    C[(l_n*56)+0] += A[84] * B[(l_n*56)+19];
    C[(l_n*56)+3] += A[85] * B[(l_n*56)+19];
    C[(l_n*56)+9] += A[86] * B[(l_n*56)+19];
    C[(l_n*56)+19] += A[87] * B[(l_n*56)+19];
    C[(l_n*56)+34] += A[88] * B[(l_n*56)+19];
    C[(l_n*56)+55] += A[89] * B[(l_n*56)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a20_0 = _mm_load_ss(&A[90]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*56)+20], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a20_1 = _mm_load_ss(&A[91]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*56)+41], c20_1);
#else
    C[(l_n*56)+20] += A[90] * B[(l_n*56)+20];
    C[(l_n*56)+41] += A[91] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a21_0 = _mm_load_ss(&A[92]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*56)+21], c21_0);
    __m128 c21_1 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a21_1 = _mm_load_ss(&A[93]);
    c21_1 = _mm_add_ss(c21_1, _mm_mul_ss(a21_1, b21));
    _mm_store_ss(&C[(l_n*56)+42], c21_1);
#else
    C[(l_n*56)+21] += A[92] * B[(l_n*56)+21];
    C[(l_n*56)+42] += A[93] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a22_0 = _mm_load_ss(&A[94]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*56)+22], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a22_1 = _mm_load_ss(&A[95]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*56)+43], c22_1);
#else
    C[(l_n*56)+22] += A[94] * B[(l_n*56)+22];
    C[(l_n*56)+43] += A[95] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a23_0 = _mm_load_ss(&A[96]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+23], c23_0);
    __m128 c23_1 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a23_1 = _mm_load_ss(&A[97]);
    c23_1 = _mm_add_ss(c23_1, _mm_mul_ss(a23_1, b23));
    _mm_store_ss(&C[(l_n*56)+44], c23_1);
#else
    C[(l_n*56)+23] += A[96] * B[(l_n*56)+23];
    C[(l_n*56)+44] += A[97] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*56)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a24_0 = _mm_load_ss(&A[98]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*56)+24], c24_0);
    __m128 c24_1 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a24_1 = _mm_load_ss(&A[99]);
    c24_1 = _mm_add_ss(c24_1, _mm_mul_ss(a24_1, b24));
    _mm_store_ss(&C[(l_n*56)+45], c24_1);
#else
    C[(l_n*56)+24] += A[98] * B[(l_n*56)+24];
    C[(l_n*56)+45] += A[99] * B[(l_n*56)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a25_0 = _mm_load_ss(&A[100]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*56)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a25_1 = _mm_load_ss(&A[101]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*56)+25], c25_1);
    __m128 c25_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a25_2 = _mm_load_ss(&A[102]);
    c25_2 = _mm_add_ss(c25_2, _mm_mul_ss(a25_2, b25));
    _mm_store_ss(&C[(l_n*56)+46], c25_2);
#else
    C[(l_n*56)+10] += A[100] * B[(l_n*56)+25];
    C[(l_n*56)+25] += A[101] * B[(l_n*56)+25];
    C[(l_n*56)+46] += A[102] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a26_0 = _mm_load_ss(&A[103]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*56)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a26_1 = _mm_load_ss(&A[104]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*56)+26], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a26_2 = _mm_load_ss(&A[105]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*56)+47], c26_2);
#else
    C[(l_n*56)+11] += A[103] * B[(l_n*56)+26];
    C[(l_n*56)+26] += A[104] * B[(l_n*56)+26];
    C[(l_n*56)+47] += A[105] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a27_0 = _mm_load_ss(&A[106]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*56)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a27_1 = _mm_load_ss(&A[107]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*56)+27], c27_1);
    __m128 c27_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a27_2 = _mm_load_ss(&A[108]);
    c27_2 = _mm_add_ss(c27_2, _mm_mul_ss(a27_2, b27));
    _mm_store_ss(&C[(l_n*56)+48], c27_2);
#else
    C[(l_n*56)+12] += A[106] * B[(l_n*56)+27];
    C[(l_n*56)+27] += A[107] * B[(l_n*56)+27];
    C[(l_n*56)+48] += A[108] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*56)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a28_0 = _mm_load_ss(&A[109]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*56)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a28_1 = _mm_load_ss(&A[110]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*56)+28], c28_1);
    __m128 c28_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a28_2 = _mm_load_ss(&A[111]);
    c28_2 = _mm_add_ss(c28_2, _mm_mul_ss(a28_2, b28));
    _mm_store_ss(&C[(l_n*56)+49], c28_2);
#else
    C[(l_n*56)+13] += A[109] * B[(l_n*56)+28];
    C[(l_n*56)+28] += A[110] * B[(l_n*56)+28];
    C[(l_n*56)+49] += A[111] * B[(l_n*56)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a29_0 = _mm_load_ss(&A[112]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*56)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a29_1 = _mm_load_ss(&A[113]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*56)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a29_2 = _mm_load_ss(&A[114]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+29], c29_2);
    __m128 c29_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a29_3 = _mm_load_ss(&A[115]);
    c29_3 = _mm_add_ss(c29_3, _mm_mul_ss(a29_3, b29));
    _mm_store_ss(&C[(l_n*56)+50], c29_3);
#else
    C[(l_n*56)+4] += A[112] * B[(l_n*56)+29];
    C[(l_n*56)+14] += A[113] * B[(l_n*56)+29];
    C[(l_n*56)+29] += A[114] * B[(l_n*56)+29];
    C[(l_n*56)+50] += A[115] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a30_0 = _mm_load_ss(&A[116]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*56)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a30_1 = _mm_load_ss(&A[117]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*56)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a30_2 = _mm_load_ss(&A[118]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*56)+30], c30_2);
    __m128 c30_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a30_3 = _mm_load_ss(&A[119]);
    c30_3 = _mm_add_ss(c30_3, _mm_mul_ss(a30_3, b30));
    _mm_store_ss(&C[(l_n*56)+51], c30_3);
#else
    C[(l_n*56)+5] += A[116] * B[(l_n*56)+30];
    C[(l_n*56)+15] += A[117] * B[(l_n*56)+30];
    C[(l_n*56)+30] += A[118] * B[(l_n*56)+30];
    C[(l_n*56)+51] += A[119] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*56)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a31_0 = _mm_load_ss(&A[120]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*56)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a31_1 = _mm_load_ss(&A[121]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*56)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a31_2 = _mm_load_ss(&A[122]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*56)+31], c31_2);
    __m128 c31_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a31_3 = _mm_load_ss(&A[123]);
    c31_3 = _mm_add_ss(c31_3, _mm_mul_ss(a31_3, b31));
    _mm_store_ss(&C[(l_n*56)+52], c31_3);
#else
    C[(l_n*56)+6] += A[120] * B[(l_n*56)+31];
    C[(l_n*56)+16] += A[121] * B[(l_n*56)+31];
    C[(l_n*56)+31] += A[122] * B[(l_n*56)+31];
    C[(l_n*56)+52] += A[123] * B[(l_n*56)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a32_0 = _mm_load_ss(&A[124]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*56)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a32_1 = _mm_load_ss(&A[125]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*56)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a32_2 = _mm_load_ss(&A[126]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*56)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a32_3 = _mm_load_ss(&A[127]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*56)+32], c32_3);
    __m128 c32_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a32_4 = _mm_load_ss(&A[128]);
    c32_4 = _mm_add_ss(c32_4, _mm_mul_ss(a32_4, b32));
    _mm_store_ss(&C[(l_n*56)+53], c32_4);
#else
    C[(l_n*56)+1] += A[124] * B[(l_n*56)+32];
    C[(l_n*56)+7] += A[125] * B[(l_n*56)+32];
    C[(l_n*56)+17] += A[126] * B[(l_n*56)+32];
    C[(l_n*56)+32] += A[127] * B[(l_n*56)+32];
    C[(l_n*56)+53] += A[128] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*56)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a33_0 = _mm_load_ss(&A[129]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*56)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a33_1 = _mm_load_ss(&A[130]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*56)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a33_2 = _mm_load_ss(&A[131]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*56)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a33_3 = _mm_load_ss(&A[132]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*56)+33], c33_3);
    __m128 c33_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a33_4 = _mm_load_ss(&A[133]);
    c33_4 = _mm_add_ss(c33_4, _mm_mul_ss(a33_4, b33));
    _mm_store_ss(&C[(l_n*56)+54], c33_4);
#else
    C[(l_n*56)+2] += A[129] * B[(l_n*56)+33];
    C[(l_n*56)+8] += A[130] * B[(l_n*56)+33];
    C[(l_n*56)+18] += A[131] * B[(l_n*56)+33];
    C[(l_n*56)+33] += A[132] * B[(l_n*56)+33];
    C[(l_n*56)+54] += A[133] * B[(l_n*56)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*56)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a34_0 = _mm_load_ss(&A[134]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*56)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a34_1 = _mm_load_ss(&A[135]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*56)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a34_2 = _mm_load_ss(&A[136]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*56)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a34_3 = _mm_load_ss(&A[137]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*56)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a34_4 = _mm_load_ss(&A[138]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*56)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a34_5 = _mm_load_ss(&A[139]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*56)+55], c34_5);
#else
    C[(l_n*56)+0] += A[134] * B[(l_n*56)+34];
    C[(l_n*56)+3] += A[135] * B[(l_n*56)+34];
    C[(l_n*56)+9] += A[136] * B[(l_n*56)+34];
    C[(l_n*56)+19] += A[137] * B[(l_n*56)+34];
    C[(l_n*56)+34] += A[138] * B[(l_n*56)+34];
    C[(l_n*56)+55] += A[139] * B[(l_n*56)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*56)+35]);
    __m128 a35_0 = _mm_load_ss(&A[140]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*56)+35], c35_0);
#else
    C[(l_n*56)+35] += A[140] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*56)+36]);
    __m128 a36_0 = _mm_load_ss(&A[141]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*56)+36], c36_0);
#else
    C[(l_n*56)+36] += A[141] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a37_0 = _mm_load_ss(&A[142]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*56)+37], c37_0);
#else
    C[(l_n*56)+37] += A[142] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a38_0 = _mm_load_ss(&A[143]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*56)+38], c38_0);
#else
    C[(l_n*56)+38] += A[143] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a39_0 = _mm_load_ss(&A[144]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*56)+39], c39_0);
#else
    C[(l_n*56)+39] += A[144] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*56)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a40_0 = _mm_load_ss(&A[145]);
    c40_0 = _mm_add_ss(c40_0, _mm_mul_ss(a40_0, b40));
    _mm_store_ss(&C[(l_n*56)+40], c40_0);
#else
    C[(l_n*56)+40] += A[145] * B[(l_n*56)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a41_0 = _mm_load_ss(&A[146]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*56)+20], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a41_1 = _mm_load_ss(&A[147]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*56)+41], c41_1);
#else
    C[(l_n*56)+20] += A[146] * B[(l_n*56)+41];
    C[(l_n*56)+41] += A[147] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a42_0 = _mm_load_ss(&A[148]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*56)+21], c42_0);
    __m128 c42_1 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a42_1 = _mm_load_ss(&A[149]);
    c42_1 = _mm_add_ss(c42_1, _mm_mul_ss(a42_1, b42));
    _mm_store_ss(&C[(l_n*56)+42], c42_1);
#else
    C[(l_n*56)+21] += A[148] * B[(l_n*56)+42];
    C[(l_n*56)+42] += A[149] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a43_0 = _mm_load_ss(&A[150]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*56)+22], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a43_1 = _mm_load_ss(&A[151]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*56)+43], c43_1);
#else
    C[(l_n*56)+22] += A[150] * B[(l_n*56)+43];
    C[(l_n*56)+43] += A[151] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a44_0 = _mm_load_ss(&A[152]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+23], c44_0);
    __m128 c44_1 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a44_1 = _mm_load_ss(&A[153]);
    c44_1 = _mm_add_ss(c44_1, _mm_mul_ss(a44_1, b44));
    _mm_store_ss(&C[(l_n*56)+44], c44_1);
#else
    C[(l_n*56)+23] += A[152] * B[(l_n*56)+44];
    C[(l_n*56)+44] += A[153] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*56)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a45_0 = _mm_load_ss(&A[154]);
    c45_0 = _mm_add_ss(c45_0, _mm_mul_ss(a45_0, b45));
    _mm_store_ss(&C[(l_n*56)+24], c45_0);
    __m128 c45_1 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a45_1 = _mm_load_ss(&A[155]);
    c45_1 = _mm_add_ss(c45_1, _mm_mul_ss(a45_1, b45));
    _mm_store_ss(&C[(l_n*56)+45], c45_1);
#else
    C[(l_n*56)+24] += A[154] * B[(l_n*56)+45];
    C[(l_n*56)+45] += A[155] * B[(l_n*56)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a46_0 = _mm_load_ss(&A[156]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*56)+10], c46_0);
    __m128 c46_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a46_1 = _mm_load_ss(&A[157]);
    c46_1 = _mm_add_ss(c46_1, _mm_mul_ss(a46_1, b46));
    _mm_store_ss(&C[(l_n*56)+25], c46_1);
    __m128 c46_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a46_2 = _mm_load_ss(&A[158]);
    c46_2 = _mm_add_ss(c46_2, _mm_mul_ss(a46_2, b46));
    _mm_store_ss(&C[(l_n*56)+46], c46_2);
#else
    C[(l_n*56)+10] += A[156] * B[(l_n*56)+46];
    C[(l_n*56)+25] += A[157] * B[(l_n*56)+46];
    C[(l_n*56)+46] += A[158] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a47_0 = _mm_load_ss(&A[159]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*56)+11], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a47_1 = _mm_load_ss(&A[160]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*56)+26], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a47_2 = _mm_load_ss(&A[161]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*56)+47], c47_2);
#else
    C[(l_n*56)+11] += A[159] * B[(l_n*56)+47];
    C[(l_n*56)+26] += A[160] * B[(l_n*56)+47];
    C[(l_n*56)+47] += A[161] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a48_0 = _mm_load_ss(&A[162]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*56)+12], c48_0);
    __m128 c48_1 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a48_1 = _mm_load_ss(&A[163]);
    c48_1 = _mm_add_ss(c48_1, _mm_mul_ss(a48_1, b48));
    _mm_store_ss(&C[(l_n*56)+27], c48_1);
    __m128 c48_2 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a48_2 = _mm_load_ss(&A[164]);
    c48_2 = _mm_add_ss(c48_2, _mm_mul_ss(a48_2, b48));
    _mm_store_ss(&C[(l_n*56)+48], c48_2);
#else
    C[(l_n*56)+12] += A[162] * B[(l_n*56)+48];
    C[(l_n*56)+27] += A[163] * B[(l_n*56)+48];
    C[(l_n*56)+48] += A[164] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*56)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a49_0 = _mm_load_ss(&A[165]);
    c49_0 = _mm_add_ss(c49_0, _mm_mul_ss(a49_0, b49));
    _mm_store_ss(&C[(l_n*56)+13], c49_0);
    __m128 c49_1 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a49_1 = _mm_load_ss(&A[166]);
    c49_1 = _mm_add_ss(c49_1, _mm_mul_ss(a49_1, b49));
    _mm_store_ss(&C[(l_n*56)+28], c49_1);
    __m128 c49_2 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a49_2 = _mm_load_ss(&A[167]);
    c49_2 = _mm_add_ss(c49_2, _mm_mul_ss(a49_2, b49));
    _mm_store_ss(&C[(l_n*56)+49], c49_2);
#else
    C[(l_n*56)+13] += A[165] * B[(l_n*56)+49];
    C[(l_n*56)+28] += A[166] * B[(l_n*56)+49];
    C[(l_n*56)+49] += A[167] * B[(l_n*56)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a50_0 = _mm_load_ss(&A[168]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*56)+4], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a50_1 = _mm_load_ss(&A[169]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*56)+14], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a50_2 = _mm_load_ss(&A[170]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+29], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a50_3 = _mm_load_ss(&A[171]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*56)+50], c50_3);
#else
    C[(l_n*56)+4] += A[168] * B[(l_n*56)+50];
    C[(l_n*56)+14] += A[169] * B[(l_n*56)+50];
    C[(l_n*56)+29] += A[170] * B[(l_n*56)+50];
    C[(l_n*56)+50] += A[171] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a51_0 = _mm_load_ss(&A[172]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*56)+5], c51_0);
    __m128 c51_1 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a51_1 = _mm_load_ss(&A[173]);
    c51_1 = _mm_add_ss(c51_1, _mm_mul_ss(a51_1, b51));
    _mm_store_ss(&C[(l_n*56)+15], c51_1);
    __m128 c51_2 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a51_2 = _mm_load_ss(&A[174]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*56)+30], c51_2);
    __m128 c51_3 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a51_3 = _mm_load_ss(&A[175]);
    c51_3 = _mm_add_ss(c51_3, _mm_mul_ss(a51_3, b51));
    _mm_store_ss(&C[(l_n*56)+51], c51_3);
#else
    C[(l_n*56)+5] += A[172] * B[(l_n*56)+51];
    C[(l_n*56)+15] += A[173] * B[(l_n*56)+51];
    C[(l_n*56)+30] += A[174] * B[(l_n*56)+51];
    C[(l_n*56)+51] += A[175] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*56)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a52_0 = _mm_load_ss(&A[176]);
    c52_0 = _mm_add_ss(c52_0, _mm_mul_ss(a52_0, b52));
    _mm_store_ss(&C[(l_n*56)+6], c52_0);
    __m128 c52_1 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a52_1 = _mm_load_ss(&A[177]);
    c52_1 = _mm_add_ss(c52_1, _mm_mul_ss(a52_1, b52));
    _mm_store_ss(&C[(l_n*56)+16], c52_1);
    __m128 c52_2 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a52_2 = _mm_load_ss(&A[178]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*56)+31], c52_2);
    __m128 c52_3 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a52_3 = _mm_load_ss(&A[179]);
    c52_3 = _mm_add_ss(c52_3, _mm_mul_ss(a52_3, b52));
    _mm_store_ss(&C[(l_n*56)+52], c52_3);
#else
    C[(l_n*56)+6] += A[176] * B[(l_n*56)+52];
    C[(l_n*56)+16] += A[177] * B[(l_n*56)+52];
    C[(l_n*56)+31] += A[178] * B[(l_n*56)+52];
    C[(l_n*56)+52] += A[179] * B[(l_n*56)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a53_0 = _mm_load_ss(&A[180]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*56)+1], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a53_1 = _mm_load_ss(&A[181]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*56)+7], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a53_2 = _mm_load_ss(&A[182]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*56)+17], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a53_3 = _mm_load_ss(&A[183]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*56)+32], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a53_4 = _mm_load_ss(&A[184]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*56)+53], c53_4);
#else
    C[(l_n*56)+1] += A[180] * B[(l_n*56)+53];
    C[(l_n*56)+7] += A[181] * B[(l_n*56)+53];
    C[(l_n*56)+17] += A[182] * B[(l_n*56)+53];
    C[(l_n*56)+32] += A[183] * B[(l_n*56)+53];
    C[(l_n*56)+53] += A[184] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*56)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a54_0 = _mm_load_ss(&A[185]);
    c54_0 = _mm_add_ss(c54_0, _mm_mul_ss(a54_0, b54));
    _mm_store_ss(&C[(l_n*56)+2], c54_0);
    __m128 c54_1 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a54_1 = _mm_load_ss(&A[186]);
    c54_1 = _mm_add_ss(c54_1, _mm_mul_ss(a54_1, b54));
    _mm_store_ss(&C[(l_n*56)+8], c54_1);
    __m128 c54_2 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a54_2 = _mm_load_ss(&A[187]);
    c54_2 = _mm_add_ss(c54_2, _mm_mul_ss(a54_2, b54));
    _mm_store_ss(&C[(l_n*56)+18], c54_2);
    __m128 c54_3 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a54_3 = _mm_load_ss(&A[188]);
    c54_3 = _mm_add_ss(c54_3, _mm_mul_ss(a54_3, b54));
    _mm_store_ss(&C[(l_n*56)+33], c54_3);
    __m128 c54_4 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a54_4 = _mm_load_ss(&A[189]);
    c54_4 = _mm_add_ss(c54_4, _mm_mul_ss(a54_4, b54));
    _mm_store_ss(&C[(l_n*56)+54], c54_4);
#else
    C[(l_n*56)+2] += A[185] * B[(l_n*56)+54];
    C[(l_n*56)+8] += A[186] * B[(l_n*56)+54];
    C[(l_n*56)+18] += A[187] * B[(l_n*56)+54];
    C[(l_n*56)+33] += A[188] * B[(l_n*56)+54];
    C[(l_n*56)+54] += A[189] * B[(l_n*56)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*56)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a55_0 = _mm_load_ss(&A[190]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*56)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a55_1 = _mm_load_ss(&A[191]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*56)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a55_2 = _mm_load_ss(&A[192]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*56)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a55_3 = _mm_load_ss(&A[193]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*56)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a55_4 = _mm_load_ss(&A[194]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*56)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a55_5 = _mm_load_ss(&A[195]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*56)+55], c55_5);
#else
    C[(l_n*56)+0] += A[190] * B[(l_n*56)+55];
    C[(l_n*56)+3] += A[191] * B[(l_n*56)+55];
    C[(l_n*56)+9] += A[192] * B[(l_n*56)+55];
    C[(l_n*56)+19] += A[193] * B[(l_n*56)+55];
    C[(l_n*56)+34] += A[194] * B[(l_n*56)+55];
    C[(l_n*56)+55] += A[195] * B[(l_n*56)+55];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 3528;
#endif
}

void ssparse_fP111DivM_m56_n9_k56_ldAna6_ldB56_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*56)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*56)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*56)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*56)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*56)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*56)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*56)+55], c0_5);
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*56)+0];
    C[(l_n*56)+3] += A[1] * B[(l_n*56)+0];
    C[(l_n*56)+9] += A[2] * B[(l_n*56)+0];
    C[(l_n*56)+19] += A[3] * B[(l_n*56)+0];
    C[(l_n*56)+34] += A[4] * B[(l_n*56)+0];
    C[(l_n*56)+55] += A[5] * B[(l_n*56)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[6]));
    c1_0 = _mm_add_ps(c1_0, _mm_mul_ps(a1_0, b1));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c1_0));
    __m128 c1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c1_2 = _mm_add_ps(c1_2, _mm_mul_ps(a1_2, b1));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c1_2));
    __m128 c1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[10]));
    c1_4 = _mm_add_ps(c1_4, _mm_mul_ps(a1_4, b1));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c1_4));
    __m128 c1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c1_6 = _mm_add_ps(c1_6, _mm_mul_ps(a1_6, b1));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c1_6));
    __m128 c1_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a1_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[14]));
    c1_8 = _mm_add_ps(c1_8, _mm_mul_ps(a1_8, b1));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c1_8));
#else
    C[(l_n*56)+1] += A[6] * B[(l_n*56)+1];
    C[(l_n*56)+2] += A[7] * B[(l_n*56)+1];
    C[(l_n*56)+7] += A[8] * B[(l_n*56)+1];
    C[(l_n*56)+8] += A[9] * B[(l_n*56)+1];
    C[(l_n*56)+17] += A[10] * B[(l_n*56)+1];
    C[(l_n*56)+18] += A[11] * B[(l_n*56)+1];
    C[(l_n*56)+32] += A[12] * B[(l_n*56)+1];
    C[(l_n*56)+33] += A[13] * B[(l_n*56)+1];
    C[(l_n*56)+53] += A[14] * B[(l_n*56)+1];
    C[(l_n*56)+54] += A[15] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*56)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[16]));
    c2_0 = _mm_add_ps(c2_0, _mm_mul_ps(a2_0, b2));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c2_0));
    __m128 c2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c2_2 = _mm_add_ps(c2_2, _mm_mul_ps(a2_2, b2));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c2_2));
    __m128 c2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[20]));
    c2_4 = _mm_add_ps(c2_4, _mm_mul_ps(a2_4, b2));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c2_4));
    __m128 c2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[22]));
    c2_6 = _mm_add_ps(c2_6, _mm_mul_ps(a2_6, b2));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c2_6));
    __m128 c2_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a2_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[24]));
    c2_8 = _mm_add_ps(c2_8, _mm_mul_ps(a2_8, b2));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c2_8));
#else
    C[(l_n*56)+1] += A[16] * B[(l_n*56)+2];
    C[(l_n*56)+2] += A[17] * B[(l_n*56)+2];
    C[(l_n*56)+7] += A[18] * B[(l_n*56)+2];
    C[(l_n*56)+8] += A[19] * B[(l_n*56)+2];
    C[(l_n*56)+17] += A[20] * B[(l_n*56)+2];
    C[(l_n*56)+18] += A[21] * B[(l_n*56)+2];
    C[(l_n*56)+32] += A[22] * B[(l_n*56)+2];
    C[(l_n*56)+33] += A[23] * B[(l_n*56)+2];
    C[(l_n*56)+53] += A[24] * B[(l_n*56)+2];
    C[(l_n*56)+54] += A[25] * B[(l_n*56)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*56)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a3_0 = _mm_load_ss(&A[26]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*56)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a3_1 = _mm_load_ss(&A[27]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*56)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a3_2 = _mm_load_ss(&A[28]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*56)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a3_3 = _mm_load_ss(&A[29]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*56)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a3_4 = _mm_load_ss(&A[30]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*56)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a3_5 = _mm_load_ss(&A[31]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*56)+55], c3_5);
#else
    C[(l_n*56)+0] += A[26] * B[(l_n*56)+3];
    C[(l_n*56)+3] += A[27] * B[(l_n*56)+3];
    C[(l_n*56)+9] += A[28] * B[(l_n*56)+3];
    C[(l_n*56)+19] += A[29] * B[(l_n*56)+3];
    C[(l_n*56)+34] += A[30] * B[(l_n*56)+3];
    C[(l_n*56)+55] += A[31] * B[(l_n*56)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[32]));
    c4_0 = _mm_add_ps(c4_0, _mm_mul_ps(a4_0, b4));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c4_0));
    __m128 c4_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a4_2 = _mm_load_ss(&A[34]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*56)+6], c4_2);
    __m128 c4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[35]));
    c4_3 = _mm_add_ps(c4_3, _mm_mul_ps(a4_3, b4));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c4_3));
    __m128 c4_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a4_5 = _mm_load_ss(&A[37]);
    c4_5 = _mm_add_ss(c4_5, _mm_mul_ss(a4_5, b4));
    _mm_store_ss(&C[(l_n*56)+16], c4_5);
    __m128 c4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[38]));
    c4_6 = _mm_add_ps(c4_6, _mm_mul_ps(a4_6, b4));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c4_6));
    __m128 c4_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a4_8 = _mm_load_ss(&A[40]);
    c4_8 = _mm_add_ss(c4_8, _mm_mul_ss(a4_8, b4));
    _mm_store_ss(&C[(l_n*56)+31], c4_8);
    __m128 c4_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a4_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c4_9 = _mm_add_ps(c4_9, _mm_mul_ps(a4_9, b4));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c4_9));
    __m128 c4_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a4_11 = _mm_load_ss(&A[43]);
    c4_11 = _mm_add_ss(c4_11, _mm_mul_ss(a4_11, b4));
    _mm_store_ss(&C[(l_n*56)+52], c4_11);
#else
    C[(l_n*56)+4] += A[32] * B[(l_n*56)+4];
    C[(l_n*56)+5] += A[33] * B[(l_n*56)+4];
    C[(l_n*56)+6] += A[34] * B[(l_n*56)+4];
    C[(l_n*56)+14] += A[35] * B[(l_n*56)+4];
    C[(l_n*56)+15] += A[36] * B[(l_n*56)+4];
    C[(l_n*56)+16] += A[37] * B[(l_n*56)+4];
    C[(l_n*56)+29] += A[38] * B[(l_n*56)+4];
    C[(l_n*56)+30] += A[39] * B[(l_n*56)+4];
    C[(l_n*56)+31] += A[40] * B[(l_n*56)+4];
    C[(l_n*56)+50] += A[41] * B[(l_n*56)+4];
    C[(l_n*56)+51] += A[42] * B[(l_n*56)+4];
    C[(l_n*56)+52] += A[43] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[44]));
    c5_0 = _mm_add_ps(c5_0, _mm_mul_ps(a5_0, b5));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c5_0));
    __m128 c5_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a5_2 = _mm_load_ss(&A[46]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*56)+6], c5_2);
    __m128 c5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[47]));
    c5_3 = _mm_add_ps(c5_3, _mm_mul_ps(a5_3, b5));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c5_3));
    __m128 c5_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a5_5 = _mm_load_ss(&A[49]);
    c5_5 = _mm_add_ss(c5_5, _mm_mul_ss(a5_5, b5));
    _mm_store_ss(&C[(l_n*56)+16], c5_5);
    __m128 c5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[50]));
    c5_6 = _mm_add_ps(c5_6, _mm_mul_ps(a5_6, b5));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c5_6));
    __m128 c5_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a5_8 = _mm_load_ss(&A[52]);
    c5_8 = _mm_add_ss(c5_8, _mm_mul_ss(a5_8, b5));
    _mm_store_ss(&C[(l_n*56)+31], c5_8);
    __m128 c5_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a5_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[53]));
    c5_9 = _mm_add_ps(c5_9, _mm_mul_ps(a5_9, b5));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c5_9));
    __m128 c5_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a5_11 = _mm_load_ss(&A[55]);
    c5_11 = _mm_add_ss(c5_11, _mm_mul_ss(a5_11, b5));
    _mm_store_ss(&C[(l_n*56)+52], c5_11);
#else
    C[(l_n*56)+4] += A[44] * B[(l_n*56)+5];
    C[(l_n*56)+5] += A[45] * B[(l_n*56)+5];
    C[(l_n*56)+6] += A[46] * B[(l_n*56)+5];
    C[(l_n*56)+14] += A[47] * B[(l_n*56)+5];
    C[(l_n*56)+15] += A[48] * B[(l_n*56)+5];
    C[(l_n*56)+16] += A[49] * B[(l_n*56)+5];
    C[(l_n*56)+29] += A[50] * B[(l_n*56)+5];
    C[(l_n*56)+30] += A[51] * B[(l_n*56)+5];
    C[(l_n*56)+31] += A[52] * B[(l_n*56)+5];
    C[(l_n*56)+50] += A[53] * B[(l_n*56)+5];
    C[(l_n*56)+51] += A[54] * B[(l_n*56)+5];
    C[(l_n*56)+52] += A[55] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*56)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[56]));
    c6_0 = _mm_add_ps(c6_0, _mm_mul_ps(a6_0, b6));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c6_0));
    __m128 c6_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a6_2 = _mm_load_ss(&A[58]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*56)+6], c6_2);
    __m128 c6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[59]));
    c6_3 = _mm_add_ps(c6_3, _mm_mul_ps(a6_3, b6));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c6_3));
    __m128 c6_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a6_5 = _mm_load_ss(&A[61]);
    c6_5 = _mm_add_ss(c6_5, _mm_mul_ss(a6_5, b6));
    _mm_store_ss(&C[(l_n*56)+16], c6_5);
    __m128 c6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[62]));
    c6_6 = _mm_add_ps(c6_6, _mm_mul_ps(a6_6, b6));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c6_6));
    __m128 c6_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a6_8 = _mm_load_ss(&A[64]);
    c6_8 = _mm_add_ss(c6_8, _mm_mul_ss(a6_8, b6));
    _mm_store_ss(&C[(l_n*56)+31], c6_8);
    __m128 c6_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a6_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[65]));
    c6_9 = _mm_add_ps(c6_9, _mm_mul_ps(a6_9, b6));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c6_9));
    __m128 c6_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a6_11 = _mm_load_ss(&A[67]);
    c6_11 = _mm_add_ss(c6_11, _mm_mul_ss(a6_11, b6));
    _mm_store_ss(&C[(l_n*56)+52], c6_11);
#else
    C[(l_n*56)+4] += A[56] * B[(l_n*56)+6];
    C[(l_n*56)+5] += A[57] * B[(l_n*56)+6];
    C[(l_n*56)+6] += A[58] * B[(l_n*56)+6];
    C[(l_n*56)+14] += A[59] * B[(l_n*56)+6];
    C[(l_n*56)+15] += A[60] * B[(l_n*56)+6];
    C[(l_n*56)+16] += A[61] * B[(l_n*56)+6];
    C[(l_n*56)+29] += A[62] * B[(l_n*56)+6];
    C[(l_n*56)+30] += A[63] * B[(l_n*56)+6];
    C[(l_n*56)+31] += A[64] * B[(l_n*56)+6];
    C[(l_n*56)+50] += A[65] * B[(l_n*56)+6];
    C[(l_n*56)+51] += A[66] * B[(l_n*56)+6];
    C[(l_n*56)+52] += A[67] * B[(l_n*56)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[68]));
    c7_0 = _mm_add_ps(c7_0, _mm_mul_ps(a7_0, b7));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c7_0));
    __m128 c7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c7_2 = _mm_add_ps(c7_2, _mm_mul_ps(a7_2, b7));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c7_2));
    __m128 c7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[72]));
    c7_4 = _mm_add_ps(c7_4, _mm_mul_ps(a7_4, b7));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c7_4));
    __m128 c7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c7_6 = _mm_add_ps(c7_6, _mm_mul_ps(a7_6, b7));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c7_6));
    __m128 c7_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a7_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[76]));
    c7_8 = _mm_add_ps(c7_8, _mm_mul_ps(a7_8, b7));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c7_8));
#else
    C[(l_n*56)+1] += A[68] * B[(l_n*56)+7];
    C[(l_n*56)+2] += A[69] * B[(l_n*56)+7];
    C[(l_n*56)+7] += A[70] * B[(l_n*56)+7];
    C[(l_n*56)+8] += A[71] * B[(l_n*56)+7];
    C[(l_n*56)+17] += A[72] * B[(l_n*56)+7];
    C[(l_n*56)+18] += A[73] * B[(l_n*56)+7];
    C[(l_n*56)+32] += A[74] * B[(l_n*56)+7];
    C[(l_n*56)+33] += A[75] * B[(l_n*56)+7];
    C[(l_n*56)+53] += A[76] * B[(l_n*56)+7];
    C[(l_n*56)+54] += A[77] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*56)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c8_0 = _mm_add_ps(c8_0, _mm_mul_ps(a8_0, b8));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c8_0));
    __m128 c8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[80]));
    c8_2 = _mm_add_ps(c8_2, _mm_mul_ps(a8_2, b8));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c8_2));
    __m128 c8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[82]));
    c8_4 = _mm_add_ps(c8_4, _mm_mul_ps(a8_4, b8));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c8_4));
    __m128 c8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[84]));
    c8_6 = _mm_add_ps(c8_6, _mm_mul_ps(a8_6, b8));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c8_6));
    __m128 c8_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a8_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c8_8 = _mm_add_ps(c8_8, _mm_mul_ps(a8_8, b8));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c8_8));
#else
    C[(l_n*56)+1] += A[78] * B[(l_n*56)+8];
    C[(l_n*56)+2] += A[79] * B[(l_n*56)+8];
    C[(l_n*56)+7] += A[80] * B[(l_n*56)+8];
    C[(l_n*56)+8] += A[81] * B[(l_n*56)+8];
    C[(l_n*56)+17] += A[82] * B[(l_n*56)+8];
    C[(l_n*56)+18] += A[83] * B[(l_n*56)+8];
    C[(l_n*56)+32] += A[84] * B[(l_n*56)+8];
    C[(l_n*56)+33] += A[85] * B[(l_n*56)+8];
    C[(l_n*56)+53] += A[86] * B[(l_n*56)+8];
    C[(l_n*56)+54] += A[87] * B[(l_n*56)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*56)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a9_0 = _mm_load_ss(&A[88]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*56)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a9_1 = _mm_load_ss(&A[89]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*56)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a9_2 = _mm_load_ss(&A[90]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*56)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a9_3 = _mm_load_ss(&A[91]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*56)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a9_4 = _mm_load_ss(&A[92]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*56)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a9_5 = _mm_load_ss(&A[93]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*56)+55], c9_5);
#else
    C[(l_n*56)+0] += A[88] * B[(l_n*56)+9];
    C[(l_n*56)+3] += A[89] * B[(l_n*56)+9];
    C[(l_n*56)+9] += A[90] * B[(l_n*56)+9];
    C[(l_n*56)+19] += A[91] * B[(l_n*56)+9];
    C[(l_n*56)+34] += A[92] * B[(l_n*56)+9];
    C[(l_n*56)+55] += A[93] * B[(l_n*56)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a10_0 = _mm_loadu_ps(&A[94]);
    c10_0 = _mm_add_ps(c10_0, _mm_mul_ps(a10_0, b10));
    _mm_storeu_ps(&C[(l_n*56)+10], c10_0);
    __m128 c10_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a10_4 = _mm_loadu_ps(&A[98]);
    c10_4 = _mm_add_ps(c10_4, _mm_mul_ps(a10_4, b10));
    _mm_storeu_ps(&C[(l_n*56)+25], c10_4);
    __m128 c10_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a10_8 = _mm_loadu_ps(&A[102]);
    c10_8 = _mm_add_ps(c10_8, _mm_mul_ps(a10_8, b10));
    _mm_storeu_ps(&C[(l_n*56)+46], c10_8);
#else
    C[(l_n*56)+10] += A[94] * B[(l_n*56)+10];
    C[(l_n*56)+11] += A[95] * B[(l_n*56)+10];
    C[(l_n*56)+12] += A[96] * B[(l_n*56)+10];
    C[(l_n*56)+13] += A[97] * B[(l_n*56)+10];
    C[(l_n*56)+25] += A[98] * B[(l_n*56)+10];
    C[(l_n*56)+26] += A[99] * B[(l_n*56)+10];
    C[(l_n*56)+27] += A[100] * B[(l_n*56)+10];
    C[(l_n*56)+28] += A[101] * B[(l_n*56)+10];
    C[(l_n*56)+46] += A[102] * B[(l_n*56)+10];
    C[(l_n*56)+47] += A[103] * B[(l_n*56)+10];
    C[(l_n*56)+48] += A[104] * B[(l_n*56)+10];
    C[(l_n*56)+49] += A[105] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a11_0 = _mm_loadu_ps(&A[106]);
    c11_0 = _mm_add_ps(c11_0, _mm_mul_ps(a11_0, b11));
    _mm_storeu_ps(&C[(l_n*56)+10], c11_0);
    __m128 c11_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a11_4 = _mm_loadu_ps(&A[110]);
    c11_4 = _mm_add_ps(c11_4, _mm_mul_ps(a11_4, b11));
    _mm_storeu_ps(&C[(l_n*56)+25], c11_4);
    __m128 c11_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a11_8 = _mm_loadu_ps(&A[114]);
    c11_8 = _mm_add_ps(c11_8, _mm_mul_ps(a11_8, b11));
    _mm_storeu_ps(&C[(l_n*56)+46], c11_8);
#else
    C[(l_n*56)+10] += A[106] * B[(l_n*56)+11];
    C[(l_n*56)+11] += A[107] * B[(l_n*56)+11];
    C[(l_n*56)+12] += A[108] * B[(l_n*56)+11];
    C[(l_n*56)+13] += A[109] * B[(l_n*56)+11];
    C[(l_n*56)+25] += A[110] * B[(l_n*56)+11];
    C[(l_n*56)+26] += A[111] * B[(l_n*56)+11];
    C[(l_n*56)+27] += A[112] * B[(l_n*56)+11];
    C[(l_n*56)+28] += A[113] * B[(l_n*56)+11];
    C[(l_n*56)+46] += A[114] * B[(l_n*56)+11];
    C[(l_n*56)+47] += A[115] * B[(l_n*56)+11];
    C[(l_n*56)+48] += A[116] * B[(l_n*56)+11];
    C[(l_n*56)+49] += A[117] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a12_0 = _mm_loadu_ps(&A[118]);
    c12_0 = _mm_add_ps(c12_0, _mm_mul_ps(a12_0, b12));
    _mm_storeu_ps(&C[(l_n*56)+10], c12_0);
    __m128 c12_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a12_4 = _mm_loadu_ps(&A[122]);
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_storeu_ps(&C[(l_n*56)+25], c12_4);
    __m128 c12_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a12_8 = _mm_loadu_ps(&A[126]);
    c12_8 = _mm_add_ps(c12_8, _mm_mul_ps(a12_8, b12));
    _mm_storeu_ps(&C[(l_n*56)+46], c12_8);
#else
    C[(l_n*56)+10] += A[118] * B[(l_n*56)+12];
    C[(l_n*56)+11] += A[119] * B[(l_n*56)+12];
    C[(l_n*56)+12] += A[120] * B[(l_n*56)+12];
    C[(l_n*56)+13] += A[121] * B[(l_n*56)+12];
    C[(l_n*56)+25] += A[122] * B[(l_n*56)+12];
    C[(l_n*56)+26] += A[123] * B[(l_n*56)+12];
    C[(l_n*56)+27] += A[124] * B[(l_n*56)+12];
    C[(l_n*56)+28] += A[125] * B[(l_n*56)+12];
    C[(l_n*56)+46] += A[126] * B[(l_n*56)+12];
    C[(l_n*56)+47] += A[127] * B[(l_n*56)+12];
    C[(l_n*56)+48] += A[128] * B[(l_n*56)+12];
    C[(l_n*56)+49] += A[129] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*56)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a13_0 = _mm_loadu_ps(&A[130]);
    c13_0 = _mm_add_ps(c13_0, _mm_mul_ps(a13_0, b13));
    _mm_storeu_ps(&C[(l_n*56)+10], c13_0);
    __m128 c13_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a13_4 = _mm_loadu_ps(&A[134]);
    c13_4 = _mm_add_ps(c13_4, _mm_mul_ps(a13_4, b13));
    _mm_storeu_ps(&C[(l_n*56)+25], c13_4);
    __m128 c13_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a13_8 = _mm_loadu_ps(&A[138]);
    c13_8 = _mm_add_ps(c13_8, _mm_mul_ps(a13_8, b13));
    _mm_storeu_ps(&C[(l_n*56)+46], c13_8);
#else
    C[(l_n*56)+10] += A[130] * B[(l_n*56)+13];
    C[(l_n*56)+11] += A[131] * B[(l_n*56)+13];
    C[(l_n*56)+12] += A[132] * B[(l_n*56)+13];
    C[(l_n*56)+13] += A[133] * B[(l_n*56)+13];
    C[(l_n*56)+25] += A[134] * B[(l_n*56)+13];
    C[(l_n*56)+26] += A[135] * B[(l_n*56)+13];
    C[(l_n*56)+27] += A[136] * B[(l_n*56)+13];
    C[(l_n*56)+28] += A[137] * B[(l_n*56)+13];
    C[(l_n*56)+46] += A[138] * B[(l_n*56)+13];
    C[(l_n*56)+47] += A[139] * B[(l_n*56)+13];
    C[(l_n*56)+48] += A[140] * B[(l_n*56)+13];
    C[(l_n*56)+49] += A[141] * B[(l_n*56)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[142]));
    c14_0 = _mm_add_ps(c14_0, _mm_mul_ps(a14_0, b14));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c14_0));
    __m128 c14_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a14_2 = _mm_load_ss(&A[144]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*56)+6], c14_2);
    __m128 c14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[145]));
    c14_3 = _mm_add_ps(c14_3, _mm_mul_ps(a14_3, b14));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c14_3));
    __m128 c14_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a14_5 = _mm_load_ss(&A[147]);
    c14_5 = _mm_add_ss(c14_5, _mm_mul_ss(a14_5, b14));
    _mm_store_ss(&C[(l_n*56)+16], c14_5);
    __m128 c14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[148]));
    c14_6 = _mm_add_ps(c14_6, _mm_mul_ps(a14_6, b14));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c14_6));
    __m128 c14_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a14_8 = _mm_load_ss(&A[150]);
    c14_8 = _mm_add_ss(c14_8, _mm_mul_ss(a14_8, b14));
    _mm_store_ss(&C[(l_n*56)+31], c14_8);
    __m128 c14_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a14_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[151]));
    c14_9 = _mm_add_ps(c14_9, _mm_mul_ps(a14_9, b14));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c14_9));
    __m128 c14_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a14_11 = _mm_load_ss(&A[153]);
    c14_11 = _mm_add_ss(c14_11, _mm_mul_ss(a14_11, b14));
    _mm_store_ss(&C[(l_n*56)+52], c14_11);
#else
    C[(l_n*56)+4] += A[142] * B[(l_n*56)+14];
    C[(l_n*56)+5] += A[143] * B[(l_n*56)+14];
    C[(l_n*56)+6] += A[144] * B[(l_n*56)+14];
    C[(l_n*56)+14] += A[145] * B[(l_n*56)+14];
    C[(l_n*56)+15] += A[146] * B[(l_n*56)+14];
    C[(l_n*56)+16] += A[147] * B[(l_n*56)+14];
    C[(l_n*56)+29] += A[148] * B[(l_n*56)+14];
    C[(l_n*56)+30] += A[149] * B[(l_n*56)+14];
    C[(l_n*56)+31] += A[150] * B[(l_n*56)+14];
    C[(l_n*56)+50] += A[151] * B[(l_n*56)+14];
    C[(l_n*56)+51] += A[152] * B[(l_n*56)+14];
    C[(l_n*56)+52] += A[153] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[154]));
    c15_0 = _mm_add_ps(c15_0, _mm_mul_ps(a15_0, b15));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c15_0));
    __m128 c15_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a15_2 = _mm_load_ss(&A[156]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*56)+6], c15_2);
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[157]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c15_3));
    __m128 c15_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a15_5 = _mm_load_ss(&A[159]);
    c15_5 = _mm_add_ss(c15_5, _mm_mul_ss(a15_5, b15));
    _mm_store_ss(&C[(l_n*56)+16], c15_5);
    __m128 c15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[160]));
    c15_6 = _mm_add_ps(c15_6, _mm_mul_ps(a15_6, b15));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c15_6));
    __m128 c15_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a15_8 = _mm_load_ss(&A[162]);
    c15_8 = _mm_add_ss(c15_8, _mm_mul_ss(a15_8, b15));
    _mm_store_ss(&C[(l_n*56)+31], c15_8);
    __m128 c15_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a15_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[163]));
    c15_9 = _mm_add_ps(c15_9, _mm_mul_ps(a15_9, b15));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c15_9));
    __m128 c15_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a15_11 = _mm_load_ss(&A[165]);
    c15_11 = _mm_add_ss(c15_11, _mm_mul_ss(a15_11, b15));
    _mm_store_ss(&C[(l_n*56)+52], c15_11);
#else
    C[(l_n*56)+4] += A[154] * B[(l_n*56)+15];
    C[(l_n*56)+5] += A[155] * B[(l_n*56)+15];
    C[(l_n*56)+6] += A[156] * B[(l_n*56)+15];
    C[(l_n*56)+14] += A[157] * B[(l_n*56)+15];
    C[(l_n*56)+15] += A[158] * B[(l_n*56)+15];
    C[(l_n*56)+16] += A[159] * B[(l_n*56)+15];
    C[(l_n*56)+29] += A[160] * B[(l_n*56)+15];
    C[(l_n*56)+30] += A[161] * B[(l_n*56)+15];
    C[(l_n*56)+31] += A[162] * B[(l_n*56)+15];
    C[(l_n*56)+50] += A[163] * B[(l_n*56)+15];
    C[(l_n*56)+51] += A[164] * B[(l_n*56)+15];
    C[(l_n*56)+52] += A[165] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*56)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[166]));
    c16_0 = _mm_add_ps(c16_0, _mm_mul_ps(a16_0, b16));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c16_0));
    __m128 c16_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a16_2 = _mm_load_ss(&A[168]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*56)+6], c16_2);
    __m128 c16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[169]));
    c16_3 = _mm_add_ps(c16_3, _mm_mul_ps(a16_3, b16));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c16_3));
    __m128 c16_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a16_5 = _mm_load_ss(&A[171]);
    c16_5 = _mm_add_ss(c16_5, _mm_mul_ss(a16_5, b16));
    _mm_store_ss(&C[(l_n*56)+16], c16_5);
    __m128 c16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[172]));
    c16_6 = _mm_add_ps(c16_6, _mm_mul_ps(a16_6, b16));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c16_6));
    __m128 c16_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a16_8 = _mm_load_ss(&A[174]);
    c16_8 = _mm_add_ss(c16_8, _mm_mul_ss(a16_8, b16));
    _mm_store_ss(&C[(l_n*56)+31], c16_8);
    __m128 c16_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a16_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[175]));
    c16_9 = _mm_add_ps(c16_9, _mm_mul_ps(a16_9, b16));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c16_9));
    __m128 c16_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a16_11 = _mm_load_ss(&A[177]);
    c16_11 = _mm_add_ss(c16_11, _mm_mul_ss(a16_11, b16));
    _mm_store_ss(&C[(l_n*56)+52], c16_11);
#else
    C[(l_n*56)+4] += A[166] * B[(l_n*56)+16];
    C[(l_n*56)+5] += A[167] * B[(l_n*56)+16];
    C[(l_n*56)+6] += A[168] * B[(l_n*56)+16];
    C[(l_n*56)+14] += A[169] * B[(l_n*56)+16];
    C[(l_n*56)+15] += A[170] * B[(l_n*56)+16];
    C[(l_n*56)+16] += A[171] * B[(l_n*56)+16];
    C[(l_n*56)+29] += A[172] * B[(l_n*56)+16];
    C[(l_n*56)+30] += A[173] * B[(l_n*56)+16];
    C[(l_n*56)+31] += A[174] * B[(l_n*56)+16];
    C[(l_n*56)+50] += A[175] * B[(l_n*56)+16];
    C[(l_n*56)+51] += A[176] * B[(l_n*56)+16];
    C[(l_n*56)+52] += A[177] * B[(l_n*56)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[178]));
    c17_0 = _mm_add_ps(c17_0, _mm_mul_ps(a17_0, b17));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c17_0));
    __m128 c17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[180]));
    c17_2 = _mm_add_ps(c17_2, _mm_mul_ps(a17_2, b17));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c17_2));
    __m128 c17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[182]));
    c17_4 = _mm_add_ps(c17_4, _mm_mul_ps(a17_4, b17));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c17_4));
    __m128 c17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[184]));
    c17_6 = _mm_add_ps(c17_6, _mm_mul_ps(a17_6, b17));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c17_6));
    __m128 c17_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a17_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[186]));
    c17_8 = _mm_add_ps(c17_8, _mm_mul_ps(a17_8, b17));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c17_8));
#else
    C[(l_n*56)+1] += A[178] * B[(l_n*56)+17];
    C[(l_n*56)+2] += A[179] * B[(l_n*56)+17];
    C[(l_n*56)+7] += A[180] * B[(l_n*56)+17];
    C[(l_n*56)+8] += A[181] * B[(l_n*56)+17];
    C[(l_n*56)+17] += A[182] * B[(l_n*56)+17];
    C[(l_n*56)+18] += A[183] * B[(l_n*56)+17];
    C[(l_n*56)+32] += A[184] * B[(l_n*56)+17];
    C[(l_n*56)+33] += A[185] * B[(l_n*56)+17];
    C[(l_n*56)+53] += A[186] * B[(l_n*56)+17];
    C[(l_n*56)+54] += A[187] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*56)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[188]));
    c18_0 = _mm_add_ps(c18_0, _mm_mul_ps(a18_0, b18));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c18_0));
    __m128 c18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[190]));
    c18_2 = _mm_add_ps(c18_2, _mm_mul_ps(a18_2, b18));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c18_2));
    __m128 c18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[192]));
    c18_4 = _mm_add_ps(c18_4, _mm_mul_ps(a18_4, b18));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c18_4));
    __m128 c18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[194]));
    c18_6 = _mm_add_ps(c18_6, _mm_mul_ps(a18_6, b18));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c18_6));
    __m128 c18_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a18_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[196]));
    c18_8 = _mm_add_ps(c18_8, _mm_mul_ps(a18_8, b18));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c18_8));
#else
    C[(l_n*56)+1] += A[188] * B[(l_n*56)+18];
    C[(l_n*56)+2] += A[189] * B[(l_n*56)+18];
    C[(l_n*56)+7] += A[190] * B[(l_n*56)+18];
    C[(l_n*56)+8] += A[191] * B[(l_n*56)+18];
    C[(l_n*56)+17] += A[192] * B[(l_n*56)+18];
    C[(l_n*56)+18] += A[193] * B[(l_n*56)+18];
    C[(l_n*56)+32] += A[194] * B[(l_n*56)+18];
    C[(l_n*56)+33] += A[195] * B[(l_n*56)+18];
    C[(l_n*56)+53] += A[196] * B[(l_n*56)+18];
    C[(l_n*56)+54] += A[197] * B[(l_n*56)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*56)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a19_0 = _mm_load_ss(&A[198]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*56)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a19_1 = _mm_load_ss(&A[199]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*56)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a19_2 = _mm_load_ss(&A[200]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*56)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a19_3 = _mm_load_ss(&A[201]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*56)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a19_4 = _mm_load_ss(&A[202]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*56)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a19_5 = _mm_load_ss(&A[203]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*56)+55], c19_5);
#else
    C[(l_n*56)+0] += A[198] * B[(l_n*56)+19];
    C[(l_n*56)+3] += A[199] * B[(l_n*56)+19];
    C[(l_n*56)+9] += A[200] * B[(l_n*56)+19];
    C[(l_n*56)+19] += A[201] * B[(l_n*56)+19];
    C[(l_n*56)+34] += A[202] * B[(l_n*56)+19];
    C[(l_n*56)+55] += A[203] * B[(l_n*56)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a20_0 = _mm_loadu_ps(&A[204]);
    c20_0 = _mm_add_ps(c20_0, _mm_mul_ps(a20_0, b20));
    _mm_storeu_ps(&C[(l_n*56)+20], c20_0);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a20_4 = _mm_load_ss(&A[208]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*56)+24], c20_4);
    __m128 c20_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a20_5 = _mm_loadu_ps(&A[209]);
    c20_5 = _mm_add_ps(c20_5, _mm_mul_ps(a20_5, b20));
    _mm_storeu_ps(&C[(l_n*56)+41], c20_5);
    __m128 c20_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a20_9 = _mm_load_ss(&A[213]);
    c20_9 = _mm_add_ss(c20_9, _mm_mul_ss(a20_9, b20));
    _mm_store_ss(&C[(l_n*56)+45], c20_9);
#else
    C[(l_n*56)+20] += A[204] * B[(l_n*56)+20];
    C[(l_n*56)+21] += A[205] * B[(l_n*56)+20];
    C[(l_n*56)+22] += A[206] * B[(l_n*56)+20];
    C[(l_n*56)+23] += A[207] * B[(l_n*56)+20];
    C[(l_n*56)+24] += A[208] * B[(l_n*56)+20];
    C[(l_n*56)+41] += A[209] * B[(l_n*56)+20];
    C[(l_n*56)+42] += A[210] * B[(l_n*56)+20];
    C[(l_n*56)+43] += A[211] * B[(l_n*56)+20];
    C[(l_n*56)+44] += A[212] * B[(l_n*56)+20];
    C[(l_n*56)+45] += A[213] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+20]));
    __m128 a21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[214]));
    c21_0 = _mm_add_ps(c21_0, _mm_mul_ps(a21_0, b21));
    _mm_store_sd((double*)&C[(l_n*56)+20], _mm_castps_pd(c21_0));
    __m128 c21_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a21_2 = _mm_load_ss(&A[216]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*56)+22], c21_2);
    __m128 c21_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a21_3 = _mm_load_ss(&A[217]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*56)+24], c21_3);
    __m128 c21_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+41]));
    __m128 a21_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[218]));
    c21_4 = _mm_add_ps(c21_4, _mm_mul_ps(a21_4, b21));
    _mm_store_sd((double*)&C[(l_n*56)+41], _mm_castps_pd(c21_4));
    __m128 c21_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a21_6 = _mm_load_ss(&A[220]);
    c21_6 = _mm_add_ss(c21_6, _mm_mul_ss(a21_6, b21));
    _mm_store_ss(&C[(l_n*56)+43], c21_6);
    __m128 c21_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a21_7 = _mm_load_ss(&A[221]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*56)+45], c21_7);
#else
    C[(l_n*56)+20] += A[214] * B[(l_n*56)+21];
    C[(l_n*56)+21] += A[215] * B[(l_n*56)+21];
    C[(l_n*56)+22] += A[216] * B[(l_n*56)+21];
    C[(l_n*56)+24] += A[217] * B[(l_n*56)+21];
    C[(l_n*56)+41] += A[218] * B[(l_n*56)+21];
    C[(l_n*56)+42] += A[219] * B[(l_n*56)+21];
    C[(l_n*56)+43] += A[220] * B[(l_n*56)+21];
    C[(l_n*56)+45] += A[221] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a22_0 = _mm_loadu_ps(&A[222]);
    c22_0 = _mm_add_ps(c22_0, _mm_mul_ps(a22_0, b22));
    _mm_storeu_ps(&C[(l_n*56)+20], c22_0);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a22_4 = _mm_load_ss(&A[226]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*56)+24], c22_4);
    __m128 c22_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a22_5 = _mm_loadu_ps(&A[227]);
    c22_5 = _mm_add_ps(c22_5, _mm_mul_ps(a22_5, b22));
    _mm_storeu_ps(&C[(l_n*56)+41], c22_5);
    __m128 c22_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a22_9 = _mm_load_ss(&A[231]);
    c22_9 = _mm_add_ss(c22_9, _mm_mul_ss(a22_9, b22));
    _mm_store_ss(&C[(l_n*56)+45], c22_9);
#else
    C[(l_n*56)+20] += A[222] * B[(l_n*56)+22];
    C[(l_n*56)+21] += A[223] * B[(l_n*56)+22];
    C[(l_n*56)+22] += A[224] * B[(l_n*56)+22];
    C[(l_n*56)+23] += A[225] * B[(l_n*56)+22];
    C[(l_n*56)+24] += A[226] * B[(l_n*56)+22];
    C[(l_n*56)+41] += A[227] * B[(l_n*56)+22];
    C[(l_n*56)+42] += A[228] * B[(l_n*56)+22];
    C[(l_n*56)+43] += A[229] * B[(l_n*56)+22];
    C[(l_n*56)+44] += A[230] * B[(l_n*56)+22];
    C[(l_n*56)+45] += A[231] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a23_0 = _mm_load_ss(&A[232]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+20], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+22]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[233]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*56)+22], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a23_3 = _mm_load_ss(&A[235]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*56)+24], c23_3);
    __m128 c23_4 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a23_4 = _mm_load_ss(&A[236]);
    c23_4 = _mm_add_ss(c23_4, _mm_mul_ss(a23_4, b23));
    _mm_store_ss(&C[(l_n*56)+41], c23_4);
    __m128 c23_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+43]));
    __m128 a23_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[237]));
    c23_5 = _mm_add_ps(c23_5, _mm_mul_ps(a23_5, b23));
    _mm_store_sd((double*)&C[(l_n*56)+43], _mm_castps_pd(c23_5));
    __m128 c23_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a23_7 = _mm_load_ss(&A[239]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*56)+45], c23_7);
#else
    C[(l_n*56)+20] += A[232] * B[(l_n*56)+23];
    C[(l_n*56)+22] += A[233] * B[(l_n*56)+23];
    C[(l_n*56)+23] += A[234] * B[(l_n*56)+23];
    C[(l_n*56)+24] += A[235] * B[(l_n*56)+23];
    C[(l_n*56)+41] += A[236] * B[(l_n*56)+23];
    C[(l_n*56)+43] += A[237] * B[(l_n*56)+23];
    C[(l_n*56)+44] += A[238] * B[(l_n*56)+23];
    C[(l_n*56)+45] += A[239] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*56)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a24_0 = _mm_loadu_ps(&A[240]);
    c24_0 = _mm_add_ps(c24_0, _mm_mul_ps(a24_0, b24));
    _mm_storeu_ps(&C[(l_n*56)+20], c24_0);
    __m128 c24_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a24_4 = _mm_load_ss(&A[244]);
    c24_4 = _mm_add_ss(c24_4, _mm_mul_ss(a24_4, b24));
    _mm_store_ss(&C[(l_n*56)+24], c24_4);
    __m128 c24_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a24_5 = _mm_loadu_ps(&A[245]);
    c24_5 = _mm_add_ps(c24_5, _mm_mul_ps(a24_5, b24));
    _mm_storeu_ps(&C[(l_n*56)+41], c24_5);
    __m128 c24_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a24_9 = _mm_load_ss(&A[249]);
    c24_9 = _mm_add_ss(c24_9, _mm_mul_ss(a24_9, b24));
    _mm_store_ss(&C[(l_n*56)+45], c24_9);
#else
    C[(l_n*56)+20] += A[240] * B[(l_n*56)+24];
    C[(l_n*56)+21] += A[241] * B[(l_n*56)+24];
    C[(l_n*56)+22] += A[242] * B[(l_n*56)+24];
    C[(l_n*56)+23] += A[243] * B[(l_n*56)+24];
    C[(l_n*56)+24] += A[244] * B[(l_n*56)+24];
    C[(l_n*56)+41] += A[245] * B[(l_n*56)+24];
    C[(l_n*56)+42] += A[246] * B[(l_n*56)+24];
    C[(l_n*56)+43] += A[247] * B[(l_n*56)+24];
    C[(l_n*56)+44] += A[248] * B[(l_n*56)+24];
    C[(l_n*56)+45] += A[249] * B[(l_n*56)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a25_0 = _mm_loadu_ps(&A[250]);
    c25_0 = _mm_add_ps(c25_0, _mm_mul_ps(a25_0, b25));
    _mm_storeu_ps(&C[(l_n*56)+10], c25_0);
    __m128 c25_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a25_4 = _mm_loadu_ps(&A[254]);
    c25_4 = _mm_add_ps(c25_4, _mm_mul_ps(a25_4, b25));
    _mm_storeu_ps(&C[(l_n*56)+25], c25_4);
    __m128 c25_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a25_8 = _mm_loadu_ps(&A[258]);
    c25_8 = _mm_add_ps(c25_8, _mm_mul_ps(a25_8, b25));
    _mm_storeu_ps(&C[(l_n*56)+46], c25_8);
#else
    C[(l_n*56)+10] += A[250] * B[(l_n*56)+25];
    C[(l_n*56)+11] += A[251] * B[(l_n*56)+25];
    C[(l_n*56)+12] += A[252] * B[(l_n*56)+25];
    C[(l_n*56)+13] += A[253] * B[(l_n*56)+25];
    C[(l_n*56)+25] += A[254] * B[(l_n*56)+25];
    C[(l_n*56)+26] += A[255] * B[(l_n*56)+25];
    C[(l_n*56)+27] += A[256] * B[(l_n*56)+25];
    C[(l_n*56)+28] += A[257] * B[(l_n*56)+25];
    C[(l_n*56)+46] += A[258] * B[(l_n*56)+25];
    C[(l_n*56)+47] += A[259] * B[(l_n*56)+25];
    C[(l_n*56)+48] += A[260] * B[(l_n*56)+25];
    C[(l_n*56)+49] += A[261] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a26_0 = _mm_loadu_ps(&A[262]);
    c26_0 = _mm_add_ps(c26_0, _mm_mul_ps(a26_0, b26));
    _mm_storeu_ps(&C[(l_n*56)+10], c26_0);
    __m128 c26_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a26_4 = _mm_loadu_ps(&A[266]);
    c26_4 = _mm_add_ps(c26_4, _mm_mul_ps(a26_4, b26));
    _mm_storeu_ps(&C[(l_n*56)+25], c26_4);
    __m128 c26_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a26_8 = _mm_loadu_ps(&A[270]);
    c26_8 = _mm_add_ps(c26_8, _mm_mul_ps(a26_8, b26));
    _mm_storeu_ps(&C[(l_n*56)+46], c26_8);
#else
    C[(l_n*56)+10] += A[262] * B[(l_n*56)+26];
    C[(l_n*56)+11] += A[263] * B[(l_n*56)+26];
    C[(l_n*56)+12] += A[264] * B[(l_n*56)+26];
    C[(l_n*56)+13] += A[265] * B[(l_n*56)+26];
    C[(l_n*56)+25] += A[266] * B[(l_n*56)+26];
    C[(l_n*56)+26] += A[267] * B[(l_n*56)+26];
    C[(l_n*56)+27] += A[268] * B[(l_n*56)+26];
    C[(l_n*56)+28] += A[269] * B[(l_n*56)+26];
    C[(l_n*56)+46] += A[270] * B[(l_n*56)+26];
    C[(l_n*56)+47] += A[271] * B[(l_n*56)+26];
    C[(l_n*56)+48] += A[272] * B[(l_n*56)+26];
    C[(l_n*56)+49] += A[273] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a27_0 = _mm_loadu_ps(&A[274]);
    c27_0 = _mm_add_ps(c27_0, _mm_mul_ps(a27_0, b27));
    _mm_storeu_ps(&C[(l_n*56)+10], c27_0);
    __m128 c27_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a27_4 = _mm_loadu_ps(&A[278]);
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_storeu_ps(&C[(l_n*56)+25], c27_4);
    __m128 c27_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a27_8 = _mm_loadu_ps(&A[282]);
    c27_8 = _mm_add_ps(c27_8, _mm_mul_ps(a27_8, b27));
    _mm_storeu_ps(&C[(l_n*56)+46], c27_8);
#else
    C[(l_n*56)+10] += A[274] * B[(l_n*56)+27];
    C[(l_n*56)+11] += A[275] * B[(l_n*56)+27];
    C[(l_n*56)+12] += A[276] * B[(l_n*56)+27];
    C[(l_n*56)+13] += A[277] * B[(l_n*56)+27];
    C[(l_n*56)+25] += A[278] * B[(l_n*56)+27];
    C[(l_n*56)+26] += A[279] * B[(l_n*56)+27];
    C[(l_n*56)+27] += A[280] * B[(l_n*56)+27];
    C[(l_n*56)+28] += A[281] * B[(l_n*56)+27];
    C[(l_n*56)+46] += A[282] * B[(l_n*56)+27];
    C[(l_n*56)+47] += A[283] * B[(l_n*56)+27];
    C[(l_n*56)+48] += A[284] * B[(l_n*56)+27];
    C[(l_n*56)+49] += A[285] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*56)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a28_0 = _mm_loadu_ps(&A[286]);
    c28_0 = _mm_add_ps(c28_0, _mm_mul_ps(a28_0, b28));
    _mm_storeu_ps(&C[(l_n*56)+10], c28_0);
    __m128 c28_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a28_4 = _mm_loadu_ps(&A[290]);
    c28_4 = _mm_add_ps(c28_4, _mm_mul_ps(a28_4, b28));
    _mm_storeu_ps(&C[(l_n*56)+25], c28_4);
    __m128 c28_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a28_8 = _mm_loadu_ps(&A[294]);
    c28_8 = _mm_add_ps(c28_8, _mm_mul_ps(a28_8, b28));
    _mm_storeu_ps(&C[(l_n*56)+46], c28_8);
#else
    C[(l_n*56)+10] += A[286] * B[(l_n*56)+28];
    C[(l_n*56)+11] += A[287] * B[(l_n*56)+28];
    C[(l_n*56)+12] += A[288] * B[(l_n*56)+28];
    C[(l_n*56)+13] += A[289] * B[(l_n*56)+28];
    C[(l_n*56)+25] += A[290] * B[(l_n*56)+28];
    C[(l_n*56)+26] += A[291] * B[(l_n*56)+28];
    C[(l_n*56)+27] += A[292] * B[(l_n*56)+28];
    C[(l_n*56)+28] += A[293] * B[(l_n*56)+28];
    C[(l_n*56)+46] += A[294] * B[(l_n*56)+28];
    C[(l_n*56)+47] += A[295] * B[(l_n*56)+28];
    C[(l_n*56)+48] += A[296] * B[(l_n*56)+28];
    C[(l_n*56)+49] += A[297] * B[(l_n*56)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[298]));
    c29_0 = _mm_add_ps(c29_0, _mm_mul_ps(a29_0, b29));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c29_0));
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a29_2 = _mm_load_ss(&A[300]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+6], c29_2);
    __m128 c29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[301]));
    c29_3 = _mm_add_ps(c29_3, _mm_mul_ps(a29_3, b29));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c29_3));
    __m128 c29_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a29_5 = _mm_load_ss(&A[303]);
    c29_5 = _mm_add_ss(c29_5, _mm_mul_ss(a29_5, b29));
    _mm_store_ss(&C[(l_n*56)+16], c29_5);
    __m128 c29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[304]));
    c29_6 = _mm_add_ps(c29_6, _mm_mul_ps(a29_6, b29));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c29_6));
    __m128 c29_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a29_8 = _mm_load_ss(&A[306]);
    c29_8 = _mm_add_ss(c29_8, _mm_mul_ss(a29_8, b29));
    _mm_store_ss(&C[(l_n*56)+31], c29_8);
    __m128 c29_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a29_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[307]));
    c29_9 = _mm_add_ps(c29_9, _mm_mul_ps(a29_9, b29));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c29_9));
    __m128 c29_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a29_11 = _mm_load_ss(&A[309]);
    c29_11 = _mm_add_ss(c29_11, _mm_mul_ss(a29_11, b29));
    _mm_store_ss(&C[(l_n*56)+52], c29_11);
#else
    C[(l_n*56)+4] += A[298] * B[(l_n*56)+29];
    C[(l_n*56)+5] += A[299] * B[(l_n*56)+29];
    C[(l_n*56)+6] += A[300] * B[(l_n*56)+29];
    C[(l_n*56)+14] += A[301] * B[(l_n*56)+29];
    C[(l_n*56)+15] += A[302] * B[(l_n*56)+29];
    C[(l_n*56)+16] += A[303] * B[(l_n*56)+29];
    C[(l_n*56)+29] += A[304] * B[(l_n*56)+29];
    C[(l_n*56)+30] += A[305] * B[(l_n*56)+29];
    C[(l_n*56)+31] += A[306] * B[(l_n*56)+29];
    C[(l_n*56)+50] += A[307] * B[(l_n*56)+29];
    C[(l_n*56)+51] += A[308] * B[(l_n*56)+29];
    C[(l_n*56)+52] += A[309] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[310]));
    c30_0 = _mm_add_ps(c30_0, _mm_mul_ps(a30_0, b30));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c30_0));
    __m128 c30_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a30_2 = _mm_load_ss(&A[312]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*56)+6], c30_2);
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[313]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a30_5 = _mm_load_ss(&A[315]);
    c30_5 = _mm_add_ss(c30_5, _mm_mul_ss(a30_5, b30));
    _mm_store_ss(&C[(l_n*56)+16], c30_5);
    __m128 c30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[316]));
    c30_6 = _mm_add_ps(c30_6, _mm_mul_ps(a30_6, b30));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c30_6));
    __m128 c30_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a30_8 = _mm_load_ss(&A[318]);
    c30_8 = _mm_add_ss(c30_8, _mm_mul_ss(a30_8, b30));
    _mm_store_ss(&C[(l_n*56)+31], c30_8);
    __m128 c30_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a30_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[319]));
    c30_9 = _mm_add_ps(c30_9, _mm_mul_ps(a30_9, b30));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c30_9));
    __m128 c30_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a30_11 = _mm_load_ss(&A[321]);
    c30_11 = _mm_add_ss(c30_11, _mm_mul_ss(a30_11, b30));
    _mm_store_ss(&C[(l_n*56)+52], c30_11);
#else
    C[(l_n*56)+4] += A[310] * B[(l_n*56)+30];
    C[(l_n*56)+5] += A[311] * B[(l_n*56)+30];
    C[(l_n*56)+6] += A[312] * B[(l_n*56)+30];
    C[(l_n*56)+14] += A[313] * B[(l_n*56)+30];
    C[(l_n*56)+15] += A[314] * B[(l_n*56)+30];
    C[(l_n*56)+16] += A[315] * B[(l_n*56)+30];
    C[(l_n*56)+29] += A[316] * B[(l_n*56)+30];
    C[(l_n*56)+30] += A[317] * B[(l_n*56)+30];
    C[(l_n*56)+31] += A[318] * B[(l_n*56)+30];
    C[(l_n*56)+50] += A[319] * B[(l_n*56)+30];
    C[(l_n*56)+51] += A[320] * B[(l_n*56)+30];
    C[(l_n*56)+52] += A[321] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*56)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[322]));
    c31_0 = _mm_add_ps(c31_0, _mm_mul_ps(a31_0, b31));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c31_0));
    __m128 c31_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a31_2 = _mm_load_ss(&A[324]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*56)+6], c31_2);
    __m128 c31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[325]));
    c31_3 = _mm_add_ps(c31_3, _mm_mul_ps(a31_3, b31));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c31_3));
    __m128 c31_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a31_5 = _mm_load_ss(&A[327]);
    c31_5 = _mm_add_ss(c31_5, _mm_mul_ss(a31_5, b31));
    _mm_store_ss(&C[(l_n*56)+16], c31_5);
    __m128 c31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[328]));
    c31_6 = _mm_add_ps(c31_6, _mm_mul_ps(a31_6, b31));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c31_6));
    __m128 c31_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a31_8 = _mm_load_ss(&A[330]);
    c31_8 = _mm_add_ss(c31_8, _mm_mul_ss(a31_8, b31));
    _mm_store_ss(&C[(l_n*56)+31], c31_8);
    __m128 c31_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a31_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[331]));
    c31_9 = _mm_add_ps(c31_9, _mm_mul_ps(a31_9, b31));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c31_9));
    __m128 c31_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a31_11 = _mm_load_ss(&A[333]);
    c31_11 = _mm_add_ss(c31_11, _mm_mul_ss(a31_11, b31));
    _mm_store_ss(&C[(l_n*56)+52], c31_11);
#else
    C[(l_n*56)+4] += A[322] * B[(l_n*56)+31];
    C[(l_n*56)+5] += A[323] * B[(l_n*56)+31];
    C[(l_n*56)+6] += A[324] * B[(l_n*56)+31];
    C[(l_n*56)+14] += A[325] * B[(l_n*56)+31];
    C[(l_n*56)+15] += A[326] * B[(l_n*56)+31];
    C[(l_n*56)+16] += A[327] * B[(l_n*56)+31];
    C[(l_n*56)+29] += A[328] * B[(l_n*56)+31];
    C[(l_n*56)+30] += A[329] * B[(l_n*56)+31];
    C[(l_n*56)+31] += A[330] * B[(l_n*56)+31];
    C[(l_n*56)+50] += A[331] * B[(l_n*56)+31];
    C[(l_n*56)+51] += A[332] * B[(l_n*56)+31];
    C[(l_n*56)+52] += A[333] * B[(l_n*56)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[334]));
    c32_0 = _mm_add_ps(c32_0, _mm_mul_ps(a32_0, b32));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c32_0));
    __m128 c32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[336]));
    c32_2 = _mm_add_ps(c32_2, _mm_mul_ps(a32_2, b32));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c32_2));
    __m128 c32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[338]));
    c32_4 = _mm_add_ps(c32_4, _mm_mul_ps(a32_4, b32));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c32_4));
    __m128 c32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[340]));
    c32_6 = _mm_add_ps(c32_6, _mm_mul_ps(a32_6, b32));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c32_6));
    __m128 c32_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a32_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[342]));
    c32_8 = _mm_add_ps(c32_8, _mm_mul_ps(a32_8, b32));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c32_8));
#else
    C[(l_n*56)+1] += A[334] * B[(l_n*56)+32];
    C[(l_n*56)+2] += A[335] * B[(l_n*56)+32];
    C[(l_n*56)+7] += A[336] * B[(l_n*56)+32];
    C[(l_n*56)+8] += A[337] * B[(l_n*56)+32];
    C[(l_n*56)+17] += A[338] * B[(l_n*56)+32];
    C[(l_n*56)+18] += A[339] * B[(l_n*56)+32];
    C[(l_n*56)+32] += A[340] * B[(l_n*56)+32];
    C[(l_n*56)+33] += A[341] * B[(l_n*56)+32];
    C[(l_n*56)+53] += A[342] * B[(l_n*56)+32];
    C[(l_n*56)+54] += A[343] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*56)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[344]));
    c33_0 = _mm_add_ps(c33_0, _mm_mul_ps(a33_0, b33));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c33_0));
    __m128 c33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[346]));
    c33_2 = _mm_add_ps(c33_2, _mm_mul_ps(a33_2, b33));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c33_2));
    __m128 c33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[348]));
    c33_4 = _mm_add_ps(c33_4, _mm_mul_ps(a33_4, b33));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c33_4));
    __m128 c33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[350]));
    c33_6 = _mm_add_ps(c33_6, _mm_mul_ps(a33_6, b33));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c33_6));
    __m128 c33_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a33_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[352]));
    c33_8 = _mm_add_ps(c33_8, _mm_mul_ps(a33_8, b33));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c33_8));
#else
    C[(l_n*56)+1] += A[344] * B[(l_n*56)+33];
    C[(l_n*56)+2] += A[345] * B[(l_n*56)+33];
    C[(l_n*56)+7] += A[346] * B[(l_n*56)+33];
    C[(l_n*56)+8] += A[347] * B[(l_n*56)+33];
    C[(l_n*56)+17] += A[348] * B[(l_n*56)+33];
    C[(l_n*56)+18] += A[349] * B[(l_n*56)+33];
    C[(l_n*56)+32] += A[350] * B[(l_n*56)+33];
    C[(l_n*56)+33] += A[351] * B[(l_n*56)+33];
    C[(l_n*56)+53] += A[352] * B[(l_n*56)+33];
    C[(l_n*56)+54] += A[353] * B[(l_n*56)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*56)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a34_0 = _mm_load_ss(&A[354]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*56)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a34_1 = _mm_load_ss(&A[355]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*56)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a34_2 = _mm_load_ss(&A[356]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*56)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a34_3 = _mm_load_ss(&A[357]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*56)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a34_4 = _mm_load_ss(&A[358]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*56)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a34_5 = _mm_load_ss(&A[359]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*56)+55], c34_5);
#else
    C[(l_n*56)+0] += A[354] * B[(l_n*56)+34];
    C[(l_n*56)+3] += A[355] * B[(l_n*56)+34];
    C[(l_n*56)+9] += A[356] * B[(l_n*56)+34];
    C[(l_n*56)+19] += A[357] * B[(l_n*56)+34];
    C[(l_n*56)+34] += A[358] * B[(l_n*56)+34];
    C[(l_n*56)+55] += A[359] * B[(l_n*56)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a35_0 = _mm_loadu_ps(&A[360]);
    c35_0 = _mm_add_ps(c35_0, _mm_mul_ps(a35_0, b35));
    _mm_storeu_ps(&C[(l_n*56)+35], c35_0);
    __m128 c35_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a35_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[364]));
    c35_4 = _mm_add_ps(c35_4, _mm_mul_ps(a35_4, b35));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c35_4));
#else
    C[(l_n*56)+35] += A[360] * B[(l_n*56)+35];
    C[(l_n*56)+36] += A[361] * B[(l_n*56)+35];
    C[(l_n*56)+37] += A[362] * B[(l_n*56)+35];
    C[(l_n*56)+38] += A[363] * B[(l_n*56)+35];
    C[(l_n*56)+39] += A[364] * B[(l_n*56)+35];
    C[(l_n*56)+40] += A[365] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a36_0 = _mm_loadu_ps(&A[366]);
    c36_0 = _mm_add_ps(c36_0, _mm_mul_ps(a36_0, b36));
    _mm_storeu_ps(&C[(l_n*56)+35], c36_0);
    __m128 c36_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a36_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[370]));
    c36_4 = _mm_add_ps(c36_4, _mm_mul_ps(a36_4, b36));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c36_4));
#else
    C[(l_n*56)+35] += A[366] * B[(l_n*56)+36];
    C[(l_n*56)+36] += A[367] * B[(l_n*56)+36];
    C[(l_n*56)+37] += A[368] * B[(l_n*56)+36];
    C[(l_n*56)+38] += A[369] * B[(l_n*56)+36];
    C[(l_n*56)+39] += A[370] * B[(l_n*56)+36];
    C[(l_n*56)+40] += A[371] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a37_0 = _mm_loadu_ps(&A[372]);
    c37_0 = _mm_add_ps(c37_0, _mm_mul_ps(a37_0, b37));
    _mm_storeu_ps(&C[(l_n*56)+35], c37_0);
    __m128 c37_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a37_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[376]));
    c37_4 = _mm_add_ps(c37_4, _mm_mul_ps(a37_4, b37));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c37_4));
#else
    C[(l_n*56)+35] += A[372] * B[(l_n*56)+37];
    C[(l_n*56)+36] += A[373] * B[(l_n*56)+37];
    C[(l_n*56)+37] += A[374] * B[(l_n*56)+37];
    C[(l_n*56)+38] += A[375] * B[(l_n*56)+37];
    C[(l_n*56)+39] += A[376] * B[(l_n*56)+37];
    C[(l_n*56)+40] += A[377] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a38_0 = _mm_loadu_ps(&A[378]);
    c38_0 = _mm_add_ps(c38_0, _mm_mul_ps(a38_0, b38));
    _mm_storeu_ps(&C[(l_n*56)+35], c38_0);
    __m128 c38_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a38_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[382]));
    c38_4 = _mm_add_ps(c38_4, _mm_mul_ps(a38_4, b38));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c38_4));
#else
    C[(l_n*56)+35] += A[378] * B[(l_n*56)+38];
    C[(l_n*56)+36] += A[379] * B[(l_n*56)+38];
    C[(l_n*56)+37] += A[380] * B[(l_n*56)+38];
    C[(l_n*56)+38] += A[381] * B[(l_n*56)+38];
    C[(l_n*56)+39] += A[382] * B[(l_n*56)+38];
    C[(l_n*56)+40] += A[383] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a39_0 = _mm_loadu_ps(&A[384]);
    c39_0 = _mm_add_ps(c39_0, _mm_mul_ps(a39_0, b39));
    _mm_storeu_ps(&C[(l_n*56)+35], c39_0);
    __m128 c39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[388]));
    c39_4 = _mm_add_ps(c39_4, _mm_mul_ps(a39_4, b39));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c39_4));
#else
    C[(l_n*56)+35] += A[384] * B[(l_n*56)+39];
    C[(l_n*56)+36] += A[385] * B[(l_n*56)+39];
    C[(l_n*56)+37] += A[386] * B[(l_n*56)+39];
    C[(l_n*56)+38] += A[387] * B[(l_n*56)+39];
    C[(l_n*56)+39] += A[388] * B[(l_n*56)+39];
    C[(l_n*56)+40] += A[389] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*56)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a40_0 = _mm_loadu_ps(&A[390]);
    c40_0 = _mm_add_ps(c40_0, _mm_mul_ps(a40_0, b40));
    _mm_storeu_ps(&C[(l_n*56)+35], c40_0);
    __m128 c40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[394]));
    c40_4 = _mm_add_ps(c40_4, _mm_mul_ps(a40_4, b40));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c40_4));
#else
    C[(l_n*56)+35] += A[390] * B[(l_n*56)+40];
    C[(l_n*56)+36] += A[391] * B[(l_n*56)+40];
    C[(l_n*56)+37] += A[392] * B[(l_n*56)+40];
    C[(l_n*56)+38] += A[393] * B[(l_n*56)+40];
    C[(l_n*56)+39] += A[394] * B[(l_n*56)+40];
    C[(l_n*56)+40] += A[395] * B[(l_n*56)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a41_0 = _mm_loadu_ps(&A[396]);
    c41_0 = _mm_add_ps(c41_0, _mm_mul_ps(a41_0, b41));
    _mm_storeu_ps(&C[(l_n*56)+20], c41_0);
    __m128 c41_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a41_4 = _mm_load_ss(&A[400]);
    c41_4 = _mm_add_ss(c41_4, _mm_mul_ss(a41_4, b41));
    _mm_store_ss(&C[(l_n*56)+24], c41_4);
    __m128 c41_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a41_5 = _mm_loadu_ps(&A[401]);
    c41_5 = _mm_add_ps(c41_5, _mm_mul_ps(a41_5, b41));
    _mm_storeu_ps(&C[(l_n*56)+41], c41_5);
    __m128 c41_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a41_9 = _mm_load_ss(&A[405]);
    c41_9 = _mm_add_ss(c41_9, _mm_mul_ss(a41_9, b41));
    _mm_store_ss(&C[(l_n*56)+45], c41_9);
#else
    C[(l_n*56)+20] += A[396] * B[(l_n*56)+41];
    C[(l_n*56)+21] += A[397] * B[(l_n*56)+41];
    C[(l_n*56)+22] += A[398] * B[(l_n*56)+41];
    C[(l_n*56)+23] += A[399] * B[(l_n*56)+41];
    C[(l_n*56)+24] += A[400] * B[(l_n*56)+41];
    C[(l_n*56)+41] += A[401] * B[(l_n*56)+41];
    C[(l_n*56)+42] += A[402] * B[(l_n*56)+41];
    C[(l_n*56)+43] += A[403] * B[(l_n*56)+41];
    C[(l_n*56)+44] += A[404] * B[(l_n*56)+41];
    C[(l_n*56)+45] += A[405] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+20]));
    __m128 a42_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[406]));
    c42_0 = _mm_add_ps(c42_0, _mm_mul_ps(a42_0, b42));
    _mm_store_sd((double*)&C[(l_n*56)+20], _mm_castps_pd(c42_0));
    __m128 c42_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a42_2 = _mm_load_ss(&A[408]);
    c42_2 = _mm_add_ss(c42_2, _mm_mul_ss(a42_2, b42));
    _mm_store_ss(&C[(l_n*56)+22], c42_2);
    __m128 c42_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a42_3 = _mm_load_ss(&A[409]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*56)+24], c42_3);
    __m128 c42_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+41]));
    __m128 a42_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[410]));
    c42_4 = _mm_add_ps(c42_4, _mm_mul_ps(a42_4, b42));
    _mm_store_sd((double*)&C[(l_n*56)+41], _mm_castps_pd(c42_4));
    __m128 c42_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a42_6 = _mm_load_ss(&A[412]);
    c42_6 = _mm_add_ss(c42_6, _mm_mul_ss(a42_6, b42));
    _mm_store_ss(&C[(l_n*56)+43], c42_6);
    __m128 c42_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a42_7 = _mm_load_ss(&A[413]);
    c42_7 = _mm_add_ss(c42_7, _mm_mul_ss(a42_7, b42));
    _mm_store_ss(&C[(l_n*56)+45], c42_7);
#else
    C[(l_n*56)+20] += A[406] * B[(l_n*56)+42];
    C[(l_n*56)+21] += A[407] * B[(l_n*56)+42];
    C[(l_n*56)+22] += A[408] * B[(l_n*56)+42];
    C[(l_n*56)+24] += A[409] * B[(l_n*56)+42];
    C[(l_n*56)+41] += A[410] * B[(l_n*56)+42];
    C[(l_n*56)+42] += A[411] * B[(l_n*56)+42];
    C[(l_n*56)+43] += A[412] * B[(l_n*56)+42];
    C[(l_n*56)+45] += A[413] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a43_0 = _mm_loadu_ps(&A[414]);
    c43_0 = _mm_add_ps(c43_0, _mm_mul_ps(a43_0, b43));
    _mm_storeu_ps(&C[(l_n*56)+20], c43_0);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a43_4 = _mm_load_ss(&A[418]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*56)+24], c43_4);
    __m128 c43_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a43_5 = _mm_loadu_ps(&A[419]);
    c43_5 = _mm_add_ps(c43_5, _mm_mul_ps(a43_5, b43));
    _mm_storeu_ps(&C[(l_n*56)+41], c43_5);
    __m128 c43_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a43_9 = _mm_load_ss(&A[423]);
    c43_9 = _mm_add_ss(c43_9, _mm_mul_ss(a43_9, b43));
    _mm_store_ss(&C[(l_n*56)+45], c43_9);
#else
    C[(l_n*56)+20] += A[414] * B[(l_n*56)+43];
    C[(l_n*56)+21] += A[415] * B[(l_n*56)+43];
    C[(l_n*56)+22] += A[416] * B[(l_n*56)+43];
    C[(l_n*56)+23] += A[417] * B[(l_n*56)+43];
    C[(l_n*56)+24] += A[418] * B[(l_n*56)+43];
    C[(l_n*56)+41] += A[419] * B[(l_n*56)+43];
    C[(l_n*56)+42] += A[420] * B[(l_n*56)+43];
    C[(l_n*56)+43] += A[421] * B[(l_n*56)+43];
    C[(l_n*56)+44] += A[422] * B[(l_n*56)+43];
    C[(l_n*56)+45] += A[423] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a44_0 = _mm_load_ss(&A[424]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+20], c44_0);
    __m128 c44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+22]));
    __m128 a44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[425]));
    c44_1 = _mm_add_ps(c44_1, _mm_mul_ps(a44_1, b44));
    _mm_store_sd((double*)&C[(l_n*56)+22], _mm_castps_pd(c44_1));
    __m128 c44_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a44_3 = _mm_load_ss(&A[427]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*56)+24], c44_3);
    __m128 c44_4 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a44_4 = _mm_load_ss(&A[428]);
    c44_4 = _mm_add_ss(c44_4, _mm_mul_ss(a44_4, b44));
    _mm_store_ss(&C[(l_n*56)+41], c44_4);
    __m128 c44_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+43]));
    __m128 a44_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[429]));
    c44_5 = _mm_add_ps(c44_5, _mm_mul_ps(a44_5, b44));
    _mm_store_sd((double*)&C[(l_n*56)+43], _mm_castps_pd(c44_5));
    __m128 c44_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a44_7 = _mm_load_ss(&A[431]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*56)+45], c44_7);
#else
    C[(l_n*56)+20] += A[424] * B[(l_n*56)+44];
    C[(l_n*56)+22] += A[425] * B[(l_n*56)+44];
    C[(l_n*56)+23] += A[426] * B[(l_n*56)+44];
    C[(l_n*56)+24] += A[427] * B[(l_n*56)+44];
    C[(l_n*56)+41] += A[428] * B[(l_n*56)+44];
    C[(l_n*56)+43] += A[429] * B[(l_n*56)+44];
    C[(l_n*56)+44] += A[430] * B[(l_n*56)+44];
    C[(l_n*56)+45] += A[431] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*56)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a45_0 = _mm_loadu_ps(&A[432]);
    c45_0 = _mm_add_ps(c45_0, _mm_mul_ps(a45_0, b45));
    _mm_storeu_ps(&C[(l_n*56)+20], c45_0);
    __m128 c45_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a45_4 = _mm_load_ss(&A[436]);
    c45_4 = _mm_add_ss(c45_4, _mm_mul_ss(a45_4, b45));
    _mm_store_ss(&C[(l_n*56)+24], c45_4);
    __m128 c45_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a45_5 = _mm_loadu_ps(&A[437]);
    c45_5 = _mm_add_ps(c45_5, _mm_mul_ps(a45_5, b45));
    _mm_storeu_ps(&C[(l_n*56)+41], c45_5);
    __m128 c45_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a45_9 = _mm_load_ss(&A[441]);
    c45_9 = _mm_add_ss(c45_9, _mm_mul_ss(a45_9, b45));
    _mm_store_ss(&C[(l_n*56)+45], c45_9);
#else
    C[(l_n*56)+20] += A[432] * B[(l_n*56)+45];
    C[(l_n*56)+21] += A[433] * B[(l_n*56)+45];
    C[(l_n*56)+22] += A[434] * B[(l_n*56)+45];
    C[(l_n*56)+23] += A[435] * B[(l_n*56)+45];
    C[(l_n*56)+24] += A[436] * B[(l_n*56)+45];
    C[(l_n*56)+41] += A[437] * B[(l_n*56)+45];
    C[(l_n*56)+42] += A[438] * B[(l_n*56)+45];
    C[(l_n*56)+43] += A[439] * B[(l_n*56)+45];
    C[(l_n*56)+44] += A[440] * B[(l_n*56)+45];
    C[(l_n*56)+45] += A[441] * B[(l_n*56)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a46_0 = _mm_loadu_ps(&A[442]);
    c46_0 = _mm_add_ps(c46_0, _mm_mul_ps(a46_0, b46));
    _mm_storeu_ps(&C[(l_n*56)+10], c46_0);
    __m128 c46_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a46_4 = _mm_loadu_ps(&A[446]);
    c46_4 = _mm_add_ps(c46_4, _mm_mul_ps(a46_4, b46));
    _mm_storeu_ps(&C[(l_n*56)+25], c46_4);
    __m128 c46_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a46_8 = _mm_loadu_ps(&A[450]);
    c46_8 = _mm_add_ps(c46_8, _mm_mul_ps(a46_8, b46));
    _mm_storeu_ps(&C[(l_n*56)+46], c46_8);
#else
    C[(l_n*56)+10] += A[442] * B[(l_n*56)+46];
    C[(l_n*56)+11] += A[443] * B[(l_n*56)+46];
    C[(l_n*56)+12] += A[444] * B[(l_n*56)+46];
    C[(l_n*56)+13] += A[445] * B[(l_n*56)+46];
    C[(l_n*56)+25] += A[446] * B[(l_n*56)+46];
    C[(l_n*56)+26] += A[447] * B[(l_n*56)+46];
    C[(l_n*56)+27] += A[448] * B[(l_n*56)+46];
    C[(l_n*56)+28] += A[449] * B[(l_n*56)+46];
    C[(l_n*56)+46] += A[450] * B[(l_n*56)+46];
    C[(l_n*56)+47] += A[451] * B[(l_n*56)+46];
    C[(l_n*56)+48] += A[452] * B[(l_n*56)+46];
    C[(l_n*56)+49] += A[453] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a47_0 = _mm_loadu_ps(&A[454]);
    c47_0 = _mm_add_ps(c47_0, _mm_mul_ps(a47_0, b47));
    _mm_storeu_ps(&C[(l_n*56)+10], c47_0);
    __m128 c47_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a47_4 = _mm_loadu_ps(&A[458]);
    c47_4 = _mm_add_ps(c47_4, _mm_mul_ps(a47_4, b47));
    _mm_storeu_ps(&C[(l_n*56)+25], c47_4);
    __m128 c47_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a47_8 = _mm_loadu_ps(&A[462]);
    c47_8 = _mm_add_ps(c47_8, _mm_mul_ps(a47_8, b47));
    _mm_storeu_ps(&C[(l_n*56)+46], c47_8);
#else
    C[(l_n*56)+10] += A[454] * B[(l_n*56)+47];
    C[(l_n*56)+11] += A[455] * B[(l_n*56)+47];
    C[(l_n*56)+12] += A[456] * B[(l_n*56)+47];
    C[(l_n*56)+13] += A[457] * B[(l_n*56)+47];
    C[(l_n*56)+25] += A[458] * B[(l_n*56)+47];
    C[(l_n*56)+26] += A[459] * B[(l_n*56)+47];
    C[(l_n*56)+27] += A[460] * B[(l_n*56)+47];
    C[(l_n*56)+28] += A[461] * B[(l_n*56)+47];
    C[(l_n*56)+46] += A[462] * B[(l_n*56)+47];
    C[(l_n*56)+47] += A[463] * B[(l_n*56)+47];
    C[(l_n*56)+48] += A[464] * B[(l_n*56)+47];
    C[(l_n*56)+49] += A[465] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a48_0 = _mm_loadu_ps(&A[466]);
    c48_0 = _mm_add_ps(c48_0, _mm_mul_ps(a48_0, b48));
    _mm_storeu_ps(&C[(l_n*56)+10], c48_0);
    __m128 c48_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a48_4 = _mm_loadu_ps(&A[470]);
    c48_4 = _mm_add_ps(c48_4, _mm_mul_ps(a48_4, b48));
    _mm_storeu_ps(&C[(l_n*56)+25], c48_4);
    __m128 c48_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a48_8 = _mm_loadu_ps(&A[474]);
    c48_8 = _mm_add_ps(c48_8, _mm_mul_ps(a48_8, b48));
    _mm_storeu_ps(&C[(l_n*56)+46], c48_8);
#else
    C[(l_n*56)+10] += A[466] * B[(l_n*56)+48];
    C[(l_n*56)+11] += A[467] * B[(l_n*56)+48];
    C[(l_n*56)+12] += A[468] * B[(l_n*56)+48];
    C[(l_n*56)+13] += A[469] * B[(l_n*56)+48];
    C[(l_n*56)+25] += A[470] * B[(l_n*56)+48];
    C[(l_n*56)+26] += A[471] * B[(l_n*56)+48];
    C[(l_n*56)+27] += A[472] * B[(l_n*56)+48];
    C[(l_n*56)+28] += A[473] * B[(l_n*56)+48];
    C[(l_n*56)+46] += A[474] * B[(l_n*56)+48];
    C[(l_n*56)+47] += A[475] * B[(l_n*56)+48];
    C[(l_n*56)+48] += A[476] * B[(l_n*56)+48];
    C[(l_n*56)+49] += A[477] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*56)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a49_0 = _mm_loadu_ps(&A[478]);
    c49_0 = _mm_add_ps(c49_0, _mm_mul_ps(a49_0, b49));
    _mm_storeu_ps(&C[(l_n*56)+10], c49_0);
    __m128 c49_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a49_4 = _mm_loadu_ps(&A[482]);
    c49_4 = _mm_add_ps(c49_4, _mm_mul_ps(a49_4, b49));
    _mm_storeu_ps(&C[(l_n*56)+25], c49_4);
    __m128 c49_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a49_8 = _mm_loadu_ps(&A[486]);
    c49_8 = _mm_add_ps(c49_8, _mm_mul_ps(a49_8, b49));
    _mm_storeu_ps(&C[(l_n*56)+46], c49_8);
#else
    C[(l_n*56)+10] += A[478] * B[(l_n*56)+49];
    C[(l_n*56)+11] += A[479] * B[(l_n*56)+49];
    C[(l_n*56)+12] += A[480] * B[(l_n*56)+49];
    C[(l_n*56)+13] += A[481] * B[(l_n*56)+49];
    C[(l_n*56)+25] += A[482] * B[(l_n*56)+49];
    C[(l_n*56)+26] += A[483] * B[(l_n*56)+49];
    C[(l_n*56)+27] += A[484] * B[(l_n*56)+49];
    C[(l_n*56)+28] += A[485] * B[(l_n*56)+49];
    C[(l_n*56)+46] += A[486] * B[(l_n*56)+49];
    C[(l_n*56)+47] += A[487] * B[(l_n*56)+49];
    C[(l_n*56)+48] += A[488] * B[(l_n*56)+49];
    C[(l_n*56)+49] += A[489] * B[(l_n*56)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a50_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[490]));
    c50_0 = _mm_add_ps(c50_0, _mm_mul_ps(a50_0, b50));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c50_0));
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a50_2 = _mm_load_ss(&A[492]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+6], c50_2);
    __m128 c50_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a50_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[493]));
    c50_3 = _mm_add_ps(c50_3, _mm_mul_ps(a50_3, b50));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c50_3));
    __m128 c50_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a50_5 = _mm_load_ss(&A[495]);
    c50_5 = _mm_add_ss(c50_5, _mm_mul_ss(a50_5, b50));
    _mm_store_ss(&C[(l_n*56)+16], c50_5);
    __m128 c50_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a50_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[496]));
    c50_6 = _mm_add_ps(c50_6, _mm_mul_ps(a50_6, b50));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c50_6));
    __m128 c50_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a50_8 = _mm_load_ss(&A[498]);
    c50_8 = _mm_add_ss(c50_8, _mm_mul_ss(a50_8, b50));
    _mm_store_ss(&C[(l_n*56)+31], c50_8);
    __m128 c50_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a50_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[499]));
    c50_9 = _mm_add_ps(c50_9, _mm_mul_ps(a50_9, b50));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c50_9));
    __m128 c50_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a50_11 = _mm_load_ss(&A[501]);
    c50_11 = _mm_add_ss(c50_11, _mm_mul_ss(a50_11, b50));
    _mm_store_ss(&C[(l_n*56)+52], c50_11);
#else
    C[(l_n*56)+4] += A[490] * B[(l_n*56)+50];
    C[(l_n*56)+5] += A[491] * B[(l_n*56)+50];
    C[(l_n*56)+6] += A[492] * B[(l_n*56)+50];
    C[(l_n*56)+14] += A[493] * B[(l_n*56)+50];
    C[(l_n*56)+15] += A[494] * B[(l_n*56)+50];
    C[(l_n*56)+16] += A[495] * B[(l_n*56)+50];
    C[(l_n*56)+29] += A[496] * B[(l_n*56)+50];
    C[(l_n*56)+30] += A[497] * B[(l_n*56)+50];
    C[(l_n*56)+31] += A[498] * B[(l_n*56)+50];
    C[(l_n*56)+50] += A[499] * B[(l_n*56)+50];
    C[(l_n*56)+51] += A[500] * B[(l_n*56)+50];
    C[(l_n*56)+52] += A[501] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a51_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[502]));
    c51_0 = _mm_add_ps(c51_0, _mm_mul_ps(a51_0, b51));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c51_0));
    __m128 c51_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a51_2 = _mm_load_ss(&A[504]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*56)+6], c51_2);
    __m128 c51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[505]));
    c51_3 = _mm_add_ps(c51_3, _mm_mul_ps(a51_3, b51));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c51_3));
    __m128 c51_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a51_5 = _mm_load_ss(&A[507]);
    c51_5 = _mm_add_ss(c51_5, _mm_mul_ss(a51_5, b51));
    _mm_store_ss(&C[(l_n*56)+16], c51_5);
    __m128 c51_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a51_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[508]));
    c51_6 = _mm_add_ps(c51_6, _mm_mul_ps(a51_6, b51));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c51_6));
    __m128 c51_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a51_8 = _mm_load_ss(&A[510]);
    c51_8 = _mm_add_ss(c51_8, _mm_mul_ss(a51_8, b51));
    _mm_store_ss(&C[(l_n*56)+31], c51_8);
    __m128 c51_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a51_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[511]));
    c51_9 = _mm_add_ps(c51_9, _mm_mul_ps(a51_9, b51));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c51_9));
    __m128 c51_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a51_11 = _mm_load_ss(&A[513]);
    c51_11 = _mm_add_ss(c51_11, _mm_mul_ss(a51_11, b51));
    _mm_store_ss(&C[(l_n*56)+52], c51_11);
#else
    C[(l_n*56)+4] += A[502] * B[(l_n*56)+51];
    C[(l_n*56)+5] += A[503] * B[(l_n*56)+51];
    C[(l_n*56)+6] += A[504] * B[(l_n*56)+51];
    C[(l_n*56)+14] += A[505] * B[(l_n*56)+51];
    C[(l_n*56)+15] += A[506] * B[(l_n*56)+51];
    C[(l_n*56)+16] += A[507] * B[(l_n*56)+51];
    C[(l_n*56)+29] += A[508] * B[(l_n*56)+51];
    C[(l_n*56)+30] += A[509] * B[(l_n*56)+51];
    C[(l_n*56)+31] += A[510] * B[(l_n*56)+51];
    C[(l_n*56)+50] += A[511] * B[(l_n*56)+51];
    C[(l_n*56)+51] += A[512] * B[(l_n*56)+51];
    C[(l_n*56)+52] += A[513] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*56)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a52_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[514]));
    c52_0 = _mm_add_ps(c52_0, _mm_mul_ps(a52_0, b52));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c52_0));
    __m128 c52_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a52_2 = _mm_load_ss(&A[516]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*56)+6], c52_2);
    __m128 c52_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a52_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[517]));
    c52_3 = _mm_add_ps(c52_3, _mm_mul_ps(a52_3, b52));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c52_3));
    __m128 c52_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a52_5 = _mm_load_ss(&A[519]);
    c52_5 = _mm_add_ss(c52_5, _mm_mul_ss(a52_5, b52));
    _mm_store_ss(&C[(l_n*56)+16], c52_5);
    __m128 c52_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a52_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[520]));
    c52_6 = _mm_add_ps(c52_6, _mm_mul_ps(a52_6, b52));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c52_6));
    __m128 c52_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a52_8 = _mm_load_ss(&A[522]);
    c52_8 = _mm_add_ss(c52_8, _mm_mul_ss(a52_8, b52));
    _mm_store_ss(&C[(l_n*56)+31], c52_8);
    __m128 c52_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a52_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[523]));
    c52_9 = _mm_add_ps(c52_9, _mm_mul_ps(a52_9, b52));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c52_9));
    __m128 c52_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a52_11 = _mm_load_ss(&A[525]);
    c52_11 = _mm_add_ss(c52_11, _mm_mul_ss(a52_11, b52));
    _mm_store_ss(&C[(l_n*56)+52], c52_11);
#else
    C[(l_n*56)+4] += A[514] * B[(l_n*56)+52];
    C[(l_n*56)+5] += A[515] * B[(l_n*56)+52];
    C[(l_n*56)+6] += A[516] * B[(l_n*56)+52];
    C[(l_n*56)+14] += A[517] * B[(l_n*56)+52];
    C[(l_n*56)+15] += A[518] * B[(l_n*56)+52];
    C[(l_n*56)+16] += A[519] * B[(l_n*56)+52];
    C[(l_n*56)+29] += A[520] * B[(l_n*56)+52];
    C[(l_n*56)+30] += A[521] * B[(l_n*56)+52];
    C[(l_n*56)+31] += A[522] * B[(l_n*56)+52];
    C[(l_n*56)+50] += A[523] * B[(l_n*56)+52];
    C[(l_n*56)+51] += A[524] * B[(l_n*56)+52];
    C[(l_n*56)+52] += A[525] * B[(l_n*56)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a53_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[526]));
    c53_0 = _mm_add_ps(c53_0, _mm_mul_ps(a53_0, b53));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c53_0));
    __m128 c53_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a53_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[528]));
    c53_2 = _mm_add_ps(c53_2, _mm_mul_ps(a53_2, b53));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c53_2));
    __m128 c53_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a53_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[530]));
    c53_4 = _mm_add_ps(c53_4, _mm_mul_ps(a53_4, b53));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c53_4));
    __m128 c53_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a53_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[532]));
    c53_6 = _mm_add_ps(c53_6, _mm_mul_ps(a53_6, b53));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c53_6));
    __m128 c53_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a53_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[534]));
    c53_8 = _mm_add_ps(c53_8, _mm_mul_ps(a53_8, b53));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c53_8));
#else
    C[(l_n*56)+1] += A[526] * B[(l_n*56)+53];
    C[(l_n*56)+2] += A[527] * B[(l_n*56)+53];
    C[(l_n*56)+7] += A[528] * B[(l_n*56)+53];
    C[(l_n*56)+8] += A[529] * B[(l_n*56)+53];
    C[(l_n*56)+17] += A[530] * B[(l_n*56)+53];
    C[(l_n*56)+18] += A[531] * B[(l_n*56)+53];
    C[(l_n*56)+32] += A[532] * B[(l_n*56)+53];
    C[(l_n*56)+33] += A[533] * B[(l_n*56)+53];
    C[(l_n*56)+53] += A[534] * B[(l_n*56)+53];
    C[(l_n*56)+54] += A[535] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*56)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a54_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[536]));
    c54_0 = _mm_add_ps(c54_0, _mm_mul_ps(a54_0, b54));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c54_0));
    __m128 c54_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a54_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[538]));
    c54_2 = _mm_add_ps(c54_2, _mm_mul_ps(a54_2, b54));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c54_2));
    __m128 c54_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a54_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[540]));
    c54_4 = _mm_add_ps(c54_4, _mm_mul_ps(a54_4, b54));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c54_4));
    __m128 c54_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a54_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[542]));
    c54_6 = _mm_add_ps(c54_6, _mm_mul_ps(a54_6, b54));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c54_6));
    __m128 c54_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a54_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[544]));
    c54_8 = _mm_add_ps(c54_8, _mm_mul_ps(a54_8, b54));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c54_8));
#else
    C[(l_n*56)+1] += A[536] * B[(l_n*56)+54];
    C[(l_n*56)+2] += A[537] * B[(l_n*56)+54];
    C[(l_n*56)+7] += A[538] * B[(l_n*56)+54];
    C[(l_n*56)+8] += A[539] * B[(l_n*56)+54];
    C[(l_n*56)+17] += A[540] * B[(l_n*56)+54];
    C[(l_n*56)+18] += A[541] * B[(l_n*56)+54];
    C[(l_n*56)+32] += A[542] * B[(l_n*56)+54];
    C[(l_n*56)+33] += A[543] * B[(l_n*56)+54];
    C[(l_n*56)+53] += A[544] * B[(l_n*56)+54];
    C[(l_n*56)+54] += A[545] * B[(l_n*56)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*56)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a55_0 = _mm_load_ss(&A[546]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*56)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a55_1 = _mm_load_ss(&A[547]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*56)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a55_2 = _mm_load_ss(&A[548]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*56)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a55_3 = _mm_load_ss(&A[549]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*56)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a55_4 = _mm_load_ss(&A[550]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*56)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a55_5 = _mm_load_ss(&A[551]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*56)+55], c55_5);
#else
    C[(l_n*56)+0] += A[546] * B[(l_n*56)+55];
    C[(l_n*56)+3] += A[547] * B[(l_n*56)+55];
    C[(l_n*56)+9] += A[548] * B[(l_n*56)+55];
    C[(l_n*56)+19] += A[549] * B[(l_n*56)+55];
    C[(l_n*56)+34] += A[550] * B[(l_n*56)+55];
    C[(l_n*56)+55] += A[551] * B[(l_n*56)+55];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 9936;
#endif
}

void ssparse_fP112DivM_m56_n9_k56_ldAna6_ldB56_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*56)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*56)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*56)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*56)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*56)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*56)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*56)+55], c0_5);
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*56)+0];
    C[(l_n*56)+3] += A[1] * B[(l_n*56)+0];
    C[(l_n*56)+9] += A[2] * B[(l_n*56)+0];
    C[(l_n*56)+19] += A[3] * B[(l_n*56)+0];
    C[(l_n*56)+34] += A[4] * B[(l_n*56)+0];
    C[(l_n*56)+55] += A[5] * B[(l_n*56)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a1_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[6]));
    c1_0 = _mm_add_ps(c1_0, _mm_mul_ps(a1_0, b1));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c1_0));
    __m128 c1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a1_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c1_2 = _mm_add_ps(c1_2, _mm_mul_ps(a1_2, b1));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c1_2));
    __m128 c1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a1_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[10]));
    c1_4 = _mm_add_ps(c1_4, _mm_mul_ps(a1_4, b1));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c1_4));
    __m128 c1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a1_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[12]));
    c1_6 = _mm_add_ps(c1_6, _mm_mul_ps(a1_6, b1));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c1_6));
    __m128 c1_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a1_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[14]));
    c1_8 = _mm_add_ps(c1_8, _mm_mul_ps(a1_8, b1));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c1_8));
#else
    C[(l_n*56)+1] += A[6] * B[(l_n*56)+1];
    C[(l_n*56)+2] += A[7] * B[(l_n*56)+1];
    C[(l_n*56)+7] += A[8] * B[(l_n*56)+1];
    C[(l_n*56)+8] += A[9] * B[(l_n*56)+1];
    C[(l_n*56)+17] += A[10] * B[(l_n*56)+1];
    C[(l_n*56)+18] += A[11] * B[(l_n*56)+1];
    C[(l_n*56)+32] += A[12] * B[(l_n*56)+1];
    C[(l_n*56)+33] += A[13] * B[(l_n*56)+1];
    C[(l_n*56)+53] += A[14] * B[(l_n*56)+1];
    C[(l_n*56)+54] += A[15] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*56)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a2_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[16]));
    c2_0 = _mm_add_ps(c2_0, _mm_mul_ps(a2_0, b2));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c2_0));
    __m128 c2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a2_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[18]));
    c2_2 = _mm_add_ps(c2_2, _mm_mul_ps(a2_2, b2));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c2_2));
    __m128 c2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a2_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[20]));
    c2_4 = _mm_add_ps(c2_4, _mm_mul_ps(a2_4, b2));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c2_4));
    __m128 c2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a2_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[22]));
    c2_6 = _mm_add_ps(c2_6, _mm_mul_ps(a2_6, b2));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c2_6));
    __m128 c2_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a2_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[24]));
    c2_8 = _mm_add_ps(c2_8, _mm_mul_ps(a2_8, b2));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c2_8));
#else
    C[(l_n*56)+1] += A[16] * B[(l_n*56)+2];
    C[(l_n*56)+2] += A[17] * B[(l_n*56)+2];
    C[(l_n*56)+7] += A[18] * B[(l_n*56)+2];
    C[(l_n*56)+8] += A[19] * B[(l_n*56)+2];
    C[(l_n*56)+17] += A[20] * B[(l_n*56)+2];
    C[(l_n*56)+18] += A[21] * B[(l_n*56)+2];
    C[(l_n*56)+32] += A[22] * B[(l_n*56)+2];
    C[(l_n*56)+33] += A[23] * B[(l_n*56)+2];
    C[(l_n*56)+53] += A[24] * B[(l_n*56)+2];
    C[(l_n*56)+54] += A[25] * B[(l_n*56)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*56)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a3_0 = _mm_load_ss(&A[26]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*56)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a3_1 = _mm_load_ss(&A[27]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*56)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a3_2 = _mm_load_ss(&A[28]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*56)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a3_3 = _mm_load_ss(&A[29]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*56)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a3_4 = _mm_load_ss(&A[30]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*56)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a3_5 = _mm_load_ss(&A[31]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*56)+55], c3_5);
#else
    C[(l_n*56)+0] += A[26] * B[(l_n*56)+3];
    C[(l_n*56)+3] += A[27] * B[(l_n*56)+3];
    C[(l_n*56)+9] += A[28] * B[(l_n*56)+3];
    C[(l_n*56)+19] += A[29] * B[(l_n*56)+3];
    C[(l_n*56)+34] += A[30] * B[(l_n*56)+3];
    C[(l_n*56)+55] += A[31] * B[(l_n*56)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a4_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[32]));
    c4_0 = _mm_add_ps(c4_0, _mm_mul_ps(a4_0, b4));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c4_0));
    __m128 c4_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a4_2 = _mm_load_ss(&A[34]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*56)+6], c4_2);
    __m128 c4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a4_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[35]));
    c4_3 = _mm_add_ps(c4_3, _mm_mul_ps(a4_3, b4));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c4_3));
    __m128 c4_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a4_5 = _mm_load_ss(&A[37]);
    c4_5 = _mm_add_ss(c4_5, _mm_mul_ss(a4_5, b4));
    _mm_store_ss(&C[(l_n*56)+16], c4_5);
    __m128 c4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a4_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[38]));
    c4_6 = _mm_add_ps(c4_6, _mm_mul_ps(a4_6, b4));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c4_6));
    __m128 c4_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a4_8 = _mm_load_ss(&A[40]);
    c4_8 = _mm_add_ss(c4_8, _mm_mul_ss(a4_8, b4));
    _mm_store_ss(&C[(l_n*56)+31], c4_8);
    __m128 c4_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a4_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[41]));
    c4_9 = _mm_add_ps(c4_9, _mm_mul_ps(a4_9, b4));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c4_9));
    __m128 c4_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a4_11 = _mm_load_ss(&A[43]);
    c4_11 = _mm_add_ss(c4_11, _mm_mul_ss(a4_11, b4));
    _mm_store_ss(&C[(l_n*56)+52], c4_11);
#else
    C[(l_n*56)+4] += A[32] * B[(l_n*56)+4];
    C[(l_n*56)+5] += A[33] * B[(l_n*56)+4];
    C[(l_n*56)+6] += A[34] * B[(l_n*56)+4];
    C[(l_n*56)+14] += A[35] * B[(l_n*56)+4];
    C[(l_n*56)+15] += A[36] * B[(l_n*56)+4];
    C[(l_n*56)+16] += A[37] * B[(l_n*56)+4];
    C[(l_n*56)+29] += A[38] * B[(l_n*56)+4];
    C[(l_n*56)+30] += A[39] * B[(l_n*56)+4];
    C[(l_n*56)+31] += A[40] * B[(l_n*56)+4];
    C[(l_n*56)+50] += A[41] * B[(l_n*56)+4];
    C[(l_n*56)+51] += A[42] * B[(l_n*56)+4];
    C[(l_n*56)+52] += A[43] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a5_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[44]));
    c5_0 = _mm_add_ps(c5_0, _mm_mul_ps(a5_0, b5));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c5_0));
    __m128 c5_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a5_2 = _mm_load_ss(&A[46]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*56)+6], c5_2);
    __m128 c5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a5_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[47]));
    c5_3 = _mm_add_ps(c5_3, _mm_mul_ps(a5_3, b5));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c5_3));
    __m128 c5_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a5_5 = _mm_load_ss(&A[49]);
    c5_5 = _mm_add_ss(c5_5, _mm_mul_ss(a5_5, b5));
    _mm_store_ss(&C[(l_n*56)+16], c5_5);
    __m128 c5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a5_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[50]));
    c5_6 = _mm_add_ps(c5_6, _mm_mul_ps(a5_6, b5));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c5_6));
    __m128 c5_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a5_8 = _mm_load_ss(&A[52]);
    c5_8 = _mm_add_ss(c5_8, _mm_mul_ss(a5_8, b5));
    _mm_store_ss(&C[(l_n*56)+31], c5_8);
    __m128 c5_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a5_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[53]));
    c5_9 = _mm_add_ps(c5_9, _mm_mul_ps(a5_9, b5));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c5_9));
    __m128 c5_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a5_11 = _mm_load_ss(&A[55]);
    c5_11 = _mm_add_ss(c5_11, _mm_mul_ss(a5_11, b5));
    _mm_store_ss(&C[(l_n*56)+52], c5_11);
#else
    C[(l_n*56)+4] += A[44] * B[(l_n*56)+5];
    C[(l_n*56)+5] += A[45] * B[(l_n*56)+5];
    C[(l_n*56)+6] += A[46] * B[(l_n*56)+5];
    C[(l_n*56)+14] += A[47] * B[(l_n*56)+5];
    C[(l_n*56)+15] += A[48] * B[(l_n*56)+5];
    C[(l_n*56)+16] += A[49] * B[(l_n*56)+5];
    C[(l_n*56)+29] += A[50] * B[(l_n*56)+5];
    C[(l_n*56)+30] += A[51] * B[(l_n*56)+5];
    C[(l_n*56)+31] += A[52] * B[(l_n*56)+5];
    C[(l_n*56)+50] += A[53] * B[(l_n*56)+5];
    C[(l_n*56)+51] += A[54] * B[(l_n*56)+5];
    C[(l_n*56)+52] += A[55] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*56)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a6_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[56]));
    c6_0 = _mm_add_ps(c6_0, _mm_mul_ps(a6_0, b6));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c6_0));
    __m128 c6_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a6_2 = _mm_load_ss(&A[58]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*56)+6], c6_2);
    __m128 c6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a6_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[59]));
    c6_3 = _mm_add_ps(c6_3, _mm_mul_ps(a6_3, b6));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c6_3));
    __m128 c6_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a6_5 = _mm_load_ss(&A[61]);
    c6_5 = _mm_add_ss(c6_5, _mm_mul_ss(a6_5, b6));
    _mm_store_ss(&C[(l_n*56)+16], c6_5);
    __m128 c6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a6_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[62]));
    c6_6 = _mm_add_ps(c6_6, _mm_mul_ps(a6_6, b6));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c6_6));
    __m128 c6_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a6_8 = _mm_load_ss(&A[64]);
    c6_8 = _mm_add_ss(c6_8, _mm_mul_ss(a6_8, b6));
    _mm_store_ss(&C[(l_n*56)+31], c6_8);
    __m128 c6_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a6_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[65]));
    c6_9 = _mm_add_ps(c6_9, _mm_mul_ps(a6_9, b6));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c6_9));
    __m128 c6_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a6_11 = _mm_load_ss(&A[67]);
    c6_11 = _mm_add_ss(c6_11, _mm_mul_ss(a6_11, b6));
    _mm_store_ss(&C[(l_n*56)+52], c6_11);
#else
    C[(l_n*56)+4] += A[56] * B[(l_n*56)+6];
    C[(l_n*56)+5] += A[57] * B[(l_n*56)+6];
    C[(l_n*56)+6] += A[58] * B[(l_n*56)+6];
    C[(l_n*56)+14] += A[59] * B[(l_n*56)+6];
    C[(l_n*56)+15] += A[60] * B[(l_n*56)+6];
    C[(l_n*56)+16] += A[61] * B[(l_n*56)+6];
    C[(l_n*56)+29] += A[62] * B[(l_n*56)+6];
    C[(l_n*56)+30] += A[63] * B[(l_n*56)+6];
    C[(l_n*56)+31] += A[64] * B[(l_n*56)+6];
    C[(l_n*56)+50] += A[65] * B[(l_n*56)+6];
    C[(l_n*56)+51] += A[66] * B[(l_n*56)+6];
    C[(l_n*56)+52] += A[67] * B[(l_n*56)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a7_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[68]));
    c7_0 = _mm_add_ps(c7_0, _mm_mul_ps(a7_0, b7));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c7_0));
    __m128 c7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a7_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c7_2 = _mm_add_ps(c7_2, _mm_mul_ps(a7_2, b7));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c7_2));
    __m128 c7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a7_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[72]));
    c7_4 = _mm_add_ps(c7_4, _mm_mul_ps(a7_4, b7));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c7_4));
    __m128 c7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a7_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[74]));
    c7_6 = _mm_add_ps(c7_6, _mm_mul_ps(a7_6, b7));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c7_6));
    __m128 c7_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a7_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[76]));
    c7_8 = _mm_add_ps(c7_8, _mm_mul_ps(a7_8, b7));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c7_8));
#else
    C[(l_n*56)+1] += A[68] * B[(l_n*56)+7];
    C[(l_n*56)+2] += A[69] * B[(l_n*56)+7];
    C[(l_n*56)+7] += A[70] * B[(l_n*56)+7];
    C[(l_n*56)+8] += A[71] * B[(l_n*56)+7];
    C[(l_n*56)+17] += A[72] * B[(l_n*56)+7];
    C[(l_n*56)+18] += A[73] * B[(l_n*56)+7];
    C[(l_n*56)+32] += A[74] * B[(l_n*56)+7];
    C[(l_n*56)+33] += A[75] * B[(l_n*56)+7];
    C[(l_n*56)+53] += A[76] * B[(l_n*56)+7];
    C[(l_n*56)+54] += A[77] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*56)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a8_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[78]));
    c8_0 = _mm_add_ps(c8_0, _mm_mul_ps(a8_0, b8));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c8_0));
    __m128 c8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a8_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[80]));
    c8_2 = _mm_add_ps(c8_2, _mm_mul_ps(a8_2, b8));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c8_2));
    __m128 c8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a8_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[82]));
    c8_4 = _mm_add_ps(c8_4, _mm_mul_ps(a8_4, b8));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c8_4));
    __m128 c8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a8_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[84]));
    c8_6 = _mm_add_ps(c8_6, _mm_mul_ps(a8_6, b8));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c8_6));
    __m128 c8_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a8_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[86]));
    c8_8 = _mm_add_ps(c8_8, _mm_mul_ps(a8_8, b8));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c8_8));
#else
    C[(l_n*56)+1] += A[78] * B[(l_n*56)+8];
    C[(l_n*56)+2] += A[79] * B[(l_n*56)+8];
    C[(l_n*56)+7] += A[80] * B[(l_n*56)+8];
    C[(l_n*56)+8] += A[81] * B[(l_n*56)+8];
    C[(l_n*56)+17] += A[82] * B[(l_n*56)+8];
    C[(l_n*56)+18] += A[83] * B[(l_n*56)+8];
    C[(l_n*56)+32] += A[84] * B[(l_n*56)+8];
    C[(l_n*56)+33] += A[85] * B[(l_n*56)+8];
    C[(l_n*56)+53] += A[86] * B[(l_n*56)+8];
    C[(l_n*56)+54] += A[87] * B[(l_n*56)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*56)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a9_0 = _mm_load_ss(&A[88]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*56)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a9_1 = _mm_load_ss(&A[89]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*56)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a9_2 = _mm_load_ss(&A[90]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*56)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a9_3 = _mm_load_ss(&A[91]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*56)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a9_4 = _mm_load_ss(&A[92]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*56)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a9_5 = _mm_load_ss(&A[93]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*56)+55], c9_5);
#else
    C[(l_n*56)+0] += A[88] * B[(l_n*56)+9];
    C[(l_n*56)+3] += A[89] * B[(l_n*56)+9];
    C[(l_n*56)+9] += A[90] * B[(l_n*56)+9];
    C[(l_n*56)+19] += A[91] * B[(l_n*56)+9];
    C[(l_n*56)+34] += A[92] * B[(l_n*56)+9];
    C[(l_n*56)+55] += A[93] * B[(l_n*56)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a10_0 = _mm_loadu_ps(&A[94]);
    c10_0 = _mm_add_ps(c10_0, _mm_mul_ps(a10_0, b10));
    _mm_storeu_ps(&C[(l_n*56)+10], c10_0);
    __m128 c10_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a10_4 = _mm_loadu_ps(&A[98]);
    c10_4 = _mm_add_ps(c10_4, _mm_mul_ps(a10_4, b10));
    _mm_storeu_ps(&C[(l_n*56)+25], c10_4);
    __m128 c10_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a10_8 = _mm_loadu_ps(&A[102]);
    c10_8 = _mm_add_ps(c10_8, _mm_mul_ps(a10_8, b10));
    _mm_storeu_ps(&C[(l_n*56)+46], c10_8);
#else
    C[(l_n*56)+10] += A[94] * B[(l_n*56)+10];
    C[(l_n*56)+11] += A[95] * B[(l_n*56)+10];
    C[(l_n*56)+12] += A[96] * B[(l_n*56)+10];
    C[(l_n*56)+13] += A[97] * B[(l_n*56)+10];
    C[(l_n*56)+25] += A[98] * B[(l_n*56)+10];
    C[(l_n*56)+26] += A[99] * B[(l_n*56)+10];
    C[(l_n*56)+27] += A[100] * B[(l_n*56)+10];
    C[(l_n*56)+28] += A[101] * B[(l_n*56)+10];
    C[(l_n*56)+46] += A[102] * B[(l_n*56)+10];
    C[(l_n*56)+47] += A[103] * B[(l_n*56)+10];
    C[(l_n*56)+48] += A[104] * B[(l_n*56)+10];
    C[(l_n*56)+49] += A[105] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a11_0 = _mm_loadu_ps(&A[106]);
    c11_0 = _mm_add_ps(c11_0, _mm_mul_ps(a11_0, b11));
    _mm_storeu_ps(&C[(l_n*56)+10], c11_0);
    __m128 c11_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a11_4 = _mm_loadu_ps(&A[110]);
    c11_4 = _mm_add_ps(c11_4, _mm_mul_ps(a11_4, b11));
    _mm_storeu_ps(&C[(l_n*56)+25], c11_4);
    __m128 c11_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a11_8 = _mm_loadu_ps(&A[114]);
    c11_8 = _mm_add_ps(c11_8, _mm_mul_ps(a11_8, b11));
    _mm_storeu_ps(&C[(l_n*56)+46], c11_8);
#else
    C[(l_n*56)+10] += A[106] * B[(l_n*56)+11];
    C[(l_n*56)+11] += A[107] * B[(l_n*56)+11];
    C[(l_n*56)+12] += A[108] * B[(l_n*56)+11];
    C[(l_n*56)+13] += A[109] * B[(l_n*56)+11];
    C[(l_n*56)+25] += A[110] * B[(l_n*56)+11];
    C[(l_n*56)+26] += A[111] * B[(l_n*56)+11];
    C[(l_n*56)+27] += A[112] * B[(l_n*56)+11];
    C[(l_n*56)+28] += A[113] * B[(l_n*56)+11];
    C[(l_n*56)+46] += A[114] * B[(l_n*56)+11];
    C[(l_n*56)+47] += A[115] * B[(l_n*56)+11];
    C[(l_n*56)+48] += A[116] * B[(l_n*56)+11];
    C[(l_n*56)+49] += A[117] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a12_0 = _mm_loadu_ps(&A[118]);
    c12_0 = _mm_add_ps(c12_0, _mm_mul_ps(a12_0, b12));
    _mm_storeu_ps(&C[(l_n*56)+10], c12_0);
    __m128 c12_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a12_4 = _mm_loadu_ps(&A[122]);
    c12_4 = _mm_add_ps(c12_4, _mm_mul_ps(a12_4, b12));
    _mm_storeu_ps(&C[(l_n*56)+25], c12_4);
    __m128 c12_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a12_8 = _mm_loadu_ps(&A[126]);
    c12_8 = _mm_add_ps(c12_8, _mm_mul_ps(a12_8, b12));
    _mm_storeu_ps(&C[(l_n*56)+46], c12_8);
#else
    C[(l_n*56)+10] += A[118] * B[(l_n*56)+12];
    C[(l_n*56)+11] += A[119] * B[(l_n*56)+12];
    C[(l_n*56)+12] += A[120] * B[(l_n*56)+12];
    C[(l_n*56)+13] += A[121] * B[(l_n*56)+12];
    C[(l_n*56)+25] += A[122] * B[(l_n*56)+12];
    C[(l_n*56)+26] += A[123] * B[(l_n*56)+12];
    C[(l_n*56)+27] += A[124] * B[(l_n*56)+12];
    C[(l_n*56)+28] += A[125] * B[(l_n*56)+12];
    C[(l_n*56)+46] += A[126] * B[(l_n*56)+12];
    C[(l_n*56)+47] += A[127] * B[(l_n*56)+12];
    C[(l_n*56)+48] += A[128] * B[(l_n*56)+12];
    C[(l_n*56)+49] += A[129] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*56)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a13_0 = _mm_loadu_ps(&A[130]);
    c13_0 = _mm_add_ps(c13_0, _mm_mul_ps(a13_0, b13));
    _mm_storeu_ps(&C[(l_n*56)+10], c13_0);
    __m128 c13_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a13_4 = _mm_loadu_ps(&A[134]);
    c13_4 = _mm_add_ps(c13_4, _mm_mul_ps(a13_4, b13));
    _mm_storeu_ps(&C[(l_n*56)+25], c13_4);
    __m128 c13_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a13_8 = _mm_loadu_ps(&A[138]);
    c13_8 = _mm_add_ps(c13_8, _mm_mul_ps(a13_8, b13));
    _mm_storeu_ps(&C[(l_n*56)+46], c13_8);
#else
    C[(l_n*56)+10] += A[130] * B[(l_n*56)+13];
    C[(l_n*56)+11] += A[131] * B[(l_n*56)+13];
    C[(l_n*56)+12] += A[132] * B[(l_n*56)+13];
    C[(l_n*56)+13] += A[133] * B[(l_n*56)+13];
    C[(l_n*56)+25] += A[134] * B[(l_n*56)+13];
    C[(l_n*56)+26] += A[135] * B[(l_n*56)+13];
    C[(l_n*56)+27] += A[136] * B[(l_n*56)+13];
    C[(l_n*56)+28] += A[137] * B[(l_n*56)+13];
    C[(l_n*56)+46] += A[138] * B[(l_n*56)+13];
    C[(l_n*56)+47] += A[139] * B[(l_n*56)+13];
    C[(l_n*56)+48] += A[140] * B[(l_n*56)+13];
    C[(l_n*56)+49] += A[141] * B[(l_n*56)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a14_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[142]));
    c14_0 = _mm_add_ps(c14_0, _mm_mul_ps(a14_0, b14));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c14_0));
    __m128 c14_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a14_2 = _mm_load_ss(&A[144]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*56)+6], c14_2);
    __m128 c14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a14_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[145]));
    c14_3 = _mm_add_ps(c14_3, _mm_mul_ps(a14_3, b14));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c14_3));
    __m128 c14_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a14_5 = _mm_load_ss(&A[147]);
    c14_5 = _mm_add_ss(c14_5, _mm_mul_ss(a14_5, b14));
    _mm_store_ss(&C[(l_n*56)+16], c14_5);
    __m128 c14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a14_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[148]));
    c14_6 = _mm_add_ps(c14_6, _mm_mul_ps(a14_6, b14));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c14_6));
    __m128 c14_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a14_8 = _mm_load_ss(&A[150]);
    c14_8 = _mm_add_ss(c14_8, _mm_mul_ss(a14_8, b14));
    _mm_store_ss(&C[(l_n*56)+31], c14_8);
    __m128 c14_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a14_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[151]));
    c14_9 = _mm_add_ps(c14_9, _mm_mul_ps(a14_9, b14));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c14_9));
    __m128 c14_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a14_11 = _mm_load_ss(&A[153]);
    c14_11 = _mm_add_ss(c14_11, _mm_mul_ss(a14_11, b14));
    _mm_store_ss(&C[(l_n*56)+52], c14_11);
#else
    C[(l_n*56)+4] += A[142] * B[(l_n*56)+14];
    C[(l_n*56)+5] += A[143] * B[(l_n*56)+14];
    C[(l_n*56)+6] += A[144] * B[(l_n*56)+14];
    C[(l_n*56)+14] += A[145] * B[(l_n*56)+14];
    C[(l_n*56)+15] += A[146] * B[(l_n*56)+14];
    C[(l_n*56)+16] += A[147] * B[(l_n*56)+14];
    C[(l_n*56)+29] += A[148] * B[(l_n*56)+14];
    C[(l_n*56)+30] += A[149] * B[(l_n*56)+14];
    C[(l_n*56)+31] += A[150] * B[(l_n*56)+14];
    C[(l_n*56)+50] += A[151] * B[(l_n*56)+14];
    C[(l_n*56)+51] += A[152] * B[(l_n*56)+14];
    C[(l_n*56)+52] += A[153] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a15_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[154]));
    c15_0 = _mm_add_ps(c15_0, _mm_mul_ps(a15_0, b15));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c15_0));
    __m128 c15_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a15_2 = _mm_load_ss(&A[156]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*56)+6], c15_2);
    __m128 c15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a15_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[157]));
    c15_3 = _mm_add_ps(c15_3, _mm_mul_ps(a15_3, b15));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c15_3));
    __m128 c15_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a15_5 = _mm_load_ss(&A[159]);
    c15_5 = _mm_add_ss(c15_5, _mm_mul_ss(a15_5, b15));
    _mm_store_ss(&C[(l_n*56)+16], c15_5);
    __m128 c15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a15_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[160]));
    c15_6 = _mm_add_ps(c15_6, _mm_mul_ps(a15_6, b15));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c15_6));
    __m128 c15_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a15_8 = _mm_load_ss(&A[162]);
    c15_8 = _mm_add_ss(c15_8, _mm_mul_ss(a15_8, b15));
    _mm_store_ss(&C[(l_n*56)+31], c15_8);
    __m128 c15_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a15_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[163]));
    c15_9 = _mm_add_ps(c15_9, _mm_mul_ps(a15_9, b15));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c15_9));
    __m128 c15_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a15_11 = _mm_load_ss(&A[165]);
    c15_11 = _mm_add_ss(c15_11, _mm_mul_ss(a15_11, b15));
    _mm_store_ss(&C[(l_n*56)+52], c15_11);
#else
    C[(l_n*56)+4] += A[154] * B[(l_n*56)+15];
    C[(l_n*56)+5] += A[155] * B[(l_n*56)+15];
    C[(l_n*56)+6] += A[156] * B[(l_n*56)+15];
    C[(l_n*56)+14] += A[157] * B[(l_n*56)+15];
    C[(l_n*56)+15] += A[158] * B[(l_n*56)+15];
    C[(l_n*56)+16] += A[159] * B[(l_n*56)+15];
    C[(l_n*56)+29] += A[160] * B[(l_n*56)+15];
    C[(l_n*56)+30] += A[161] * B[(l_n*56)+15];
    C[(l_n*56)+31] += A[162] * B[(l_n*56)+15];
    C[(l_n*56)+50] += A[163] * B[(l_n*56)+15];
    C[(l_n*56)+51] += A[164] * B[(l_n*56)+15];
    C[(l_n*56)+52] += A[165] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*56)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a16_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[166]));
    c16_0 = _mm_add_ps(c16_0, _mm_mul_ps(a16_0, b16));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c16_0));
    __m128 c16_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a16_2 = _mm_load_ss(&A[168]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*56)+6], c16_2);
    __m128 c16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a16_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[169]));
    c16_3 = _mm_add_ps(c16_3, _mm_mul_ps(a16_3, b16));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c16_3));
    __m128 c16_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a16_5 = _mm_load_ss(&A[171]);
    c16_5 = _mm_add_ss(c16_5, _mm_mul_ss(a16_5, b16));
    _mm_store_ss(&C[(l_n*56)+16], c16_5);
    __m128 c16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a16_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[172]));
    c16_6 = _mm_add_ps(c16_6, _mm_mul_ps(a16_6, b16));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c16_6));
    __m128 c16_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a16_8 = _mm_load_ss(&A[174]);
    c16_8 = _mm_add_ss(c16_8, _mm_mul_ss(a16_8, b16));
    _mm_store_ss(&C[(l_n*56)+31], c16_8);
    __m128 c16_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a16_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[175]));
    c16_9 = _mm_add_ps(c16_9, _mm_mul_ps(a16_9, b16));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c16_9));
    __m128 c16_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a16_11 = _mm_load_ss(&A[177]);
    c16_11 = _mm_add_ss(c16_11, _mm_mul_ss(a16_11, b16));
    _mm_store_ss(&C[(l_n*56)+52], c16_11);
#else
    C[(l_n*56)+4] += A[166] * B[(l_n*56)+16];
    C[(l_n*56)+5] += A[167] * B[(l_n*56)+16];
    C[(l_n*56)+6] += A[168] * B[(l_n*56)+16];
    C[(l_n*56)+14] += A[169] * B[(l_n*56)+16];
    C[(l_n*56)+15] += A[170] * B[(l_n*56)+16];
    C[(l_n*56)+16] += A[171] * B[(l_n*56)+16];
    C[(l_n*56)+29] += A[172] * B[(l_n*56)+16];
    C[(l_n*56)+30] += A[173] * B[(l_n*56)+16];
    C[(l_n*56)+31] += A[174] * B[(l_n*56)+16];
    C[(l_n*56)+50] += A[175] * B[(l_n*56)+16];
    C[(l_n*56)+51] += A[176] * B[(l_n*56)+16];
    C[(l_n*56)+52] += A[177] * B[(l_n*56)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a17_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[178]));
    c17_0 = _mm_add_ps(c17_0, _mm_mul_ps(a17_0, b17));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c17_0));
    __m128 c17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a17_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[180]));
    c17_2 = _mm_add_ps(c17_2, _mm_mul_ps(a17_2, b17));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c17_2));
    __m128 c17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a17_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[182]));
    c17_4 = _mm_add_ps(c17_4, _mm_mul_ps(a17_4, b17));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c17_4));
    __m128 c17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a17_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[184]));
    c17_6 = _mm_add_ps(c17_6, _mm_mul_ps(a17_6, b17));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c17_6));
    __m128 c17_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a17_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[186]));
    c17_8 = _mm_add_ps(c17_8, _mm_mul_ps(a17_8, b17));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c17_8));
#else
    C[(l_n*56)+1] += A[178] * B[(l_n*56)+17];
    C[(l_n*56)+2] += A[179] * B[(l_n*56)+17];
    C[(l_n*56)+7] += A[180] * B[(l_n*56)+17];
    C[(l_n*56)+8] += A[181] * B[(l_n*56)+17];
    C[(l_n*56)+17] += A[182] * B[(l_n*56)+17];
    C[(l_n*56)+18] += A[183] * B[(l_n*56)+17];
    C[(l_n*56)+32] += A[184] * B[(l_n*56)+17];
    C[(l_n*56)+33] += A[185] * B[(l_n*56)+17];
    C[(l_n*56)+53] += A[186] * B[(l_n*56)+17];
    C[(l_n*56)+54] += A[187] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*56)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a18_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[188]));
    c18_0 = _mm_add_ps(c18_0, _mm_mul_ps(a18_0, b18));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c18_0));
    __m128 c18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a18_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[190]));
    c18_2 = _mm_add_ps(c18_2, _mm_mul_ps(a18_2, b18));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c18_2));
    __m128 c18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a18_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[192]));
    c18_4 = _mm_add_ps(c18_4, _mm_mul_ps(a18_4, b18));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c18_4));
    __m128 c18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a18_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[194]));
    c18_6 = _mm_add_ps(c18_6, _mm_mul_ps(a18_6, b18));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c18_6));
    __m128 c18_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a18_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[196]));
    c18_8 = _mm_add_ps(c18_8, _mm_mul_ps(a18_8, b18));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c18_8));
#else
    C[(l_n*56)+1] += A[188] * B[(l_n*56)+18];
    C[(l_n*56)+2] += A[189] * B[(l_n*56)+18];
    C[(l_n*56)+7] += A[190] * B[(l_n*56)+18];
    C[(l_n*56)+8] += A[191] * B[(l_n*56)+18];
    C[(l_n*56)+17] += A[192] * B[(l_n*56)+18];
    C[(l_n*56)+18] += A[193] * B[(l_n*56)+18];
    C[(l_n*56)+32] += A[194] * B[(l_n*56)+18];
    C[(l_n*56)+33] += A[195] * B[(l_n*56)+18];
    C[(l_n*56)+53] += A[196] * B[(l_n*56)+18];
    C[(l_n*56)+54] += A[197] * B[(l_n*56)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*56)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a19_0 = _mm_load_ss(&A[198]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*56)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a19_1 = _mm_load_ss(&A[199]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*56)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a19_2 = _mm_load_ss(&A[200]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*56)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a19_3 = _mm_load_ss(&A[201]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*56)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a19_4 = _mm_load_ss(&A[202]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*56)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a19_5 = _mm_load_ss(&A[203]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*56)+55], c19_5);
#else
    C[(l_n*56)+0] += A[198] * B[(l_n*56)+19];
    C[(l_n*56)+3] += A[199] * B[(l_n*56)+19];
    C[(l_n*56)+9] += A[200] * B[(l_n*56)+19];
    C[(l_n*56)+19] += A[201] * B[(l_n*56)+19];
    C[(l_n*56)+34] += A[202] * B[(l_n*56)+19];
    C[(l_n*56)+55] += A[203] * B[(l_n*56)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a20_0 = _mm_loadu_ps(&A[204]);
    c20_0 = _mm_add_ps(c20_0, _mm_mul_ps(a20_0, b20));
    _mm_storeu_ps(&C[(l_n*56)+20], c20_0);
    __m128 c20_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a20_4 = _mm_load_ss(&A[208]);
    c20_4 = _mm_add_ss(c20_4, _mm_mul_ss(a20_4, b20));
    _mm_store_ss(&C[(l_n*56)+24], c20_4);
    __m128 c20_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a20_5 = _mm_loadu_ps(&A[209]);
    c20_5 = _mm_add_ps(c20_5, _mm_mul_ps(a20_5, b20));
    _mm_storeu_ps(&C[(l_n*56)+41], c20_5);
    __m128 c20_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a20_9 = _mm_load_ss(&A[213]);
    c20_9 = _mm_add_ss(c20_9, _mm_mul_ss(a20_9, b20));
    _mm_store_ss(&C[(l_n*56)+45], c20_9);
#else
    C[(l_n*56)+20] += A[204] * B[(l_n*56)+20];
    C[(l_n*56)+21] += A[205] * B[(l_n*56)+20];
    C[(l_n*56)+22] += A[206] * B[(l_n*56)+20];
    C[(l_n*56)+23] += A[207] * B[(l_n*56)+20];
    C[(l_n*56)+24] += A[208] * B[(l_n*56)+20];
    C[(l_n*56)+41] += A[209] * B[(l_n*56)+20];
    C[(l_n*56)+42] += A[210] * B[(l_n*56)+20];
    C[(l_n*56)+43] += A[211] * B[(l_n*56)+20];
    C[(l_n*56)+44] += A[212] * B[(l_n*56)+20];
    C[(l_n*56)+45] += A[213] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+20]));
    __m128 a21_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[214]));
    c21_0 = _mm_add_ps(c21_0, _mm_mul_ps(a21_0, b21));
    _mm_store_sd((double*)&C[(l_n*56)+20], _mm_castps_pd(c21_0));
    __m128 c21_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a21_2 = _mm_load_ss(&A[216]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*56)+22], c21_2);
    __m128 c21_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a21_3 = _mm_load_ss(&A[217]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*56)+24], c21_3);
    __m128 c21_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+41]));
    __m128 a21_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[218]));
    c21_4 = _mm_add_ps(c21_4, _mm_mul_ps(a21_4, b21));
    _mm_store_sd((double*)&C[(l_n*56)+41], _mm_castps_pd(c21_4));
    __m128 c21_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a21_6 = _mm_load_ss(&A[220]);
    c21_6 = _mm_add_ss(c21_6, _mm_mul_ss(a21_6, b21));
    _mm_store_ss(&C[(l_n*56)+43], c21_6);
    __m128 c21_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a21_7 = _mm_load_ss(&A[221]);
    c21_7 = _mm_add_ss(c21_7, _mm_mul_ss(a21_7, b21));
    _mm_store_ss(&C[(l_n*56)+45], c21_7);
#else
    C[(l_n*56)+20] += A[214] * B[(l_n*56)+21];
    C[(l_n*56)+21] += A[215] * B[(l_n*56)+21];
    C[(l_n*56)+22] += A[216] * B[(l_n*56)+21];
    C[(l_n*56)+24] += A[217] * B[(l_n*56)+21];
    C[(l_n*56)+41] += A[218] * B[(l_n*56)+21];
    C[(l_n*56)+42] += A[219] * B[(l_n*56)+21];
    C[(l_n*56)+43] += A[220] * B[(l_n*56)+21];
    C[(l_n*56)+45] += A[221] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a22_0 = _mm_loadu_ps(&A[222]);
    c22_0 = _mm_add_ps(c22_0, _mm_mul_ps(a22_0, b22));
    _mm_storeu_ps(&C[(l_n*56)+20], c22_0);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a22_4 = _mm_load_ss(&A[226]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*56)+24], c22_4);
    __m128 c22_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a22_5 = _mm_loadu_ps(&A[227]);
    c22_5 = _mm_add_ps(c22_5, _mm_mul_ps(a22_5, b22));
    _mm_storeu_ps(&C[(l_n*56)+41], c22_5);
    __m128 c22_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a22_9 = _mm_load_ss(&A[231]);
    c22_9 = _mm_add_ss(c22_9, _mm_mul_ss(a22_9, b22));
    _mm_store_ss(&C[(l_n*56)+45], c22_9);
#else
    C[(l_n*56)+20] += A[222] * B[(l_n*56)+22];
    C[(l_n*56)+21] += A[223] * B[(l_n*56)+22];
    C[(l_n*56)+22] += A[224] * B[(l_n*56)+22];
    C[(l_n*56)+23] += A[225] * B[(l_n*56)+22];
    C[(l_n*56)+24] += A[226] * B[(l_n*56)+22];
    C[(l_n*56)+41] += A[227] * B[(l_n*56)+22];
    C[(l_n*56)+42] += A[228] * B[(l_n*56)+22];
    C[(l_n*56)+43] += A[229] * B[(l_n*56)+22];
    C[(l_n*56)+44] += A[230] * B[(l_n*56)+22];
    C[(l_n*56)+45] += A[231] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a23_0 = _mm_load_ss(&A[232]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+20], c23_0);
    __m128 c23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+22]));
    __m128 a23_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[233]));
    c23_1 = _mm_add_ps(c23_1, _mm_mul_ps(a23_1, b23));
    _mm_store_sd((double*)&C[(l_n*56)+22], _mm_castps_pd(c23_1));
    __m128 c23_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a23_3 = _mm_load_ss(&A[235]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*56)+24], c23_3);
    __m128 c23_4 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a23_4 = _mm_load_ss(&A[236]);
    c23_4 = _mm_add_ss(c23_4, _mm_mul_ss(a23_4, b23));
    _mm_store_ss(&C[(l_n*56)+41], c23_4);
    __m128 c23_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+43]));
    __m128 a23_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[237]));
    c23_5 = _mm_add_ps(c23_5, _mm_mul_ps(a23_5, b23));
    _mm_store_sd((double*)&C[(l_n*56)+43], _mm_castps_pd(c23_5));
    __m128 c23_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a23_7 = _mm_load_ss(&A[239]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*56)+45], c23_7);
#else
    C[(l_n*56)+20] += A[232] * B[(l_n*56)+23];
    C[(l_n*56)+22] += A[233] * B[(l_n*56)+23];
    C[(l_n*56)+23] += A[234] * B[(l_n*56)+23];
    C[(l_n*56)+24] += A[235] * B[(l_n*56)+23];
    C[(l_n*56)+41] += A[236] * B[(l_n*56)+23];
    C[(l_n*56)+43] += A[237] * B[(l_n*56)+23];
    C[(l_n*56)+44] += A[238] * B[(l_n*56)+23];
    C[(l_n*56)+45] += A[239] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*56)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a24_0 = _mm_loadu_ps(&A[240]);
    c24_0 = _mm_add_ps(c24_0, _mm_mul_ps(a24_0, b24));
    _mm_storeu_ps(&C[(l_n*56)+20], c24_0);
    __m128 c24_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a24_4 = _mm_load_ss(&A[244]);
    c24_4 = _mm_add_ss(c24_4, _mm_mul_ss(a24_4, b24));
    _mm_store_ss(&C[(l_n*56)+24], c24_4);
    __m128 c24_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a24_5 = _mm_loadu_ps(&A[245]);
    c24_5 = _mm_add_ps(c24_5, _mm_mul_ps(a24_5, b24));
    _mm_storeu_ps(&C[(l_n*56)+41], c24_5);
    __m128 c24_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a24_9 = _mm_load_ss(&A[249]);
    c24_9 = _mm_add_ss(c24_9, _mm_mul_ss(a24_9, b24));
    _mm_store_ss(&C[(l_n*56)+45], c24_9);
#else
    C[(l_n*56)+20] += A[240] * B[(l_n*56)+24];
    C[(l_n*56)+21] += A[241] * B[(l_n*56)+24];
    C[(l_n*56)+22] += A[242] * B[(l_n*56)+24];
    C[(l_n*56)+23] += A[243] * B[(l_n*56)+24];
    C[(l_n*56)+24] += A[244] * B[(l_n*56)+24];
    C[(l_n*56)+41] += A[245] * B[(l_n*56)+24];
    C[(l_n*56)+42] += A[246] * B[(l_n*56)+24];
    C[(l_n*56)+43] += A[247] * B[(l_n*56)+24];
    C[(l_n*56)+44] += A[248] * B[(l_n*56)+24];
    C[(l_n*56)+45] += A[249] * B[(l_n*56)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a25_0 = _mm_loadu_ps(&A[250]);
    c25_0 = _mm_add_ps(c25_0, _mm_mul_ps(a25_0, b25));
    _mm_storeu_ps(&C[(l_n*56)+10], c25_0);
    __m128 c25_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a25_4 = _mm_loadu_ps(&A[254]);
    c25_4 = _mm_add_ps(c25_4, _mm_mul_ps(a25_4, b25));
    _mm_storeu_ps(&C[(l_n*56)+25], c25_4);
    __m128 c25_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a25_8 = _mm_loadu_ps(&A[258]);
    c25_8 = _mm_add_ps(c25_8, _mm_mul_ps(a25_8, b25));
    _mm_storeu_ps(&C[(l_n*56)+46], c25_8);
#else
    C[(l_n*56)+10] += A[250] * B[(l_n*56)+25];
    C[(l_n*56)+11] += A[251] * B[(l_n*56)+25];
    C[(l_n*56)+12] += A[252] * B[(l_n*56)+25];
    C[(l_n*56)+13] += A[253] * B[(l_n*56)+25];
    C[(l_n*56)+25] += A[254] * B[(l_n*56)+25];
    C[(l_n*56)+26] += A[255] * B[(l_n*56)+25];
    C[(l_n*56)+27] += A[256] * B[(l_n*56)+25];
    C[(l_n*56)+28] += A[257] * B[(l_n*56)+25];
    C[(l_n*56)+46] += A[258] * B[(l_n*56)+25];
    C[(l_n*56)+47] += A[259] * B[(l_n*56)+25];
    C[(l_n*56)+48] += A[260] * B[(l_n*56)+25];
    C[(l_n*56)+49] += A[261] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a26_0 = _mm_loadu_ps(&A[262]);
    c26_0 = _mm_add_ps(c26_0, _mm_mul_ps(a26_0, b26));
    _mm_storeu_ps(&C[(l_n*56)+10], c26_0);
    __m128 c26_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a26_4 = _mm_loadu_ps(&A[266]);
    c26_4 = _mm_add_ps(c26_4, _mm_mul_ps(a26_4, b26));
    _mm_storeu_ps(&C[(l_n*56)+25], c26_4);
    __m128 c26_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a26_8 = _mm_loadu_ps(&A[270]);
    c26_8 = _mm_add_ps(c26_8, _mm_mul_ps(a26_8, b26));
    _mm_storeu_ps(&C[(l_n*56)+46], c26_8);
#else
    C[(l_n*56)+10] += A[262] * B[(l_n*56)+26];
    C[(l_n*56)+11] += A[263] * B[(l_n*56)+26];
    C[(l_n*56)+12] += A[264] * B[(l_n*56)+26];
    C[(l_n*56)+13] += A[265] * B[(l_n*56)+26];
    C[(l_n*56)+25] += A[266] * B[(l_n*56)+26];
    C[(l_n*56)+26] += A[267] * B[(l_n*56)+26];
    C[(l_n*56)+27] += A[268] * B[(l_n*56)+26];
    C[(l_n*56)+28] += A[269] * B[(l_n*56)+26];
    C[(l_n*56)+46] += A[270] * B[(l_n*56)+26];
    C[(l_n*56)+47] += A[271] * B[(l_n*56)+26];
    C[(l_n*56)+48] += A[272] * B[(l_n*56)+26];
    C[(l_n*56)+49] += A[273] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a27_0 = _mm_loadu_ps(&A[274]);
    c27_0 = _mm_add_ps(c27_0, _mm_mul_ps(a27_0, b27));
    _mm_storeu_ps(&C[(l_n*56)+10], c27_0);
    __m128 c27_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a27_4 = _mm_loadu_ps(&A[278]);
    c27_4 = _mm_add_ps(c27_4, _mm_mul_ps(a27_4, b27));
    _mm_storeu_ps(&C[(l_n*56)+25], c27_4);
    __m128 c27_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a27_8 = _mm_loadu_ps(&A[282]);
    c27_8 = _mm_add_ps(c27_8, _mm_mul_ps(a27_8, b27));
    _mm_storeu_ps(&C[(l_n*56)+46], c27_8);
#else
    C[(l_n*56)+10] += A[274] * B[(l_n*56)+27];
    C[(l_n*56)+11] += A[275] * B[(l_n*56)+27];
    C[(l_n*56)+12] += A[276] * B[(l_n*56)+27];
    C[(l_n*56)+13] += A[277] * B[(l_n*56)+27];
    C[(l_n*56)+25] += A[278] * B[(l_n*56)+27];
    C[(l_n*56)+26] += A[279] * B[(l_n*56)+27];
    C[(l_n*56)+27] += A[280] * B[(l_n*56)+27];
    C[(l_n*56)+28] += A[281] * B[(l_n*56)+27];
    C[(l_n*56)+46] += A[282] * B[(l_n*56)+27];
    C[(l_n*56)+47] += A[283] * B[(l_n*56)+27];
    C[(l_n*56)+48] += A[284] * B[(l_n*56)+27];
    C[(l_n*56)+49] += A[285] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*56)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a28_0 = _mm_loadu_ps(&A[286]);
    c28_0 = _mm_add_ps(c28_0, _mm_mul_ps(a28_0, b28));
    _mm_storeu_ps(&C[(l_n*56)+10], c28_0);
    __m128 c28_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a28_4 = _mm_loadu_ps(&A[290]);
    c28_4 = _mm_add_ps(c28_4, _mm_mul_ps(a28_4, b28));
    _mm_storeu_ps(&C[(l_n*56)+25], c28_4);
    __m128 c28_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a28_8 = _mm_loadu_ps(&A[294]);
    c28_8 = _mm_add_ps(c28_8, _mm_mul_ps(a28_8, b28));
    _mm_storeu_ps(&C[(l_n*56)+46], c28_8);
#else
    C[(l_n*56)+10] += A[286] * B[(l_n*56)+28];
    C[(l_n*56)+11] += A[287] * B[(l_n*56)+28];
    C[(l_n*56)+12] += A[288] * B[(l_n*56)+28];
    C[(l_n*56)+13] += A[289] * B[(l_n*56)+28];
    C[(l_n*56)+25] += A[290] * B[(l_n*56)+28];
    C[(l_n*56)+26] += A[291] * B[(l_n*56)+28];
    C[(l_n*56)+27] += A[292] * B[(l_n*56)+28];
    C[(l_n*56)+28] += A[293] * B[(l_n*56)+28];
    C[(l_n*56)+46] += A[294] * B[(l_n*56)+28];
    C[(l_n*56)+47] += A[295] * B[(l_n*56)+28];
    C[(l_n*56)+48] += A[296] * B[(l_n*56)+28];
    C[(l_n*56)+49] += A[297] * B[(l_n*56)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a29_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[298]));
    c29_0 = _mm_add_ps(c29_0, _mm_mul_ps(a29_0, b29));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c29_0));
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a29_2 = _mm_load_ss(&A[300]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+6], c29_2);
    __m128 c29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a29_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[301]));
    c29_3 = _mm_add_ps(c29_3, _mm_mul_ps(a29_3, b29));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c29_3));
    __m128 c29_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a29_5 = _mm_load_ss(&A[303]);
    c29_5 = _mm_add_ss(c29_5, _mm_mul_ss(a29_5, b29));
    _mm_store_ss(&C[(l_n*56)+16], c29_5);
    __m128 c29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a29_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[304]));
    c29_6 = _mm_add_ps(c29_6, _mm_mul_ps(a29_6, b29));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c29_6));
    __m128 c29_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a29_8 = _mm_load_ss(&A[306]);
    c29_8 = _mm_add_ss(c29_8, _mm_mul_ss(a29_8, b29));
    _mm_store_ss(&C[(l_n*56)+31], c29_8);
    __m128 c29_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a29_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[307]));
    c29_9 = _mm_add_ps(c29_9, _mm_mul_ps(a29_9, b29));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c29_9));
    __m128 c29_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a29_11 = _mm_load_ss(&A[309]);
    c29_11 = _mm_add_ss(c29_11, _mm_mul_ss(a29_11, b29));
    _mm_store_ss(&C[(l_n*56)+52], c29_11);
#else
    C[(l_n*56)+4] += A[298] * B[(l_n*56)+29];
    C[(l_n*56)+5] += A[299] * B[(l_n*56)+29];
    C[(l_n*56)+6] += A[300] * B[(l_n*56)+29];
    C[(l_n*56)+14] += A[301] * B[(l_n*56)+29];
    C[(l_n*56)+15] += A[302] * B[(l_n*56)+29];
    C[(l_n*56)+16] += A[303] * B[(l_n*56)+29];
    C[(l_n*56)+29] += A[304] * B[(l_n*56)+29];
    C[(l_n*56)+30] += A[305] * B[(l_n*56)+29];
    C[(l_n*56)+31] += A[306] * B[(l_n*56)+29];
    C[(l_n*56)+50] += A[307] * B[(l_n*56)+29];
    C[(l_n*56)+51] += A[308] * B[(l_n*56)+29];
    C[(l_n*56)+52] += A[309] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a30_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[310]));
    c30_0 = _mm_add_ps(c30_0, _mm_mul_ps(a30_0, b30));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c30_0));
    __m128 c30_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a30_2 = _mm_load_ss(&A[312]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*56)+6], c30_2);
    __m128 c30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a30_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[313]));
    c30_3 = _mm_add_ps(c30_3, _mm_mul_ps(a30_3, b30));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c30_3));
    __m128 c30_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a30_5 = _mm_load_ss(&A[315]);
    c30_5 = _mm_add_ss(c30_5, _mm_mul_ss(a30_5, b30));
    _mm_store_ss(&C[(l_n*56)+16], c30_5);
    __m128 c30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a30_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[316]));
    c30_6 = _mm_add_ps(c30_6, _mm_mul_ps(a30_6, b30));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c30_6));
    __m128 c30_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a30_8 = _mm_load_ss(&A[318]);
    c30_8 = _mm_add_ss(c30_8, _mm_mul_ss(a30_8, b30));
    _mm_store_ss(&C[(l_n*56)+31], c30_8);
    __m128 c30_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a30_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[319]));
    c30_9 = _mm_add_ps(c30_9, _mm_mul_ps(a30_9, b30));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c30_9));
    __m128 c30_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a30_11 = _mm_load_ss(&A[321]);
    c30_11 = _mm_add_ss(c30_11, _mm_mul_ss(a30_11, b30));
    _mm_store_ss(&C[(l_n*56)+52], c30_11);
#else
    C[(l_n*56)+4] += A[310] * B[(l_n*56)+30];
    C[(l_n*56)+5] += A[311] * B[(l_n*56)+30];
    C[(l_n*56)+6] += A[312] * B[(l_n*56)+30];
    C[(l_n*56)+14] += A[313] * B[(l_n*56)+30];
    C[(l_n*56)+15] += A[314] * B[(l_n*56)+30];
    C[(l_n*56)+16] += A[315] * B[(l_n*56)+30];
    C[(l_n*56)+29] += A[316] * B[(l_n*56)+30];
    C[(l_n*56)+30] += A[317] * B[(l_n*56)+30];
    C[(l_n*56)+31] += A[318] * B[(l_n*56)+30];
    C[(l_n*56)+50] += A[319] * B[(l_n*56)+30];
    C[(l_n*56)+51] += A[320] * B[(l_n*56)+30];
    C[(l_n*56)+52] += A[321] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*56)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a31_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[322]));
    c31_0 = _mm_add_ps(c31_0, _mm_mul_ps(a31_0, b31));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c31_0));
    __m128 c31_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a31_2 = _mm_load_ss(&A[324]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*56)+6], c31_2);
    __m128 c31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a31_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[325]));
    c31_3 = _mm_add_ps(c31_3, _mm_mul_ps(a31_3, b31));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c31_3));
    __m128 c31_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a31_5 = _mm_load_ss(&A[327]);
    c31_5 = _mm_add_ss(c31_5, _mm_mul_ss(a31_5, b31));
    _mm_store_ss(&C[(l_n*56)+16], c31_5);
    __m128 c31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a31_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[328]));
    c31_6 = _mm_add_ps(c31_6, _mm_mul_ps(a31_6, b31));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c31_6));
    __m128 c31_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a31_8 = _mm_load_ss(&A[330]);
    c31_8 = _mm_add_ss(c31_8, _mm_mul_ss(a31_8, b31));
    _mm_store_ss(&C[(l_n*56)+31], c31_8);
    __m128 c31_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a31_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[331]));
    c31_9 = _mm_add_ps(c31_9, _mm_mul_ps(a31_9, b31));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c31_9));
    __m128 c31_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a31_11 = _mm_load_ss(&A[333]);
    c31_11 = _mm_add_ss(c31_11, _mm_mul_ss(a31_11, b31));
    _mm_store_ss(&C[(l_n*56)+52], c31_11);
#else
    C[(l_n*56)+4] += A[322] * B[(l_n*56)+31];
    C[(l_n*56)+5] += A[323] * B[(l_n*56)+31];
    C[(l_n*56)+6] += A[324] * B[(l_n*56)+31];
    C[(l_n*56)+14] += A[325] * B[(l_n*56)+31];
    C[(l_n*56)+15] += A[326] * B[(l_n*56)+31];
    C[(l_n*56)+16] += A[327] * B[(l_n*56)+31];
    C[(l_n*56)+29] += A[328] * B[(l_n*56)+31];
    C[(l_n*56)+30] += A[329] * B[(l_n*56)+31];
    C[(l_n*56)+31] += A[330] * B[(l_n*56)+31];
    C[(l_n*56)+50] += A[331] * B[(l_n*56)+31];
    C[(l_n*56)+51] += A[332] * B[(l_n*56)+31];
    C[(l_n*56)+52] += A[333] * B[(l_n*56)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a32_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[334]));
    c32_0 = _mm_add_ps(c32_0, _mm_mul_ps(a32_0, b32));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c32_0));
    __m128 c32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a32_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[336]));
    c32_2 = _mm_add_ps(c32_2, _mm_mul_ps(a32_2, b32));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c32_2));
    __m128 c32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a32_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[338]));
    c32_4 = _mm_add_ps(c32_4, _mm_mul_ps(a32_4, b32));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c32_4));
    __m128 c32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a32_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[340]));
    c32_6 = _mm_add_ps(c32_6, _mm_mul_ps(a32_6, b32));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c32_6));
    __m128 c32_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a32_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[342]));
    c32_8 = _mm_add_ps(c32_8, _mm_mul_ps(a32_8, b32));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c32_8));
#else
    C[(l_n*56)+1] += A[334] * B[(l_n*56)+32];
    C[(l_n*56)+2] += A[335] * B[(l_n*56)+32];
    C[(l_n*56)+7] += A[336] * B[(l_n*56)+32];
    C[(l_n*56)+8] += A[337] * B[(l_n*56)+32];
    C[(l_n*56)+17] += A[338] * B[(l_n*56)+32];
    C[(l_n*56)+18] += A[339] * B[(l_n*56)+32];
    C[(l_n*56)+32] += A[340] * B[(l_n*56)+32];
    C[(l_n*56)+33] += A[341] * B[(l_n*56)+32];
    C[(l_n*56)+53] += A[342] * B[(l_n*56)+32];
    C[(l_n*56)+54] += A[343] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*56)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a33_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[344]));
    c33_0 = _mm_add_ps(c33_0, _mm_mul_ps(a33_0, b33));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c33_0));
    __m128 c33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a33_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[346]));
    c33_2 = _mm_add_ps(c33_2, _mm_mul_ps(a33_2, b33));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c33_2));
    __m128 c33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a33_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[348]));
    c33_4 = _mm_add_ps(c33_4, _mm_mul_ps(a33_4, b33));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c33_4));
    __m128 c33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a33_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[350]));
    c33_6 = _mm_add_ps(c33_6, _mm_mul_ps(a33_6, b33));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c33_6));
    __m128 c33_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a33_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[352]));
    c33_8 = _mm_add_ps(c33_8, _mm_mul_ps(a33_8, b33));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c33_8));
#else
    C[(l_n*56)+1] += A[344] * B[(l_n*56)+33];
    C[(l_n*56)+2] += A[345] * B[(l_n*56)+33];
    C[(l_n*56)+7] += A[346] * B[(l_n*56)+33];
    C[(l_n*56)+8] += A[347] * B[(l_n*56)+33];
    C[(l_n*56)+17] += A[348] * B[(l_n*56)+33];
    C[(l_n*56)+18] += A[349] * B[(l_n*56)+33];
    C[(l_n*56)+32] += A[350] * B[(l_n*56)+33];
    C[(l_n*56)+33] += A[351] * B[(l_n*56)+33];
    C[(l_n*56)+53] += A[352] * B[(l_n*56)+33];
    C[(l_n*56)+54] += A[353] * B[(l_n*56)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*56)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a34_0 = _mm_load_ss(&A[354]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*56)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a34_1 = _mm_load_ss(&A[355]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*56)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a34_2 = _mm_load_ss(&A[356]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*56)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a34_3 = _mm_load_ss(&A[357]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*56)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a34_4 = _mm_load_ss(&A[358]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*56)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a34_5 = _mm_load_ss(&A[359]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*56)+55], c34_5);
#else
    C[(l_n*56)+0] += A[354] * B[(l_n*56)+34];
    C[(l_n*56)+3] += A[355] * B[(l_n*56)+34];
    C[(l_n*56)+9] += A[356] * B[(l_n*56)+34];
    C[(l_n*56)+19] += A[357] * B[(l_n*56)+34];
    C[(l_n*56)+34] += A[358] * B[(l_n*56)+34];
    C[(l_n*56)+55] += A[359] * B[(l_n*56)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a35_0 = _mm_loadu_ps(&A[360]);
    c35_0 = _mm_add_ps(c35_0, _mm_mul_ps(a35_0, b35));
    _mm_storeu_ps(&C[(l_n*56)+35], c35_0);
    __m128 c35_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a35_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[364]));
    c35_4 = _mm_add_ps(c35_4, _mm_mul_ps(a35_4, b35));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c35_4));
#else
    C[(l_n*56)+35] += A[360] * B[(l_n*56)+35];
    C[(l_n*56)+36] += A[361] * B[(l_n*56)+35];
    C[(l_n*56)+37] += A[362] * B[(l_n*56)+35];
    C[(l_n*56)+38] += A[363] * B[(l_n*56)+35];
    C[(l_n*56)+39] += A[364] * B[(l_n*56)+35];
    C[(l_n*56)+40] += A[365] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a36_0 = _mm_loadu_ps(&A[366]);
    c36_0 = _mm_add_ps(c36_0, _mm_mul_ps(a36_0, b36));
    _mm_storeu_ps(&C[(l_n*56)+35], c36_0);
    __m128 c36_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a36_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[370]));
    c36_4 = _mm_add_ps(c36_4, _mm_mul_ps(a36_4, b36));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c36_4));
#else
    C[(l_n*56)+35] += A[366] * B[(l_n*56)+36];
    C[(l_n*56)+36] += A[367] * B[(l_n*56)+36];
    C[(l_n*56)+37] += A[368] * B[(l_n*56)+36];
    C[(l_n*56)+38] += A[369] * B[(l_n*56)+36];
    C[(l_n*56)+39] += A[370] * B[(l_n*56)+36];
    C[(l_n*56)+40] += A[371] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a37_0 = _mm_loadu_ps(&A[372]);
    c37_0 = _mm_add_ps(c37_0, _mm_mul_ps(a37_0, b37));
    _mm_storeu_ps(&C[(l_n*56)+35], c37_0);
    __m128 c37_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a37_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[376]));
    c37_4 = _mm_add_ps(c37_4, _mm_mul_ps(a37_4, b37));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c37_4));
#else
    C[(l_n*56)+35] += A[372] * B[(l_n*56)+37];
    C[(l_n*56)+36] += A[373] * B[(l_n*56)+37];
    C[(l_n*56)+37] += A[374] * B[(l_n*56)+37];
    C[(l_n*56)+38] += A[375] * B[(l_n*56)+37];
    C[(l_n*56)+39] += A[376] * B[(l_n*56)+37];
    C[(l_n*56)+40] += A[377] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a38_0 = _mm_loadu_ps(&A[378]);
    c38_0 = _mm_add_ps(c38_0, _mm_mul_ps(a38_0, b38));
    _mm_storeu_ps(&C[(l_n*56)+35], c38_0);
    __m128 c38_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a38_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[382]));
    c38_4 = _mm_add_ps(c38_4, _mm_mul_ps(a38_4, b38));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c38_4));
#else
    C[(l_n*56)+35] += A[378] * B[(l_n*56)+38];
    C[(l_n*56)+36] += A[379] * B[(l_n*56)+38];
    C[(l_n*56)+37] += A[380] * B[(l_n*56)+38];
    C[(l_n*56)+38] += A[381] * B[(l_n*56)+38];
    C[(l_n*56)+39] += A[382] * B[(l_n*56)+38];
    C[(l_n*56)+40] += A[383] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a39_0 = _mm_loadu_ps(&A[384]);
    c39_0 = _mm_add_ps(c39_0, _mm_mul_ps(a39_0, b39));
    _mm_storeu_ps(&C[(l_n*56)+35], c39_0);
    __m128 c39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a39_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[388]));
    c39_4 = _mm_add_ps(c39_4, _mm_mul_ps(a39_4, b39));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c39_4));
#else
    C[(l_n*56)+35] += A[384] * B[(l_n*56)+39];
    C[(l_n*56)+36] += A[385] * B[(l_n*56)+39];
    C[(l_n*56)+37] += A[386] * B[(l_n*56)+39];
    C[(l_n*56)+38] += A[387] * B[(l_n*56)+39];
    C[(l_n*56)+39] += A[388] * B[(l_n*56)+39];
    C[(l_n*56)+40] += A[389] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*56)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_loadu_ps(&C[(l_n*56)+35]);
    __m128 a40_0 = _mm_loadu_ps(&A[390]);
    c40_0 = _mm_add_ps(c40_0, _mm_mul_ps(a40_0, b40));
    _mm_storeu_ps(&C[(l_n*56)+35], c40_0);
    __m128 c40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+39]));
    __m128 a40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[394]));
    c40_4 = _mm_add_ps(c40_4, _mm_mul_ps(a40_4, b40));
    _mm_store_sd((double*)&C[(l_n*56)+39], _mm_castps_pd(c40_4));
#else
    C[(l_n*56)+35] += A[390] * B[(l_n*56)+40];
    C[(l_n*56)+36] += A[391] * B[(l_n*56)+40];
    C[(l_n*56)+37] += A[392] * B[(l_n*56)+40];
    C[(l_n*56)+38] += A[393] * B[(l_n*56)+40];
    C[(l_n*56)+39] += A[394] * B[(l_n*56)+40];
    C[(l_n*56)+40] += A[395] * B[(l_n*56)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a41_0 = _mm_loadu_ps(&A[396]);
    c41_0 = _mm_add_ps(c41_0, _mm_mul_ps(a41_0, b41));
    _mm_storeu_ps(&C[(l_n*56)+20], c41_0);
    __m128 c41_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a41_4 = _mm_load_ss(&A[400]);
    c41_4 = _mm_add_ss(c41_4, _mm_mul_ss(a41_4, b41));
    _mm_store_ss(&C[(l_n*56)+24], c41_4);
    __m128 c41_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a41_5 = _mm_loadu_ps(&A[401]);
    c41_5 = _mm_add_ps(c41_5, _mm_mul_ps(a41_5, b41));
    _mm_storeu_ps(&C[(l_n*56)+41], c41_5);
    __m128 c41_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a41_9 = _mm_load_ss(&A[405]);
    c41_9 = _mm_add_ss(c41_9, _mm_mul_ss(a41_9, b41));
    _mm_store_ss(&C[(l_n*56)+45], c41_9);
#else
    C[(l_n*56)+20] += A[396] * B[(l_n*56)+41];
    C[(l_n*56)+21] += A[397] * B[(l_n*56)+41];
    C[(l_n*56)+22] += A[398] * B[(l_n*56)+41];
    C[(l_n*56)+23] += A[399] * B[(l_n*56)+41];
    C[(l_n*56)+24] += A[400] * B[(l_n*56)+41];
    C[(l_n*56)+41] += A[401] * B[(l_n*56)+41];
    C[(l_n*56)+42] += A[402] * B[(l_n*56)+41];
    C[(l_n*56)+43] += A[403] * B[(l_n*56)+41];
    C[(l_n*56)+44] += A[404] * B[(l_n*56)+41];
    C[(l_n*56)+45] += A[405] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+20]));
    __m128 a42_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[406]));
    c42_0 = _mm_add_ps(c42_0, _mm_mul_ps(a42_0, b42));
    _mm_store_sd((double*)&C[(l_n*56)+20], _mm_castps_pd(c42_0));
    __m128 c42_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a42_2 = _mm_load_ss(&A[408]);
    c42_2 = _mm_add_ss(c42_2, _mm_mul_ss(a42_2, b42));
    _mm_store_ss(&C[(l_n*56)+22], c42_2);
    __m128 c42_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a42_3 = _mm_load_ss(&A[409]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*56)+24], c42_3);
    __m128 c42_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+41]));
    __m128 a42_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[410]));
    c42_4 = _mm_add_ps(c42_4, _mm_mul_ps(a42_4, b42));
    _mm_store_sd((double*)&C[(l_n*56)+41], _mm_castps_pd(c42_4));
    __m128 c42_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a42_6 = _mm_load_ss(&A[412]);
    c42_6 = _mm_add_ss(c42_6, _mm_mul_ss(a42_6, b42));
    _mm_store_ss(&C[(l_n*56)+43], c42_6);
    __m128 c42_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a42_7 = _mm_load_ss(&A[413]);
    c42_7 = _mm_add_ss(c42_7, _mm_mul_ss(a42_7, b42));
    _mm_store_ss(&C[(l_n*56)+45], c42_7);
#else
    C[(l_n*56)+20] += A[406] * B[(l_n*56)+42];
    C[(l_n*56)+21] += A[407] * B[(l_n*56)+42];
    C[(l_n*56)+22] += A[408] * B[(l_n*56)+42];
    C[(l_n*56)+24] += A[409] * B[(l_n*56)+42];
    C[(l_n*56)+41] += A[410] * B[(l_n*56)+42];
    C[(l_n*56)+42] += A[411] * B[(l_n*56)+42];
    C[(l_n*56)+43] += A[412] * B[(l_n*56)+42];
    C[(l_n*56)+45] += A[413] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a43_0 = _mm_loadu_ps(&A[414]);
    c43_0 = _mm_add_ps(c43_0, _mm_mul_ps(a43_0, b43));
    _mm_storeu_ps(&C[(l_n*56)+20], c43_0);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a43_4 = _mm_load_ss(&A[418]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*56)+24], c43_4);
    __m128 c43_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a43_5 = _mm_loadu_ps(&A[419]);
    c43_5 = _mm_add_ps(c43_5, _mm_mul_ps(a43_5, b43));
    _mm_storeu_ps(&C[(l_n*56)+41], c43_5);
    __m128 c43_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a43_9 = _mm_load_ss(&A[423]);
    c43_9 = _mm_add_ss(c43_9, _mm_mul_ss(a43_9, b43));
    _mm_store_ss(&C[(l_n*56)+45], c43_9);
#else
    C[(l_n*56)+20] += A[414] * B[(l_n*56)+43];
    C[(l_n*56)+21] += A[415] * B[(l_n*56)+43];
    C[(l_n*56)+22] += A[416] * B[(l_n*56)+43];
    C[(l_n*56)+23] += A[417] * B[(l_n*56)+43];
    C[(l_n*56)+24] += A[418] * B[(l_n*56)+43];
    C[(l_n*56)+41] += A[419] * B[(l_n*56)+43];
    C[(l_n*56)+42] += A[420] * B[(l_n*56)+43];
    C[(l_n*56)+43] += A[421] * B[(l_n*56)+43];
    C[(l_n*56)+44] += A[422] * B[(l_n*56)+43];
    C[(l_n*56)+45] += A[423] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a44_0 = _mm_load_ss(&A[424]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+20], c44_0);
    __m128 c44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+22]));
    __m128 a44_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[425]));
    c44_1 = _mm_add_ps(c44_1, _mm_mul_ps(a44_1, b44));
    _mm_store_sd((double*)&C[(l_n*56)+22], _mm_castps_pd(c44_1));
    __m128 c44_3 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a44_3 = _mm_load_ss(&A[427]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*56)+24], c44_3);
    __m128 c44_4 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a44_4 = _mm_load_ss(&A[428]);
    c44_4 = _mm_add_ss(c44_4, _mm_mul_ss(a44_4, b44));
    _mm_store_ss(&C[(l_n*56)+41], c44_4);
    __m128 c44_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+43]));
    __m128 a44_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[429]));
    c44_5 = _mm_add_ps(c44_5, _mm_mul_ps(a44_5, b44));
    _mm_store_sd((double*)&C[(l_n*56)+43], _mm_castps_pd(c44_5));
    __m128 c44_7 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a44_7 = _mm_load_ss(&A[431]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*56)+45], c44_7);
#else
    C[(l_n*56)+20] += A[424] * B[(l_n*56)+44];
    C[(l_n*56)+22] += A[425] * B[(l_n*56)+44];
    C[(l_n*56)+23] += A[426] * B[(l_n*56)+44];
    C[(l_n*56)+24] += A[427] * B[(l_n*56)+44];
    C[(l_n*56)+41] += A[428] * B[(l_n*56)+44];
    C[(l_n*56)+43] += A[429] * B[(l_n*56)+44];
    C[(l_n*56)+44] += A[430] * B[(l_n*56)+44];
    C[(l_n*56)+45] += A[431] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*56)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_loadu_ps(&C[(l_n*56)+20]);
    __m128 a45_0 = _mm_loadu_ps(&A[432]);
    c45_0 = _mm_add_ps(c45_0, _mm_mul_ps(a45_0, b45));
    _mm_storeu_ps(&C[(l_n*56)+20], c45_0);
    __m128 c45_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a45_4 = _mm_load_ss(&A[436]);
    c45_4 = _mm_add_ss(c45_4, _mm_mul_ss(a45_4, b45));
    _mm_store_ss(&C[(l_n*56)+24], c45_4);
    __m128 c45_5 = _mm_loadu_ps(&C[(l_n*56)+41]);
    __m128 a45_5 = _mm_loadu_ps(&A[437]);
    c45_5 = _mm_add_ps(c45_5, _mm_mul_ps(a45_5, b45));
    _mm_storeu_ps(&C[(l_n*56)+41], c45_5);
    __m128 c45_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a45_9 = _mm_load_ss(&A[441]);
    c45_9 = _mm_add_ss(c45_9, _mm_mul_ss(a45_9, b45));
    _mm_store_ss(&C[(l_n*56)+45], c45_9);
#else
    C[(l_n*56)+20] += A[432] * B[(l_n*56)+45];
    C[(l_n*56)+21] += A[433] * B[(l_n*56)+45];
    C[(l_n*56)+22] += A[434] * B[(l_n*56)+45];
    C[(l_n*56)+23] += A[435] * B[(l_n*56)+45];
    C[(l_n*56)+24] += A[436] * B[(l_n*56)+45];
    C[(l_n*56)+41] += A[437] * B[(l_n*56)+45];
    C[(l_n*56)+42] += A[438] * B[(l_n*56)+45];
    C[(l_n*56)+43] += A[439] * B[(l_n*56)+45];
    C[(l_n*56)+44] += A[440] * B[(l_n*56)+45];
    C[(l_n*56)+45] += A[441] * B[(l_n*56)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a46_0 = _mm_loadu_ps(&A[442]);
    c46_0 = _mm_add_ps(c46_0, _mm_mul_ps(a46_0, b46));
    _mm_storeu_ps(&C[(l_n*56)+10], c46_0);
    __m128 c46_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a46_4 = _mm_loadu_ps(&A[446]);
    c46_4 = _mm_add_ps(c46_4, _mm_mul_ps(a46_4, b46));
    _mm_storeu_ps(&C[(l_n*56)+25], c46_4);
    __m128 c46_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a46_8 = _mm_loadu_ps(&A[450]);
    c46_8 = _mm_add_ps(c46_8, _mm_mul_ps(a46_8, b46));
    _mm_storeu_ps(&C[(l_n*56)+46], c46_8);
#else
    C[(l_n*56)+10] += A[442] * B[(l_n*56)+46];
    C[(l_n*56)+11] += A[443] * B[(l_n*56)+46];
    C[(l_n*56)+12] += A[444] * B[(l_n*56)+46];
    C[(l_n*56)+13] += A[445] * B[(l_n*56)+46];
    C[(l_n*56)+25] += A[446] * B[(l_n*56)+46];
    C[(l_n*56)+26] += A[447] * B[(l_n*56)+46];
    C[(l_n*56)+27] += A[448] * B[(l_n*56)+46];
    C[(l_n*56)+28] += A[449] * B[(l_n*56)+46];
    C[(l_n*56)+46] += A[450] * B[(l_n*56)+46];
    C[(l_n*56)+47] += A[451] * B[(l_n*56)+46];
    C[(l_n*56)+48] += A[452] * B[(l_n*56)+46];
    C[(l_n*56)+49] += A[453] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a47_0 = _mm_loadu_ps(&A[454]);
    c47_0 = _mm_add_ps(c47_0, _mm_mul_ps(a47_0, b47));
    _mm_storeu_ps(&C[(l_n*56)+10], c47_0);
    __m128 c47_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a47_4 = _mm_loadu_ps(&A[458]);
    c47_4 = _mm_add_ps(c47_4, _mm_mul_ps(a47_4, b47));
    _mm_storeu_ps(&C[(l_n*56)+25], c47_4);
    __m128 c47_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a47_8 = _mm_loadu_ps(&A[462]);
    c47_8 = _mm_add_ps(c47_8, _mm_mul_ps(a47_8, b47));
    _mm_storeu_ps(&C[(l_n*56)+46], c47_8);
#else
    C[(l_n*56)+10] += A[454] * B[(l_n*56)+47];
    C[(l_n*56)+11] += A[455] * B[(l_n*56)+47];
    C[(l_n*56)+12] += A[456] * B[(l_n*56)+47];
    C[(l_n*56)+13] += A[457] * B[(l_n*56)+47];
    C[(l_n*56)+25] += A[458] * B[(l_n*56)+47];
    C[(l_n*56)+26] += A[459] * B[(l_n*56)+47];
    C[(l_n*56)+27] += A[460] * B[(l_n*56)+47];
    C[(l_n*56)+28] += A[461] * B[(l_n*56)+47];
    C[(l_n*56)+46] += A[462] * B[(l_n*56)+47];
    C[(l_n*56)+47] += A[463] * B[(l_n*56)+47];
    C[(l_n*56)+48] += A[464] * B[(l_n*56)+47];
    C[(l_n*56)+49] += A[465] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a48_0 = _mm_loadu_ps(&A[466]);
    c48_0 = _mm_add_ps(c48_0, _mm_mul_ps(a48_0, b48));
    _mm_storeu_ps(&C[(l_n*56)+10], c48_0);
    __m128 c48_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a48_4 = _mm_loadu_ps(&A[470]);
    c48_4 = _mm_add_ps(c48_4, _mm_mul_ps(a48_4, b48));
    _mm_storeu_ps(&C[(l_n*56)+25], c48_4);
    __m128 c48_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a48_8 = _mm_loadu_ps(&A[474]);
    c48_8 = _mm_add_ps(c48_8, _mm_mul_ps(a48_8, b48));
    _mm_storeu_ps(&C[(l_n*56)+46], c48_8);
#else
    C[(l_n*56)+10] += A[466] * B[(l_n*56)+48];
    C[(l_n*56)+11] += A[467] * B[(l_n*56)+48];
    C[(l_n*56)+12] += A[468] * B[(l_n*56)+48];
    C[(l_n*56)+13] += A[469] * B[(l_n*56)+48];
    C[(l_n*56)+25] += A[470] * B[(l_n*56)+48];
    C[(l_n*56)+26] += A[471] * B[(l_n*56)+48];
    C[(l_n*56)+27] += A[472] * B[(l_n*56)+48];
    C[(l_n*56)+28] += A[473] * B[(l_n*56)+48];
    C[(l_n*56)+46] += A[474] * B[(l_n*56)+48];
    C[(l_n*56)+47] += A[475] * B[(l_n*56)+48];
    C[(l_n*56)+48] += A[476] * B[(l_n*56)+48];
    C[(l_n*56)+49] += A[477] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*56)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_loadu_ps(&C[(l_n*56)+10]);
    __m128 a49_0 = _mm_loadu_ps(&A[478]);
    c49_0 = _mm_add_ps(c49_0, _mm_mul_ps(a49_0, b49));
    _mm_storeu_ps(&C[(l_n*56)+10], c49_0);
    __m128 c49_4 = _mm_loadu_ps(&C[(l_n*56)+25]);
    __m128 a49_4 = _mm_loadu_ps(&A[482]);
    c49_4 = _mm_add_ps(c49_4, _mm_mul_ps(a49_4, b49));
    _mm_storeu_ps(&C[(l_n*56)+25], c49_4);
    __m128 c49_8 = _mm_loadu_ps(&C[(l_n*56)+46]);
    __m128 a49_8 = _mm_loadu_ps(&A[486]);
    c49_8 = _mm_add_ps(c49_8, _mm_mul_ps(a49_8, b49));
    _mm_storeu_ps(&C[(l_n*56)+46], c49_8);
#else
    C[(l_n*56)+10] += A[478] * B[(l_n*56)+49];
    C[(l_n*56)+11] += A[479] * B[(l_n*56)+49];
    C[(l_n*56)+12] += A[480] * B[(l_n*56)+49];
    C[(l_n*56)+13] += A[481] * B[(l_n*56)+49];
    C[(l_n*56)+25] += A[482] * B[(l_n*56)+49];
    C[(l_n*56)+26] += A[483] * B[(l_n*56)+49];
    C[(l_n*56)+27] += A[484] * B[(l_n*56)+49];
    C[(l_n*56)+28] += A[485] * B[(l_n*56)+49];
    C[(l_n*56)+46] += A[486] * B[(l_n*56)+49];
    C[(l_n*56)+47] += A[487] * B[(l_n*56)+49];
    C[(l_n*56)+48] += A[488] * B[(l_n*56)+49];
    C[(l_n*56)+49] += A[489] * B[(l_n*56)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a50_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[490]));
    c50_0 = _mm_add_ps(c50_0, _mm_mul_ps(a50_0, b50));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c50_0));
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a50_2 = _mm_load_ss(&A[492]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+6], c50_2);
    __m128 c50_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a50_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[493]));
    c50_3 = _mm_add_ps(c50_3, _mm_mul_ps(a50_3, b50));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c50_3));
    __m128 c50_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a50_5 = _mm_load_ss(&A[495]);
    c50_5 = _mm_add_ss(c50_5, _mm_mul_ss(a50_5, b50));
    _mm_store_ss(&C[(l_n*56)+16], c50_5);
    __m128 c50_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a50_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[496]));
    c50_6 = _mm_add_ps(c50_6, _mm_mul_ps(a50_6, b50));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c50_6));
    __m128 c50_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a50_8 = _mm_load_ss(&A[498]);
    c50_8 = _mm_add_ss(c50_8, _mm_mul_ss(a50_8, b50));
    _mm_store_ss(&C[(l_n*56)+31], c50_8);
    __m128 c50_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a50_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[499]));
    c50_9 = _mm_add_ps(c50_9, _mm_mul_ps(a50_9, b50));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c50_9));
    __m128 c50_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a50_11 = _mm_load_ss(&A[501]);
    c50_11 = _mm_add_ss(c50_11, _mm_mul_ss(a50_11, b50));
    _mm_store_ss(&C[(l_n*56)+52], c50_11);
#else
    C[(l_n*56)+4] += A[490] * B[(l_n*56)+50];
    C[(l_n*56)+5] += A[491] * B[(l_n*56)+50];
    C[(l_n*56)+6] += A[492] * B[(l_n*56)+50];
    C[(l_n*56)+14] += A[493] * B[(l_n*56)+50];
    C[(l_n*56)+15] += A[494] * B[(l_n*56)+50];
    C[(l_n*56)+16] += A[495] * B[(l_n*56)+50];
    C[(l_n*56)+29] += A[496] * B[(l_n*56)+50];
    C[(l_n*56)+30] += A[497] * B[(l_n*56)+50];
    C[(l_n*56)+31] += A[498] * B[(l_n*56)+50];
    C[(l_n*56)+50] += A[499] * B[(l_n*56)+50];
    C[(l_n*56)+51] += A[500] * B[(l_n*56)+50];
    C[(l_n*56)+52] += A[501] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a51_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[502]));
    c51_0 = _mm_add_ps(c51_0, _mm_mul_ps(a51_0, b51));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c51_0));
    __m128 c51_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a51_2 = _mm_load_ss(&A[504]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*56)+6], c51_2);
    __m128 c51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a51_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[505]));
    c51_3 = _mm_add_ps(c51_3, _mm_mul_ps(a51_3, b51));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c51_3));
    __m128 c51_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a51_5 = _mm_load_ss(&A[507]);
    c51_5 = _mm_add_ss(c51_5, _mm_mul_ss(a51_5, b51));
    _mm_store_ss(&C[(l_n*56)+16], c51_5);
    __m128 c51_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a51_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[508]));
    c51_6 = _mm_add_ps(c51_6, _mm_mul_ps(a51_6, b51));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c51_6));
    __m128 c51_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a51_8 = _mm_load_ss(&A[510]);
    c51_8 = _mm_add_ss(c51_8, _mm_mul_ss(a51_8, b51));
    _mm_store_ss(&C[(l_n*56)+31], c51_8);
    __m128 c51_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a51_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[511]));
    c51_9 = _mm_add_ps(c51_9, _mm_mul_ps(a51_9, b51));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c51_9));
    __m128 c51_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a51_11 = _mm_load_ss(&A[513]);
    c51_11 = _mm_add_ss(c51_11, _mm_mul_ss(a51_11, b51));
    _mm_store_ss(&C[(l_n*56)+52], c51_11);
#else
    C[(l_n*56)+4] += A[502] * B[(l_n*56)+51];
    C[(l_n*56)+5] += A[503] * B[(l_n*56)+51];
    C[(l_n*56)+6] += A[504] * B[(l_n*56)+51];
    C[(l_n*56)+14] += A[505] * B[(l_n*56)+51];
    C[(l_n*56)+15] += A[506] * B[(l_n*56)+51];
    C[(l_n*56)+16] += A[507] * B[(l_n*56)+51];
    C[(l_n*56)+29] += A[508] * B[(l_n*56)+51];
    C[(l_n*56)+30] += A[509] * B[(l_n*56)+51];
    C[(l_n*56)+31] += A[510] * B[(l_n*56)+51];
    C[(l_n*56)+50] += A[511] * B[(l_n*56)+51];
    C[(l_n*56)+51] += A[512] * B[(l_n*56)+51];
    C[(l_n*56)+52] += A[513] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*56)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+4]));
    __m128 a52_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[514]));
    c52_0 = _mm_add_ps(c52_0, _mm_mul_ps(a52_0, b52));
    _mm_store_sd((double*)&C[(l_n*56)+4], _mm_castps_pd(c52_0));
    __m128 c52_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a52_2 = _mm_load_ss(&A[516]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*56)+6], c52_2);
    __m128 c52_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+14]));
    __m128 a52_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[517]));
    c52_3 = _mm_add_ps(c52_3, _mm_mul_ps(a52_3, b52));
    _mm_store_sd((double*)&C[(l_n*56)+14], _mm_castps_pd(c52_3));
    __m128 c52_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a52_5 = _mm_load_ss(&A[519]);
    c52_5 = _mm_add_ss(c52_5, _mm_mul_ss(a52_5, b52));
    _mm_store_ss(&C[(l_n*56)+16], c52_5);
    __m128 c52_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+29]));
    __m128 a52_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[520]));
    c52_6 = _mm_add_ps(c52_6, _mm_mul_ps(a52_6, b52));
    _mm_store_sd((double*)&C[(l_n*56)+29], _mm_castps_pd(c52_6));
    __m128 c52_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a52_8 = _mm_load_ss(&A[522]);
    c52_8 = _mm_add_ss(c52_8, _mm_mul_ss(a52_8, b52));
    _mm_store_ss(&C[(l_n*56)+31], c52_8);
    __m128 c52_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+50]));
    __m128 a52_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[523]));
    c52_9 = _mm_add_ps(c52_9, _mm_mul_ps(a52_9, b52));
    _mm_store_sd((double*)&C[(l_n*56)+50], _mm_castps_pd(c52_9));
    __m128 c52_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a52_11 = _mm_load_ss(&A[525]);
    c52_11 = _mm_add_ss(c52_11, _mm_mul_ss(a52_11, b52));
    _mm_store_ss(&C[(l_n*56)+52], c52_11);
#else
    C[(l_n*56)+4] += A[514] * B[(l_n*56)+52];
    C[(l_n*56)+5] += A[515] * B[(l_n*56)+52];
    C[(l_n*56)+6] += A[516] * B[(l_n*56)+52];
    C[(l_n*56)+14] += A[517] * B[(l_n*56)+52];
    C[(l_n*56)+15] += A[518] * B[(l_n*56)+52];
    C[(l_n*56)+16] += A[519] * B[(l_n*56)+52];
    C[(l_n*56)+29] += A[520] * B[(l_n*56)+52];
    C[(l_n*56)+30] += A[521] * B[(l_n*56)+52];
    C[(l_n*56)+31] += A[522] * B[(l_n*56)+52];
    C[(l_n*56)+50] += A[523] * B[(l_n*56)+52];
    C[(l_n*56)+51] += A[524] * B[(l_n*56)+52];
    C[(l_n*56)+52] += A[525] * B[(l_n*56)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a53_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[526]));
    c53_0 = _mm_add_ps(c53_0, _mm_mul_ps(a53_0, b53));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c53_0));
    __m128 c53_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a53_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[528]));
    c53_2 = _mm_add_ps(c53_2, _mm_mul_ps(a53_2, b53));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c53_2));
    __m128 c53_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a53_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[530]));
    c53_4 = _mm_add_ps(c53_4, _mm_mul_ps(a53_4, b53));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c53_4));
    __m128 c53_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a53_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[532]));
    c53_6 = _mm_add_ps(c53_6, _mm_mul_ps(a53_6, b53));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c53_6));
    __m128 c53_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a53_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[534]));
    c53_8 = _mm_add_ps(c53_8, _mm_mul_ps(a53_8, b53));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c53_8));
#else
    C[(l_n*56)+1] += A[526] * B[(l_n*56)+53];
    C[(l_n*56)+2] += A[527] * B[(l_n*56)+53];
    C[(l_n*56)+7] += A[528] * B[(l_n*56)+53];
    C[(l_n*56)+8] += A[529] * B[(l_n*56)+53];
    C[(l_n*56)+17] += A[530] * B[(l_n*56)+53];
    C[(l_n*56)+18] += A[531] * B[(l_n*56)+53];
    C[(l_n*56)+32] += A[532] * B[(l_n*56)+53];
    C[(l_n*56)+33] += A[533] * B[(l_n*56)+53];
    C[(l_n*56)+53] += A[534] * B[(l_n*56)+53];
    C[(l_n*56)+54] += A[535] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*56)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+1]));
    __m128 a54_0 = _mm_castpd_ps(_mm_load_sd((const double*)&A[536]));
    c54_0 = _mm_add_ps(c54_0, _mm_mul_ps(a54_0, b54));
    _mm_store_sd((double*)&C[(l_n*56)+1], _mm_castps_pd(c54_0));
    __m128 c54_2 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+7]));
    __m128 a54_2 = _mm_castpd_ps(_mm_load_sd((const double*)&A[538]));
    c54_2 = _mm_add_ps(c54_2, _mm_mul_ps(a54_2, b54));
    _mm_store_sd((double*)&C[(l_n*56)+7], _mm_castps_pd(c54_2));
    __m128 c54_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+17]));
    __m128 a54_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[540]));
    c54_4 = _mm_add_ps(c54_4, _mm_mul_ps(a54_4, b54));
    _mm_store_sd((double*)&C[(l_n*56)+17], _mm_castps_pd(c54_4));
    __m128 c54_6 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+32]));
    __m128 a54_6 = _mm_castpd_ps(_mm_load_sd((const double*)&A[542]));
    c54_6 = _mm_add_ps(c54_6, _mm_mul_ps(a54_6, b54));
    _mm_store_sd((double*)&C[(l_n*56)+32], _mm_castps_pd(c54_6));
    __m128 c54_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+53]));
    __m128 a54_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[544]));
    c54_8 = _mm_add_ps(c54_8, _mm_mul_ps(a54_8, b54));
    _mm_store_sd((double*)&C[(l_n*56)+53], _mm_castps_pd(c54_8));
#else
    C[(l_n*56)+1] += A[536] * B[(l_n*56)+54];
    C[(l_n*56)+2] += A[537] * B[(l_n*56)+54];
    C[(l_n*56)+7] += A[538] * B[(l_n*56)+54];
    C[(l_n*56)+8] += A[539] * B[(l_n*56)+54];
    C[(l_n*56)+17] += A[540] * B[(l_n*56)+54];
    C[(l_n*56)+18] += A[541] * B[(l_n*56)+54];
    C[(l_n*56)+32] += A[542] * B[(l_n*56)+54];
    C[(l_n*56)+33] += A[543] * B[(l_n*56)+54];
    C[(l_n*56)+53] += A[544] * B[(l_n*56)+54];
    C[(l_n*56)+54] += A[545] * B[(l_n*56)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*56)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a55_0 = _mm_load_ss(&A[546]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*56)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a55_1 = _mm_load_ss(&A[547]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*56)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a55_2 = _mm_load_ss(&A[548]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*56)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a55_3 = _mm_load_ss(&A[549]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*56)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a55_4 = _mm_load_ss(&A[550]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*56)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a55_5 = _mm_load_ss(&A[551]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*56)+55], c55_5);
#else
    C[(l_n*56)+0] += A[546] * B[(l_n*56)+55];
    C[(l_n*56)+3] += A[547] * B[(l_n*56)+55];
    C[(l_n*56)+9] += A[548] * B[(l_n*56)+55];
    C[(l_n*56)+19] += A[549] * B[(l_n*56)+55];
    C[(l_n*56)+34] += A[550] * B[(l_n*56)+55];
    C[(l_n*56)+55] += A[551] * B[(l_n*56)+55];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 9936;
#endif
}

void ssparse_fP222DivM_m56_n9_k56_ldAna6_ldB56_ldC56_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 56; l_m++) {
      C[(l_n*56)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*56)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*56)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*56)+0], c0_0);
    __m128 c0_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a0_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[1]));
    c0_1 = _mm_add_ps(c0_1, _mm_mul_ps(a0_1, b0));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c0_1));
    __m128 c0_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*56)+6], c0_3);
    __m128 c0_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a0_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[4]));
    c0_4 = _mm_add_ps(c0_4, _mm_mul_ps(a0_4, b0));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c0_4));
    __m128 c0_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a0_6 = _mm_load_ss(&A[6]);
    c0_6 = _mm_add_ss(c0_6, _mm_mul_ss(a0_6, b0));
    _mm_store_ss(&C[(l_n*56)+13], c0_6);
    __m128 c0_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a0_7 = _mm_load_ss(&A[7]);
    c0_7 = _mm_add_ss(c0_7, _mm_mul_ss(a0_7, b0));
    _mm_store_ss(&C[(l_n*56)+16], c0_7);
    __m128 c0_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a0_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[8]));
    c0_8 = _mm_add_ps(c0_8, _mm_mul_ps(a0_8, b0));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c0_8));
    __m128 c0_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a0_10 = _mm_load_ss(&A[10]);
    c0_10 = _mm_add_ss(c0_10, _mm_mul_ss(a0_10, b0));
    _mm_store_ss(&C[(l_n*56)+24], c0_10);
    __m128 c0_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a0_11 = _mm_load_ss(&A[11]);
    c0_11 = _mm_add_ss(c0_11, _mm_mul_ss(a0_11, b0));
    _mm_store_ss(&C[(l_n*56)+28], c0_11);
    __m128 c0_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a0_12 = _mm_load_ss(&A[12]);
    c0_12 = _mm_add_ss(c0_12, _mm_mul_ss(a0_12, b0));
    _mm_store_ss(&C[(l_n*56)+31], c0_12);
    __m128 c0_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a0_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[13]));
    c0_13 = _mm_add_ps(c0_13, _mm_mul_ps(a0_13, b0));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c0_13));
    __m128 c0_15 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a0_15 = _mm_load_ss(&A[15]);
    c0_15 = _mm_add_ss(c0_15, _mm_mul_ss(a0_15, b0));
    _mm_store_ss(&C[(l_n*56)+40], c0_15);
    __m128 c0_16 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a0_16 = _mm_load_ss(&A[16]);
    c0_16 = _mm_add_ss(c0_16, _mm_mul_ss(a0_16, b0));
    _mm_store_ss(&C[(l_n*56)+45], c0_16);
    __m128 c0_17 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a0_17 = _mm_load_ss(&A[17]);
    c0_17 = _mm_add_ss(c0_17, _mm_mul_ss(a0_17, b0));
    _mm_store_ss(&C[(l_n*56)+49], c0_17);
    __m128 c0_18 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a0_18 = _mm_load_ss(&A[18]);
    c0_18 = _mm_add_ss(c0_18, _mm_mul_ss(a0_18, b0));
    _mm_store_ss(&C[(l_n*56)+52], c0_18);
    __m128 c0_19 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a0_19 = _mm_castpd_ps(_mm_load_sd((const double*)&A[19]));
    c0_19 = _mm_add_ps(c0_19, _mm_mul_ps(a0_19, b0));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c0_19));
#else
    C[(l_n*56)+0] += A[0] * B[(l_n*56)+0];
    C[(l_n*56)+2] += A[1] * B[(l_n*56)+0];
    C[(l_n*56)+3] += A[2] * B[(l_n*56)+0];
    C[(l_n*56)+6] += A[3] * B[(l_n*56)+0];
    C[(l_n*56)+8] += A[4] * B[(l_n*56)+0];
    C[(l_n*56)+9] += A[5] * B[(l_n*56)+0];
    C[(l_n*56)+13] += A[6] * B[(l_n*56)+0];
    C[(l_n*56)+16] += A[7] * B[(l_n*56)+0];
    C[(l_n*56)+18] += A[8] * B[(l_n*56)+0];
    C[(l_n*56)+19] += A[9] * B[(l_n*56)+0];
    C[(l_n*56)+24] += A[10] * B[(l_n*56)+0];
    C[(l_n*56)+28] += A[11] * B[(l_n*56)+0];
    C[(l_n*56)+31] += A[12] * B[(l_n*56)+0];
    C[(l_n*56)+33] += A[13] * B[(l_n*56)+0];
    C[(l_n*56)+34] += A[14] * B[(l_n*56)+0];
    C[(l_n*56)+40] += A[15] * B[(l_n*56)+0];
    C[(l_n*56)+45] += A[16] * B[(l_n*56)+0];
    C[(l_n*56)+49] += A[17] * B[(l_n*56)+0];
    C[(l_n*56)+52] += A[18] * B[(l_n*56)+0];
    C[(l_n*56)+54] += A[19] * B[(l_n*56)+0];
    C[(l_n*56)+55] += A[20] * B[(l_n*56)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*56)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*56)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a1_0 = _mm_load_ss(&A[21]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*56)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a1_1 = _mm_load_ss(&A[22]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*56)+5], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a1_2 = _mm_load_ss(&A[23]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*56)+7], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a1_3 = _mm_load_ss(&A[24]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*56)+12], c1_3);
    __m128 c1_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a1_4 = _mm_load_ss(&A[25]);
    c1_4 = _mm_add_ss(c1_4, _mm_mul_ss(a1_4, b1));
    _mm_store_ss(&C[(l_n*56)+15], c1_4);
    __m128 c1_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a1_5 = _mm_load_ss(&A[26]);
    c1_5 = _mm_add_ss(c1_5, _mm_mul_ss(a1_5, b1));
    _mm_store_ss(&C[(l_n*56)+17], c1_5);
    __m128 c1_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a1_6 = _mm_load_ss(&A[27]);
    c1_6 = _mm_add_ss(c1_6, _mm_mul_ss(a1_6, b1));
    _mm_store_ss(&C[(l_n*56)+23], c1_6);
    __m128 c1_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a1_7 = _mm_load_ss(&A[28]);
    c1_7 = _mm_add_ss(c1_7, _mm_mul_ss(a1_7, b1));
    _mm_store_ss(&C[(l_n*56)+27], c1_7);
    __m128 c1_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a1_8 = _mm_load_ss(&A[29]);
    c1_8 = _mm_add_ss(c1_8, _mm_mul_ss(a1_8, b1));
    _mm_store_ss(&C[(l_n*56)+30], c1_8);
    __m128 c1_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a1_9 = _mm_load_ss(&A[30]);
    c1_9 = _mm_add_ss(c1_9, _mm_mul_ss(a1_9, b1));
    _mm_store_ss(&C[(l_n*56)+32], c1_9);
    __m128 c1_10 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a1_10 = _mm_load_ss(&A[31]);
    c1_10 = _mm_add_ss(c1_10, _mm_mul_ss(a1_10, b1));
    _mm_store_ss(&C[(l_n*56)+39], c1_10);
    __m128 c1_11 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a1_11 = _mm_load_ss(&A[32]);
    c1_11 = _mm_add_ss(c1_11, _mm_mul_ss(a1_11, b1));
    _mm_store_ss(&C[(l_n*56)+44], c1_11);
    __m128 c1_12 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a1_12 = _mm_load_ss(&A[33]);
    c1_12 = _mm_add_ss(c1_12, _mm_mul_ss(a1_12, b1));
    _mm_store_ss(&C[(l_n*56)+48], c1_12);
    __m128 c1_13 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a1_13 = _mm_load_ss(&A[34]);
    c1_13 = _mm_add_ss(c1_13, _mm_mul_ss(a1_13, b1));
    _mm_store_ss(&C[(l_n*56)+51], c1_13);
    __m128 c1_14 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a1_14 = _mm_load_ss(&A[35]);
    c1_14 = _mm_add_ss(c1_14, _mm_mul_ss(a1_14, b1));
    _mm_store_ss(&C[(l_n*56)+53], c1_14);
#else
    C[(l_n*56)+1] += A[21] * B[(l_n*56)+1];
    C[(l_n*56)+5] += A[22] * B[(l_n*56)+1];
    C[(l_n*56)+7] += A[23] * B[(l_n*56)+1];
    C[(l_n*56)+12] += A[24] * B[(l_n*56)+1];
    C[(l_n*56)+15] += A[25] * B[(l_n*56)+1];
    C[(l_n*56)+17] += A[26] * B[(l_n*56)+1];
    C[(l_n*56)+23] += A[27] * B[(l_n*56)+1];
    C[(l_n*56)+27] += A[28] * B[(l_n*56)+1];
    C[(l_n*56)+30] += A[29] * B[(l_n*56)+1];
    C[(l_n*56)+32] += A[30] * B[(l_n*56)+1];
    C[(l_n*56)+39] += A[31] * B[(l_n*56)+1];
    C[(l_n*56)+44] += A[32] * B[(l_n*56)+1];
    C[(l_n*56)+48] += A[33] * B[(l_n*56)+1];
    C[(l_n*56)+51] += A[34] * B[(l_n*56)+1];
    C[(l_n*56)+53] += A[35] * B[(l_n*56)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*56)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*56)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a2_0 = _mm_load_ss(&A[36]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*56)+0], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*56)+2]);
    __m128 a2_1 = _mm_load_ss(&A[37]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*56)+2], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a2_2 = _mm_load_ss(&A[38]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*56)+6], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a2_3 = _mm_load_ss(&A[39]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*56)+8], c2_3);
    __m128 c2_4 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a2_4 = _mm_load_ss(&A[40]);
    c2_4 = _mm_add_ss(c2_4, _mm_mul_ss(a2_4, b2));
    _mm_store_ss(&C[(l_n*56)+13], c2_4);
    __m128 c2_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a2_5 = _mm_load_ss(&A[41]);
    c2_5 = _mm_add_ss(c2_5, _mm_mul_ss(a2_5, b2));
    _mm_store_ss(&C[(l_n*56)+16], c2_5);
    __m128 c2_6 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a2_6 = _mm_load_ss(&A[42]);
    c2_6 = _mm_add_ss(c2_6, _mm_mul_ss(a2_6, b2));
    _mm_store_ss(&C[(l_n*56)+18], c2_6);
    __m128 c2_7 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a2_7 = _mm_load_ss(&A[43]);
    c2_7 = _mm_add_ss(c2_7, _mm_mul_ss(a2_7, b2));
    _mm_store_ss(&C[(l_n*56)+24], c2_7);
    __m128 c2_8 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a2_8 = _mm_load_ss(&A[44]);
    c2_8 = _mm_add_ss(c2_8, _mm_mul_ss(a2_8, b2));
    _mm_store_ss(&C[(l_n*56)+28], c2_8);
    __m128 c2_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a2_9 = _mm_load_ss(&A[45]);
    c2_9 = _mm_add_ss(c2_9, _mm_mul_ss(a2_9, b2));
    _mm_store_ss(&C[(l_n*56)+31], c2_9);
    __m128 c2_10 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a2_10 = _mm_load_ss(&A[46]);
    c2_10 = _mm_add_ss(c2_10, _mm_mul_ss(a2_10, b2));
    _mm_store_ss(&C[(l_n*56)+33], c2_10);
    __m128 c2_11 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a2_11 = _mm_load_ss(&A[47]);
    c2_11 = _mm_add_ss(c2_11, _mm_mul_ss(a2_11, b2));
    _mm_store_ss(&C[(l_n*56)+40], c2_11);
    __m128 c2_12 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a2_12 = _mm_load_ss(&A[48]);
    c2_12 = _mm_add_ss(c2_12, _mm_mul_ss(a2_12, b2));
    _mm_store_ss(&C[(l_n*56)+45], c2_12);
    __m128 c2_13 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a2_13 = _mm_load_ss(&A[49]);
    c2_13 = _mm_add_ss(c2_13, _mm_mul_ss(a2_13, b2));
    _mm_store_ss(&C[(l_n*56)+49], c2_13);
    __m128 c2_14 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a2_14 = _mm_load_ss(&A[50]);
    c2_14 = _mm_add_ss(c2_14, _mm_mul_ss(a2_14, b2));
    _mm_store_ss(&C[(l_n*56)+52], c2_14);
    __m128 c2_15 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a2_15 = _mm_load_ss(&A[51]);
    c2_15 = _mm_add_ss(c2_15, _mm_mul_ss(a2_15, b2));
    _mm_store_ss(&C[(l_n*56)+54], c2_15);
#else
    C[(l_n*56)+0] += A[36] * B[(l_n*56)+2];
    C[(l_n*56)+2] += A[37] * B[(l_n*56)+2];
    C[(l_n*56)+6] += A[38] * B[(l_n*56)+2];
    C[(l_n*56)+8] += A[39] * B[(l_n*56)+2];
    C[(l_n*56)+13] += A[40] * B[(l_n*56)+2];
    C[(l_n*56)+16] += A[41] * B[(l_n*56)+2];
    C[(l_n*56)+18] += A[42] * B[(l_n*56)+2];
    C[(l_n*56)+24] += A[43] * B[(l_n*56)+2];
    C[(l_n*56)+28] += A[44] * B[(l_n*56)+2];
    C[(l_n*56)+31] += A[45] * B[(l_n*56)+2];
    C[(l_n*56)+33] += A[46] * B[(l_n*56)+2];
    C[(l_n*56)+40] += A[47] * B[(l_n*56)+2];
    C[(l_n*56)+45] += A[48] * B[(l_n*56)+2];
    C[(l_n*56)+49] += A[49] * B[(l_n*56)+2];
    C[(l_n*56)+52] += A[50] * B[(l_n*56)+2];
    C[(l_n*56)+54] += A[51] * B[(l_n*56)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*56)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*56)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a3_0 = _mm_load_ss(&A[52]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*56)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a3_1 = _mm_load_ss(&A[53]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*56)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a3_2 = _mm_load_ss(&A[54]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*56)+6], c3_2);
    __m128 c3_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a3_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[55]));
    c3_3 = _mm_add_ps(c3_3, _mm_mul_ps(a3_3, b3));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c3_3));
    __m128 c3_5 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a3_5 = _mm_load_ss(&A[57]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*56)+13], c3_5);
    __m128 c3_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a3_6 = _mm_load_ss(&A[58]);
    c3_6 = _mm_add_ss(c3_6, _mm_mul_ss(a3_6, b3));
    _mm_store_ss(&C[(l_n*56)+16], c3_6);
    __m128 c3_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a3_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[59]));
    c3_7 = _mm_add_ps(c3_7, _mm_mul_ps(a3_7, b3));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c3_7));
    __m128 c3_9 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a3_9 = _mm_load_ss(&A[61]);
    c3_9 = _mm_add_ss(c3_9, _mm_mul_ss(a3_9, b3));
    _mm_store_ss(&C[(l_n*56)+24], c3_9);
    __m128 c3_10 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a3_10 = _mm_load_ss(&A[62]);
    c3_10 = _mm_add_ss(c3_10, _mm_mul_ss(a3_10, b3));
    _mm_store_ss(&C[(l_n*56)+28], c3_10);
    __m128 c3_11 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a3_11 = _mm_load_ss(&A[63]);
    c3_11 = _mm_add_ss(c3_11, _mm_mul_ss(a3_11, b3));
    _mm_store_ss(&C[(l_n*56)+31], c3_11);
    __m128 c3_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a3_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[64]));
    c3_12 = _mm_add_ps(c3_12, _mm_mul_ps(a3_12, b3));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c3_12));
    __m128 c3_14 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a3_14 = _mm_load_ss(&A[66]);
    c3_14 = _mm_add_ss(c3_14, _mm_mul_ss(a3_14, b3));
    _mm_store_ss(&C[(l_n*56)+40], c3_14);
    __m128 c3_15 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a3_15 = _mm_load_ss(&A[67]);
    c3_15 = _mm_add_ss(c3_15, _mm_mul_ss(a3_15, b3));
    _mm_store_ss(&C[(l_n*56)+45], c3_15);
    __m128 c3_16 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a3_16 = _mm_load_ss(&A[68]);
    c3_16 = _mm_add_ss(c3_16, _mm_mul_ss(a3_16, b3));
    _mm_store_ss(&C[(l_n*56)+49], c3_16);
    __m128 c3_17 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a3_17 = _mm_load_ss(&A[69]);
    c3_17 = _mm_add_ss(c3_17, _mm_mul_ss(a3_17, b3));
    _mm_store_ss(&C[(l_n*56)+52], c3_17);
    __m128 c3_18 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a3_18 = _mm_castpd_ps(_mm_load_sd((const double*)&A[70]));
    c3_18 = _mm_add_ps(c3_18, _mm_mul_ps(a3_18, b3));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c3_18));
#else
    C[(l_n*56)+0] += A[52] * B[(l_n*56)+3];
    C[(l_n*56)+3] += A[53] * B[(l_n*56)+3];
    C[(l_n*56)+6] += A[54] * B[(l_n*56)+3];
    C[(l_n*56)+8] += A[55] * B[(l_n*56)+3];
    C[(l_n*56)+9] += A[56] * B[(l_n*56)+3];
    C[(l_n*56)+13] += A[57] * B[(l_n*56)+3];
    C[(l_n*56)+16] += A[58] * B[(l_n*56)+3];
    C[(l_n*56)+18] += A[59] * B[(l_n*56)+3];
    C[(l_n*56)+19] += A[60] * B[(l_n*56)+3];
    C[(l_n*56)+24] += A[61] * B[(l_n*56)+3];
    C[(l_n*56)+28] += A[62] * B[(l_n*56)+3];
    C[(l_n*56)+31] += A[63] * B[(l_n*56)+3];
    C[(l_n*56)+33] += A[64] * B[(l_n*56)+3];
    C[(l_n*56)+34] += A[65] * B[(l_n*56)+3];
    C[(l_n*56)+40] += A[66] * B[(l_n*56)+3];
    C[(l_n*56)+45] += A[67] * B[(l_n*56)+3];
    C[(l_n*56)+49] += A[68] * B[(l_n*56)+3];
    C[(l_n*56)+52] += A[69] * B[(l_n*56)+3];
    C[(l_n*56)+54] += A[70] * B[(l_n*56)+3];
    C[(l_n*56)+55] += A[71] * B[(l_n*56)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*56)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*56)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a4_0 = _mm_load_ss(&A[72]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*56)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a4_1 = _mm_load_ss(&A[73]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*56)+11], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a4_2 = _mm_load_ss(&A[74]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*56)+14], c4_2);
    __m128 c4_3 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a4_3 = _mm_load_ss(&A[75]);
    c4_3 = _mm_add_ss(c4_3, _mm_mul_ss(a4_3, b4));
    _mm_store_ss(&C[(l_n*56)+22], c4_3);
    __m128 c4_4 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a4_4 = _mm_load_ss(&A[76]);
    c4_4 = _mm_add_ss(c4_4, _mm_mul_ss(a4_4, b4));
    _mm_store_ss(&C[(l_n*56)+26], c4_4);
    __m128 c4_5 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a4_5 = _mm_load_ss(&A[77]);
    c4_5 = _mm_add_ss(c4_5, _mm_mul_ss(a4_5, b4));
    _mm_store_ss(&C[(l_n*56)+29], c4_5);
    __m128 c4_6 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a4_6 = _mm_load_ss(&A[78]);
    c4_6 = _mm_add_ss(c4_6, _mm_mul_ss(a4_6, b4));
    _mm_store_ss(&C[(l_n*56)+38], c4_6);
    __m128 c4_7 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a4_7 = _mm_load_ss(&A[79]);
    c4_7 = _mm_add_ss(c4_7, _mm_mul_ss(a4_7, b4));
    _mm_store_ss(&C[(l_n*56)+43], c4_7);
    __m128 c4_8 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a4_8 = _mm_load_ss(&A[80]);
    c4_8 = _mm_add_ss(c4_8, _mm_mul_ss(a4_8, b4));
    _mm_store_ss(&C[(l_n*56)+47], c4_8);
    __m128 c4_9 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a4_9 = _mm_load_ss(&A[81]);
    c4_9 = _mm_add_ss(c4_9, _mm_mul_ss(a4_9, b4));
    _mm_store_ss(&C[(l_n*56)+50], c4_9);
#else
    C[(l_n*56)+4] += A[72] * B[(l_n*56)+4];
    C[(l_n*56)+11] += A[73] * B[(l_n*56)+4];
    C[(l_n*56)+14] += A[74] * B[(l_n*56)+4];
    C[(l_n*56)+22] += A[75] * B[(l_n*56)+4];
    C[(l_n*56)+26] += A[76] * B[(l_n*56)+4];
    C[(l_n*56)+29] += A[77] * B[(l_n*56)+4];
    C[(l_n*56)+38] += A[78] * B[(l_n*56)+4];
    C[(l_n*56)+43] += A[79] * B[(l_n*56)+4];
    C[(l_n*56)+47] += A[80] * B[(l_n*56)+4];
    C[(l_n*56)+50] += A[81] * B[(l_n*56)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*56)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*56)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a5_0 = _mm_load_ss(&A[82]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*56)+1], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a5_1 = _mm_load_ss(&A[83]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*56)+5], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a5_2 = _mm_load_ss(&A[84]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*56)+12], c5_2);
    __m128 c5_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a5_3 = _mm_load_ss(&A[85]);
    c5_3 = _mm_add_ss(c5_3, _mm_mul_ss(a5_3, b5));
    _mm_store_ss(&C[(l_n*56)+15], c5_3);
    __m128 c5_4 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a5_4 = _mm_load_ss(&A[86]);
    c5_4 = _mm_add_ss(c5_4, _mm_mul_ss(a5_4, b5));
    _mm_store_ss(&C[(l_n*56)+23], c5_4);
    __m128 c5_5 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a5_5 = _mm_load_ss(&A[87]);
    c5_5 = _mm_add_ss(c5_5, _mm_mul_ss(a5_5, b5));
    _mm_store_ss(&C[(l_n*56)+27], c5_5);
    __m128 c5_6 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a5_6 = _mm_load_ss(&A[88]);
    c5_6 = _mm_add_ss(c5_6, _mm_mul_ss(a5_6, b5));
    _mm_store_ss(&C[(l_n*56)+30], c5_6);
    __m128 c5_7 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a5_7 = _mm_load_ss(&A[89]);
    c5_7 = _mm_add_ss(c5_7, _mm_mul_ss(a5_7, b5));
    _mm_store_ss(&C[(l_n*56)+39], c5_7);
    __m128 c5_8 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a5_8 = _mm_load_ss(&A[90]);
    c5_8 = _mm_add_ss(c5_8, _mm_mul_ss(a5_8, b5));
    _mm_store_ss(&C[(l_n*56)+44], c5_8);
    __m128 c5_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a5_9 = _mm_load_ss(&A[91]);
    c5_9 = _mm_add_ss(c5_9, _mm_mul_ss(a5_9, b5));
    _mm_store_ss(&C[(l_n*56)+48], c5_9);
    __m128 c5_10 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a5_10 = _mm_load_ss(&A[92]);
    c5_10 = _mm_add_ss(c5_10, _mm_mul_ss(a5_10, b5));
    _mm_store_ss(&C[(l_n*56)+51], c5_10);
#else
    C[(l_n*56)+1] += A[82] * B[(l_n*56)+5];
    C[(l_n*56)+5] += A[83] * B[(l_n*56)+5];
    C[(l_n*56)+12] += A[84] * B[(l_n*56)+5];
    C[(l_n*56)+15] += A[85] * B[(l_n*56)+5];
    C[(l_n*56)+23] += A[86] * B[(l_n*56)+5];
    C[(l_n*56)+27] += A[87] * B[(l_n*56)+5];
    C[(l_n*56)+30] += A[88] * B[(l_n*56)+5];
    C[(l_n*56)+39] += A[89] * B[(l_n*56)+5];
    C[(l_n*56)+44] += A[90] * B[(l_n*56)+5];
    C[(l_n*56)+48] += A[91] * B[(l_n*56)+5];
    C[(l_n*56)+51] += A[92] * B[(l_n*56)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*56)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*56)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a6_0 = _mm_load_ss(&A[93]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*56)+0], c6_0);
    __m128 c6_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a6_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[94]));
    c6_1 = _mm_add_ps(c6_1, _mm_mul_ps(a6_1, b6));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c6_1));
    __m128 c6_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a6_3 = _mm_load_ss(&A[96]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*56)+6], c6_3);
    __m128 c6_4 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a6_4 = _mm_load_ss(&A[97]);
    c6_4 = _mm_add_ss(c6_4, _mm_mul_ss(a6_4, b6));
    _mm_store_ss(&C[(l_n*56)+13], c6_4);
    __m128 c6_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a6_5 = _mm_load_ss(&A[98]);
    c6_5 = _mm_add_ss(c6_5, _mm_mul_ss(a6_5, b6));
    _mm_store_ss(&C[(l_n*56)+16], c6_5);
    __m128 c6_6 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a6_6 = _mm_load_ss(&A[99]);
    c6_6 = _mm_add_ss(c6_6, _mm_mul_ss(a6_6, b6));
    _mm_store_ss(&C[(l_n*56)+24], c6_6);
    __m128 c6_7 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a6_7 = _mm_load_ss(&A[100]);
    c6_7 = _mm_add_ss(c6_7, _mm_mul_ss(a6_7, b6));
    _mm_store_ss(&C[(l_n*56)+28], c6_7);
    __m128 c6_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a6_8 = _mm_load_ss(&A[101]);
    c6_8 = _mm_add_ss(c6_8, _mm_mul_ss(a6_8, b6));
    _mm_store_ss(&C[(l_n*56)+31], c6_8);
    __m128 c6_9 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a6_9 = _mm_load_ss(&A[102]);
    c6_9 = _mm_add_ss(c6_9, _mm_mul_ss(a6_9, b6));
    _mm_store_ss(&C[(l_n*56)+40], c6_9);
    __m128 c6_10 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a6_10 = _mm_load_ss(&A[103]);
    c6_10 = _mm_add_ss(c6_10, _mm_mul_ss(a6_10, b6));
    _mm_store_ss(&C[(l_n*56)+45], c6_10);
    __m128 c6_11 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a6_11 = _mm_load_ss(&A[104]);
    c6_11 = _mm_add_ss(c6_11, _mm_mul_ss(a6_11, b6));
    _mm_store_ss(&C[(l_n*56)+49], c6_11);
    __m128 c6_12 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a6_12 = _mm_load_ss(&A[105]);
    c6_12 = _mm_add_ss(c6_12, _mm_mul_ss(a6_12, b6));
    _mm_store_ss(&C[(l_n*56)+52], c6_12);
#else
    C[(l_n*56)+0] += A[93] * B[(l_n*56)+6];
    C[(l_n*56)+2] += A[94] * B[(l_n*56)+6];
    C[(l_n*56)+3] += A[95] * B[(l_n*56)+6];
    C[(l_n*56)+6] += A[96] * B[(l_n*56)+6];
    C[(l_n*56)+13] += A[97] * B[(l_n*56)+6];
    C[(l_n*56)+16] += A[98] * B[(l_n*56)+6];
    C[(l_n*56)+24] += A[99] * B[(l_n*56)+6];
    C[(l_n*56)+28] += A[100] * B[(l_n*56)+6];
    C[(l_n*56)+31] += A[101] * B[(l_n*56)+6];
    C[(l_n*56)+40] += A[102] * B[(l_n*56)+6];
    C[(l_n*56)+45] += A[103] * B[(l_n*56)+6];
    C[(l_n*56)+49] += A[104] * B[(l_n*56)+6];
    C[(l_n*56)+52] += A[105] * B[(l_n*56)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*56)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*56)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a7_0 = _mm_load_ss(&A[106]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*56)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a7_1 = _mm_load_ss(&A[107]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*56)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a7_2 = _mm_load_ss(&A[108]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*56)+12], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a7_3 = _mm_load_ss(&A[109]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*56)+15], c7_3);
    __m128 c7_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a7_4 = _mm_load_ss(&A[110]);
    c7_4 = _mm_add_ss(c7_4, _mm_mul_ss(a7_4, b7));
    _mm_store_ss(&C[(l_n*56)+17], c7_4);
    __m128 c7_5 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a7_5 = _mm_load_ss(&A[111]);
    c7_5 = _mm_add_ss(c7_5, _mm_mul_ss(a7_5, b7));
    _mm_store_ss(&C[(l_n*56)+23], c7_5);
    __m128 c7_6 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a7_6 = _mm_load_ss(&A[112]);
    c7_6 = _mm_add_ss(c7_6, _mm_mul_ss(a7_6, b7));
    _mm_store_ss(&C[(l_n*56)+27], c7_6);
    __m128 c7_7 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a7_7 = _mm_load_ss(&A[113]);
    c7_7 = _mm_add_ss(c7_7, _mm_mul_ss(a7_7, b7));
    _mm_store_ss(&C[(l_n*56)+30], c7_7);
    __m128 c7_8 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a7_8 = _mm_load_ss(&A[114]);
    c7_8 = _mm_add_ss(c7_8, _mm_mul_ss(a7_8, b7));
    _mm_store_ss(&C[(l_n*56)+32], c7_8);
    __m128 c7_9 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a7_9 = _mm_load_ss(&A[115]);
    c7_9 = _mm_add_ss(c7_9, _mm_mul_ss(a7_9, b7));
    _mm_store_ss(&C[(l_n*56)+39], c7_9);
    __m128 c7_10 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a7_10 = _mm_load_ss(&A[116]);
    c7_10 = _mm_add_ss(c7_10, _mm_mul_ss(a7_10, b7));
    _mm_store_ss(&C[(l_n*56)+44], c7_10);
    __m128 c7_11 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a7_11 = _mm_load_ss(&A[117]);
    c7_11 = _mm_add_ss(c7_11, _mm_mul_ss(a7_11, b7));
    _mm_store_ss(&C[(l_n*56)+48], c7_11);
    __m128 c7_12 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a7_12 = _mm_load_ss(&A[118]);
    c7_12 = _mm_add_ss(c7_12, _mm_mul_ss(a7_12, b7));
    _mm_store_ss(&C[(l_n*56)+51], c7_12);
    __m128 c7_13 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a7_13 = _mm_load_ss(&A[119]);
    c7_13 = _mm_add_ss(c7_13, _mm_mul_ss(a7_13, b7));
    _mm_store_ss(&C[(l_n*56)+53], c7_13);
#else
    C[(l_n*56)+1] += A[106] * B[(l_n*56)+7];
    C[(l_n*56)+7] += A[107] * B[(l_n*56)+7];
    C[(l_n*56)+12] += A[108] * B[(l_n*56)+7];
    C[(l_n*56)+15] += A[109] * B[(l_n*56)+7];
    C[(l_n*56)+17] += A[110] * B[(l_n*56)+7];
    C[(l_n*56)+23] += A[111] * B[(l_n*56)+7];
    C[(l_n*56)+27] += A[112] * B[(l_n*56)+7];
    C[(l_n*56)+30] += A[113] * B[(l_n*56)+7];
    C[(l_n*56)+32] += A[114] * B[(l_n*56)+7];
    C[(l_n*56)+39] += A[115] * B[(l_n*56)+7];
    C[(l_n*56)+44] += A[116] * B[(l_n*56)+7];
    C[(l_n*56)+48] += A[117] * B[(l_n*56)+7];
    C[(l_n*56)+51] += A[118] * B[(l_n*56)+7];
    C[(l_n*56)+53] += A[119] * B[(l_n*56)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*56)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*56)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a8_0 = _mm_load_ss(&A[120]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*56)+0], c8_0);
    __m128 c8_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a8_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[121]));
    c8_1 = _mm_add_ps(c8_1, _mm_mul_ps(a8_1, b8));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c8_1));
    __m128 c8_3 = _mm_load_ss(&C[(l_n*56)+8]);
    __m128 a8_3 = _mm_load_ss(&A[123]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*56)+8], c8_3);
    __m128 c8_4 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a8_4 = _mm_load_ss(&A[124]);
    c8_4 = _mm_add_ss(c8_4, _mm_mul_ss(a8_4, b8));
    _mm_store_ss(&C[(l_n*56)+13], c8_4);
    __m128 c8_5 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a8_5 = _mm_load_ss(&A[125]);
    c8_5 = _mm_add_ss(c8_5, _mm_mul_ss(a8_5, b8));
    _mm_store_ss(&C[(l_n*56)+16], c8_5);
    __m128 c8_6 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a8_6 = _mm_load_ss(&A[126]);
    c8_6 = _mm_add_ss(c8_6, _mm_mul_ss(a8_6, b8));
    _mm_store_ss(&C[(l_n*56)+18], c8_6);
    __m128 c8_7 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a8_7 = _mm_load_ss(&A[127]);
    c8_7 = _mm_add_ss(c8_7, _mm_mul_ss(a8_7, b8));
    _mm_store_ss(&C[(l_n*56)+24], c8_7);
    __m128 c8_8 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a8_8 = _mm_load_ss(&A[128]);
    c8_8 = _mm_add_ss(c8_8, _mm_mul_ss(a8_8, b8));
    _mm_store_ss(&C[(l_n*56)+28], c8_8);
    __m128 c8_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a8_9 = _mm_load_ss(&A[129]);
    c8_9 = _mm_add_ss(c8_9, _mm_mul_ss(a8_9, b8));
    _mm_store_ss(&C[(l_n*56)+31], c8_9);
    __m128 c8_10 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a8_10 = _mm_load_ss(&A[130]);
    c8_10 = _mm_add_ss(c8_10, _mm_mul_ss(a8_10, b8));
    _mm_store_ss(&C[(l_n*56)+33], c8_10);
    __m128 c8_11 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a8_11 = _mm_load_ss(&A[131]);
    c8_11 = _mm_add_ss(c8_11, _mm_mul_ss(a8_11, b8));
    _mm_store_ss(&C[(l_n*56)+40], c8_11);
    __m128 c8_12 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a8_12 = _mm_load_ss(&A[132]);
    c8_12 = _mm_add_ss(c8_12, _mm_mul_ss(a8_12, b8));
    _mm_store_ss(&C[(l_n*56)+45], c8_12);
    __m128 c8_13 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a8_13 = _mm_load_ss(&A[133]);
    c8_13 = _mm_add_ss(c8_13, _mm_mul_ss(a8_13, b8));
    _mm_store_ss(&C[(l_n*56)+49], c8_13);
    __m128 c8_14 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a8_14 = _mm_load_ss(&A[134]);
    c8_14 = _mm_add_ss(c8_14, _mm_mul_ss(a8_14, b8));
    _mm_store_ss(&C[(l_n*56)+52], c8_14);
    __m128 c8_15 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a8_15 = _mm_load_ss(&A[135]);
    c8_15 = _mm_add_ss(c8_15, _mm_mul_ss(a8_15, b8));
    _mm_store_ss(&C[(l_n*56)+54], c8_15);
#else
    C[(l_n*56)+0] += A[120] * B[(l_n*56)+8];
    C[(l_n*56)+2] += A[121] * B[(l_n*56)+8];
    C[(l_n*56)+3] += A[122] * B[(l_n*56)+8];
    C[(l_n*56)+8] += A[123] * B[(l_n*56)+8];
    C[(l_n*56)+13] += A[124] * B[(l_n*56)+8];
    C[(l_n*56)+16] += A[125] * B[(l_n*56)+8];
    C[(l_n*56)+18] += A[126] * B[(l_n*56)+8];
    C[(l_n*56)+24] += A[127] * B[(l_n*56)+8];
    C[(l_n*56)+28] += A[128] * B[(l_n*56)+8];
    C[(l_n*56)+31] += A[129] * B[(l_n*56)+8];
    C[(l_n*56)+33] += A[130] * B[(l_n*56)+8];
    C[(l_n*56)+40] += A[131] * B[(l_n*56)+8];
    C[(l_n*56)+45] += A[132] * B[(l_n*56)+8];
    C[(l_n*56)+49] += A[133] * B[(l_n*56)+8];
    C[(l_n*56)+52] += A[134] * B[(l_n*56)+8];
    C[(l_n*56)+54] += A[135] * B[(l_n*56)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*56)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*56)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a9_0 = _mm_load_ss(&A[136]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*56)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a9_1 = _mm_load_ss(&A[137]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*56)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a9_2 = _mm_load_ss(&A[138]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*56)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a9_3 = _mm_load_ss(&A[139]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*56)+13], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a9_4 = _mm_load_ss(&A[140]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*56)+16], c9_4);
    __m128 c9_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a9_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[141]));
    c9_5 = _mm_add_ps(c9_5, _mm_mul_ps(a9_5, b9));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c9_5));
    __m128 c9_7 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a9_7 = _mm_load_ss(&A[143]);
    c9_7 = _mm_add_ss(c9_7, _mm_mul_ss(a9_7, b9));
    _mm_store_ss(&C[(l_n*56)+24], c9_7);
    __m128 c9_8 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a9_8 = _mm_load_ss(&A[144]);
    c9_8 = _mm_add_ss(c9_8, _mm_mul_ss(a9_8, b9));
    _mm_store_ss(&C[(l_n*56)+28], c9_8);
    __m128 c9_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a9_9 = _mm_load_ss(&A[145]);
    c9_9 = _mm_add_ss(c9_9, _mm_mul_ss(a9_9, b9));
    _mm_store_ss(&C[(l_n*56)+31], c9_9);
    __m128 c9_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a9_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[146]));
    c9_10 = _mm_add_ps(c9_10, _mm_mul_ps(a9_10, b9));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c9_10));
    __m128 c9_12 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a9_12 = _mm_load_ss(&A[148]);
    c9_12 = _mm_add_ss(c9_12, _mm_mul_ss(a9_12, b9));
    _mm_store_ss(&C[(l_n*56)+40], c9_12);
    __m128 c9_13 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a9_13 = _mm_load_ss(&A[149]);
    c9_13 = _mm_add_ss(c9_13, _mm_mul_ss(a9_13, b9));
    _mm_store_ss(&C[(l_n*56)+45], c9_13);
    __m128 c9_14 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a9_14 = _mm_load_ss(&A[150]);
    c9_14 = _mm_add_ss(c9_14, _mm_mul_ss(a9_14, b9));
    _mm_store_ss(&C[(l_n*56)+49], c9_14);
    __m128 c9_15 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a9_15 = _mm_load_ss(&A[151]);
    c9_15 = _mm_add_ss(c9_15, _mm_mul_ss(a9_15, b9));
    _mm_store_ss(&C[(l_n*56)+52], c9_15);
    __m128 c9_16 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a9_16 = _mm_castpd_ps(_mm_load_sd((const double*)&A[152]));
    c9_16 = _mm_add_ps(c9_16, _mm_mul_ps(a9_16, b9));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c9_16));
#else
    C[(l_n*56)+0] += A[136] * B[(l_n*56)+9];
    C[(l_n*56)+3] += A[137] * B[(l_n*56)+9];
    C[(l_n*56)+9] += A[138] * B[(l_n*56)+9];
    C[(l_n*56)+13] += A[139] * B[(l_n*56)+9];
    C[(l_n*56)+16] += A[140] * B[(l_n*56)+9];
    C[(l_n*56)+18] += A[141] * B[(l_n*56)+9];
    C[(l_n*56)+19] += A[142] * B[(l_n*56)+9];
    C[(l_n*56)+24] += A[143] * B[(l_n*56)+9];
    C[(l_n*56)+28] += A[144] * B[(l_n*56)+9];
    C[(l_n*56)+31] += A[145] * B[(l_n*56)+9];
    C[(l_n*56)+33] += A[146] * B[(l_n*56)+9];
    C[(l_n*56)+34] += A[147] * B[(l_n*56)+9];
    C[(l_n*56)+40] += A[148] * B[(l_n*56)+9];
    C[(l_n*56)+45] += A[149] * B[(l_n*56)+9];
    C[(l_n*56)+49] += A[150] * B[(l_n*56)+9];
    C[(l_n*56)+52] += A[151] * B[(l_n*56)+9];
    C[(l_n*56)+54] += A[152] * B[(l_n*56)+9];
    C[(l_n*56)+55] += A[153] * B[(l_n*56)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*56)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*56)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a10_0 = _mm_load_ss(&A[154]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*56)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a10_1 = _mm_load_ss(&A[155]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*56)+21], c10_1);
    __m128 c10_2 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a10_2 = _mm_load_ss(&A[156]);
    c10_2 = _mm_add_ss(c10_2, _mm_mul_ss(a10_2, b10));
    _mm_store_ss(&C[(l_n*56)+25], c10_2);
    __m128 c10_3 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a10_3 = _mm_load_ss(&A[157]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*56)+37], c10_3);
    __m128 c10_4 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a10_4 = _mm_load_ss(&A[158]);
    c10_4 = _mm_add_ss(c10_4, _mm_mul_ss(a10_4, b10));
    _mm_store_ss(&C[(l_n*56)+42], c10_4);
    __m128 c10_5 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a10_5 = _mm_load_ss(&A[159]);
    c10_5 = _mm_add_ss(c10_5, _mm_mul_ss(a10_5, b10));
    _mm_store_ss(&C[(l_n*56)+46], c10_5);
#else
    C[(l_n*56)+10] += A[154] * B[(l_n*56)+10];
    C[(l_n*56)+21] += A[155] * B[(l_n*56)+10];
    C[(l_n*56)+25] += A[156] * B[(l_n*56)+10];
    C[(l_n*56)+37] += A[157] * B[(l_n*56)+10];
    C[(l_n*56)+42] += A[158] * B[(l_n*56)+10];
    C[(l_n*56)+46] += A[159] * B[(l_n*56)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*56)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*56)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a11_0 = _mm_load_ss(&A[160]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*56)+4], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a11_1 = _mm_load_ss(&A[161]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*56)+11], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a11_2 = _mm_load_ss(&A[162]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*56)+22], c11_2);
    __m128 c11_3 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a11_3 = _mm_load_ss(&A[163]);
    c11_3 = _mm_add_ss(c11_3, _mm_mul_ss(a11_3, b11));
    _mm_store_ss(&C[(l_n*56)+26], c11_3);
    __m128 c11_4 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a11_4 = _mm_load_ss(&A[164]);
    c11_4 = _mm_add_ss(c11_4, _mm_mul_ss(a11_4, b11));
    _mm_store_ss(&C[(l_n*56)+38], c11_4);
    __m128 c11_5 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a11_5 = _mm_load_ss(&A[165]);
    c11_5 = _mm_add_ss(c11_5, _mm_mul_ss(a11_5, b11));
    _mm_store_ss(&C[(l_n*56)+43], c11_5);
    __m128 c11_6 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a11_6 = _mm_load_ss(&A[166]);
    c11_6 = _mm_add_ss(c11_6, _mm_mul_ss(a11_6, b11));
    _mm_store_ss(&C[(l_n*56)+47], c11_6);
#else
    C[(l_n*56)+4] += A[160] * B[(l_n*56)+11];
    C[(l_n*56)+11] += A[161] * B[(l_n*56)+11];
    C[(l_n*56)+22] += A[162] * B[(l_n*56)+11];
    C[(l_n*56)+26] += A[163] * B[(l_n*56)+11];
    C[(l_n*56)+38] += A[164] * B[(l_n*56)+11];
    C[(l_n*56)+43] += A[165] * B[(l_n*56)+11];
    C[(l_n*56)+47] += A[166] * B[(l_n*56)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*56)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*56)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a12_0 = _mm_load_ss(&A[167]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*56)+1], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a12_1 = _mm_load_ss(&A[168]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*56)+5], c12_1);
    __m128 c12_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a12_2 = _mm_load_ss(&A[169]);
    c12_2 = _mm_add_ss(c12_2, _mm_mul_ss(a12_2, b12));
    _mm_store_ss(&C[(l_n*56)+7], c12_2);
    __m128 c12_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a12_3 = _mm_load_ss(&A[170]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*56)+12], c12_3);
    __m128 c12_4 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a12_4 = _mm_load_ss(&A[171]);
    c12_4 = _mm_add_ss(c12_4, _mm_mul_ss(a12_4, b12));
    _mm_store_ss(&C[(l_n*56)+23], c12_4);
    __m128 c12_5 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a12_5 = _mm_load_ss(&A[172]);
    c12_5 = _mm_add_ss(c12_5, _mm_mul_ss(a12_5, b12));
    _mm_store_ss(&C[(l_n*56)+27], c12_5);
    __m128 c12_6 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a12_6 = _mm_load_ss(&A[173]);
    c12_6 = _mm_add_ss(c12_6, _mm_mul_ss(a12_6, b12));
    _mm_store_ss(&C[(l_n*56)+39], c12_6);
    __m128 c12_7 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a12_7 = _mm_load_ss(&A[174]);
    c12_7 = _mm_add_ss(c12_7, _mm_mul_ss(a12_7, b12));
    _mm_store_ss(&C[(l_n*56)+44], c12_7);
    __m128 c12_8 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a12_8 = _mm_load_ss(&A[175]);
    c12_8 = _mm_add_ss(c12_8, _mm_mul_ss(a12_8, b12));
    _mm_store_ss(&C[(l_n*56)+48], c12_8);
#else
    C[(l_n*56)+1] += A[167] * B[(l_n*56)+12];
    C[(l_n*56)+5] += A[168] * B[(l_n*56)+12];
    C[(l_n*56)+7] += A[169] * B[(l_n*56)+12];
    C[(l_n*56)+12] += A[170] * B[(l_n*56)+12];
    C[(l_n*56)+23] += A[171] * B[(l_n*56)+12];
    C[(l_n*56)+27] += A[172] * B[(l_n*56)+12];
    C[(l_n*56)+39] += A[173] * B[(l_n*56)+12];
    C[(l_n*56)+44] += A[174] * B[(l_n*56)+12];
    C[(l_n*56)+48] += A[175] * B[(l_n*56)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*56)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*56)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a13_0 = _mm_load_ss(&A[176]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*56)+0], c13_0);
    __m128 c13_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a13_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[177]));
    c13_1 = _mm_add_ps(c13_1, _mm_mul_ps(a13_1, b13));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c13_1));
    __m128 c13_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a13_3 = _mm_load_ss(&A[179]);
    c13_3 = _mm_add_ss(c13_3, _mm_mul_ss(a13_3, b13));
    _mm_store_ss(&C[(l_n*56)+6], c13_3);
    __m128 c13_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a13_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[180]));
    c13_4 = _mm_add_ps(c13_4, _mm_mul_ps(a13_4, b13));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c13_4));
    __m128 c13_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a13_6 = _mm_load_ss(&A[182]);
    c13_6 = _mm_add_ss(c13_6, _mm_mul_ss(a13_6, b13));
    _mm_store_ss(&C[(l_n*56)+13], c13_6);
    __m128 c13_7 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a13_7 = _mm_load_ss(&A[183]);
    c13_7 = _mm_add_ss(c13_7, _mm_mul_ss(a13_7, b13));
    _mm_store_ss(&C[(l_n*56)+24], c13_7);
    __m128 c13_8 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a13_8 = _mm_load_ss(&A[184]);
    c13_8 = _mm_add_ss(c13_8, _mm_mul_ss(a13_8, b13));
    _mm_store_ss(&C[(l_n*56)+28], c13_8);
    __m128 c13_9 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a13_9 = _mm_load_ss(&A[185]);
    c13_9 = _mm_add_ss(c13_9, _mm_mul_ss(a13_9, b13));
    _mm_store_ss(&C[(l_n*56)+40], c13_9);
    __m128 c13_10 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a13_10 = _mm_load_ss(&A[186]);
    c13_10 = _mm_add_ss(c13_10, _mm_mul_ss(a13_10, b13));
    _mm_store_ss(&C[(l_n*56)+45], c13_10);
    __m128 c13_11 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a13_11 = _mm_load_ss(&A[187]);
    c13_11 = _mm_add_ss(c13_11, _mm_mul_ss(a13_11, b13));
    _mm_store_ss(&C[(l_n*56)+49], c13_11);
#else
    C[(l_n*56)+0] += A[176] * B[(l_n*56)+13];
    C[(l_n*56)+2] += A[177] * B[(l_n*56)+13];
    C[(l_n*56)+3] += A[178] * B[(l_n*56)+13];
    C[(l_n*56)+6] += A[179] * B[(l_n*56)+13];
    C[(l_n*56)+8] += A[180] * B[(l_n*56)+13];
    C[(l_n*56)+9] += A[181] * B[(l_n*56)+13];
    C[(l_n*56)+13] += A[182] * B[(l_n*56)+13];
    C[(l_n*56)+24] += A[183] * B[(l_n*56)+13];
    C[(l_n*56)+28] += A[184] * B[(l_n*56)+13];
    C[(l_n*56)+40] += A[185] * B[(l_n*56)+13];
    C[(l_n*56)+45] += A[186] * B[(l_n*56)+13];
    C[(l_n*56)+49] += A[187] * B[(l_n*56)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*56)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*56)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a14_0 = _mm_load_ss(&A[188]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*56)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a14_1 = _mm_load_ss(&A[189]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*56)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a14_2 = _mm_load_ss(&A[190]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*56)+22], c14_2);
    __m128 c14_3 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a14_3 = _mm_load_ss(&A[191]);
    c14_3 = _mm_add_ss(c14_3, _mm_mul_ss(a14_3, b14));
    _mm_store_ss(&C[(l_n*56)+26], c14_3);
    __m128 c14_4 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a14_4 = _mm_load_ss(&A[192]);
    c14_4 = _mm_add_ss(c14_4, _mm_mul_ss(a14_4, b14));
    _mm_store_ss(&C[(l_n*56)+29], c14_4);
    __m128 c14_5 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a14_5 = _mm_load_ss(&A[193]);
    c14_5 = _mm_add_ss(c14_5, _mm_mul_ss(a14_5, b14));
    _mm_store_ss(&C[(l_n*56)+38], c14_5);
    __m128 c14_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a14_6 = _mm_load_ss(&A[194]);
    c14_6 = _mm_add_ss(c14_6, _mm_mul_ss(a14_6, b14));
    _mm_store_ss(&C[(l_n*56)+43], c14_6);
    __m128 c14_7 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a14_7 = _mm_load_ss(&A[195]);
    c14_7 = _mm_add_ss(c14_7, _mm_mul_ss(a14_7, b14));
    _mm_store_ss(&C[(l_n*56)+47], c14_7);
    __m128 c14_8 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a14_8 = _mm_load_ss(&A[196]);
    c14_8 = _mm_add_ss(c14_8, _mm_mul_ss(a14_8, b14));
    _mm_store_ss(&C[(l_n*56)+50], c14_8);
#else
    C[(l_n*56)+4] += A[188] * B[(l_n*56)+14];
    C[(l_n*56)+14] += A[189] * B[(l_n*56)+14];
    C[(l_n*56)+22] += A[190] * B[(l_n*56)+14];
    C[(l_n*56)+26] += A[191] * B[(l_n*56)+14];
    C[(l_n*56)+29] += A[192] * B[(l_n*56)+14];
    C[(l_n*56)+38] += A[193] * B[(l_n*56)+14];
    C[(l_n*56)+43] += A[194] * B[(l_n*56)+14];
    C[(l_n*56)+47] += A[195] * B[(l_n*56)+14];
    C[(l_n*56)+50] += A[196] * B[(l_n*56)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*56)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*56)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a15_0 = _mm_load_ss(&A[197]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*56)+1], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a15_1 = _mm_load_ss(&A[198]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*56)+5], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a15_2 = _mm_load_ss(&A[199]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*56)+7], c15_2);
    __m128 c15_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a15_3 = _mm_load_ss(&A[200]);
    c15_3 = _mm_add_ss(c15_3, _mm_mul_ss(a15_3, b15));
    _mm_store_ss(&C[(l_n*56)+15], c15_3);
    __m128 c15_4 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a15_4 = _mm_load_ss(&A[201]);
    c15_4 = _mm_add_ss(c15_4, _mm_mul_ss(a15_4, b15));
    _mm_store_ss(&C[(l_n*56)+23], c15_4);
    __m128 c15_5 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a15_5 = _mm_load_ss(&A[202]);
    c15_5 = _mm_add_ss(c15_5, _mm_mul_ss(a15_5, b15));
    _mm_store_ss(&C[(l_n*56)+27], c15_5);
    __m128 c15_6 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a15_6 = _mm_load_ss(&A[203]);
    c15_6 = _mm_add_ss(c15_6, _mm_mul_ss(a15_6, b15));
    _mm_store_ss(&C[(l_n*56)+30], c15_6);
    __m128 c15_7 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a15_7 = _mm_load_ss(&A[204]);
    c15_7 = _mm_add_ss(c15_7, _mm_mul_ss(a15_7, b15));
    _mm_store_ss(&C[(l_n*56)+39], c15_7);
    __m128 c15_8 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a15_8 = _mm_load_ss(&A[205]);
    c15_8 = _mm_add_ss(c15_8, _mm_mul_ss(a15_8, b15));
    _mm_store_ss(&C[(l_n*56)+44], c15_8);
    __m128 c15_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a15_9 = _mm_load_ss(&A[206]);
    c15_9 = _mm_add_ss(c15_9, _mm_mul_ss(a15_9, b15));
    _mm_store_ss(&C[(l_n*56)+48], c15_9);
    __m128 c15_10 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a15_10 = _mm_load_ss(&A[207]);
    c15_10 = _mm_add_ss(c15_10, _mm_mul_ss(a15_10, b15));
    _mm_store_ss(&C[(l_n*56)+51], c15_10);
#else
    C[(l_n*56)+1] += A[197] * B[(l_n*56)+15];
    C[(l_n*56)+5] += A[198] * B[(l_n*56)+15];
    C[(l_n*56)+7] += A[199] * B[(l_n*56)+15];
    C[(l_n*56)+15] += A[200] * B[(l_n*56)+15];
    C[(l_n*56)+23] += A[201] * B[(l_n*56)+15];
    C[(l_n*56)+27] += A[202] * B[(l_n*56)+15];
    C[(l_n*56)+30] += A[203] * B[(l_n*56)+15];
    C[(l_n*56)+39] += A[204] * B[(l_n*56)+15];
    C[(l_n*56)+44] += A[205] * B[(l_n*56)+15];
    C[(l_n*56)+48] += A[206] * B[(l_n*56)+15];
    C[(l_n*56)+51] += A[207] * B[(l_n*56)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*56)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*56)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a16_0 = _mm_load_ss(&A[208]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*56)+0], c16_0);
    __m128 c16_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a16_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[209]));
    c16_1 = _mm_add_ps(c16_1, _mm_mul_ps(a16_1, b16));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c16_1));
    __m128 c16_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a16_3 = _mm_load_ss(&A[211]);
    c16_3 = _mm_add_ss(c16_3, _mm_mul_ss(a16_3, b16));
    _mm_store_ss(&C[(l_n*56)+6], c16_3);
    __m128 c16_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a16_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[212]));
    c16_4 = _mm_add_ps(c16_4, _mm_mul_ps(a16_4, b16));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c16_4));
    __m128 c16_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a16_6 = _mm_load_ss(&A[214]);
    c16_6 = _mm_add_ss(c16_6, _mm_mul_ss(a16_6, b16));
    _mm_store_ss(&C[(l_n*56)+16], c16_6);
    __m128 c16_7 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a16_7 = _mm_load_ss(&A[215]);
    c16_7 = _mm_add_ss(c16_7, _mm_mul_ss(a16_7, b16));
    _mm_store_ss(&C[(l_n*56)+24], c16_7);
    __m128 c16_8 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a16_8 = _mm_load_ss(&A[216]);
    c16_8 = _mm_add_ss(c16_8, _mm_mul_ss(a16_8, b16));
    _mm_store_ss(&C[(l_n*56)+28], c16_8);
    __m128 c16_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a16_9 = _mm_load_ss(&A[217]);
    c16_9 = _mm_add_ss(c16_9, _mm_mul_ss(a16_9, b16));
    _mm_store_ss(&C[(l_n*56)+31], c16_9);
    __m128 c16_10 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a16_10 = _mm_load_ss(&A[218]);
    c16_10 = _mm_add_ss(c16_10, _mm_mul_ss(a16_10, b16));
    _mm_store_ss(&C[(l_n*56)+40], c16_10);
    __m128 c16_11 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a16_11 = _mm_load_ss(&A[219]);
    c16_11 = _mm_add_ss(c16_11, _mm_mul_ss(a16_11, b16));
    _mm_store_ss(&C[(l_n*56)+45], c16_11);
    __m128 c16_12 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a16_12 = _mm_load_ss(&A[220]);
    c16_12 = _mm_add_ss(c16_12, _mm_mul_ss(a16_12, b16));
    _mm_store_ss(&C[(l_n*56)+49], c16_12);
    __m128 c16_13 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a16_13 = _mm_load_ss(&A[221]);
    c16_13 = _mm_add_ss(c16_13, _mm_mul_ss(a16_13, b16));
    _mm_store_ss(&C[(l_n*56)+52], c16_13);
#else
    C[(l_n*56)+0] += A[208] * B[(l_n*56)+16];
    C[(l_n*56)+2] += A[209] * B[(l_n*56)+16];
    C[(l_n*56)+3] += A[210] * B[(l_n*56)+16];
    C[(l_n*56)+6] += A[211] * B[(l_n*56)+16];
    C[(l_n*56)+8] += A[212] * B[(l_n*56)+16];
    C[(l_n*56)+9] += A[213] * B[(l_n*56)+16];
    C[(l_n*56)+16] += A[214] * B[(l_n*56)+16];
    C[(l_n*56)+24] += A[215] * B[(l_n*56)+16];
    C[(l_n*56)+28] += A[216] * B[(l_n*56)+16];
    C[(l_n*56)+31] += A[217] * B[(l_n*56)+16];
    C[(l_n*56)+40] += A[218] * B[(l_n*56)+16];
    C[(l_n*56)+45] += A[219] * B[(l_n*56)+16];
    C[(l_n*56)+49] += A[220] * B[(l_n*56)+16];
    C[(l_n*56)+52] += A[221] * B[(l_n*56)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*56)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*56)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a17_0 = _mm_load_ss(&A[222]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*56)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a17_1 = _mm_load_ss(&A[223]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*56)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a17_2 = _mm_load_ss(&A[224]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*56)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a17_3 = _mm_load_ss(&A[225]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*56)+23], c17_3);
    __m128 c17_4 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a17_4 = _mm_load_ss(&A[226]);
    c17_4 = _mm_add_ss(c17_4, _mm_mul_ss(a17_4, b17));
    _mm_store_ss(&C[(l_n*56)+27], c17_4);
    __m128 c17_5 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a17_5 = _mm_load_ss(&A[227]);
    c17_5 = _mm_add_ss(c17_5, _mm_mul_ss(a17_5, b17));
    _mm_store_ss(&C[(l_n*56)+30], c17_5);
    __m128 c17_6 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a17_6 = _mm_load_ss(&A[228]);
    c17_6 = _mm_add_ss(c17_6, _mm_mul_ss(a17_6, b17));
    _mm_store_ss(&C[(l_n*56)+32], c17_6);
    __m128 c17_7 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a17_7 = _mm_load_ss(&A[229]);
    c17_7 = _mm_add_ss(c17_7, _mm_mul_ss(a17_7, b17));
    _mm_store_ss(&C[(l_n*56)+39], c17_7);
    __m128 c17_8 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a17_8 = _mm_load_ss(&A[230]);
    c17_8 = _mm_add_ss(c17_8, _mm_mul_ss(a17_8, b17));
    _mm_store_ss(&C[(l_n*56)+44], c17_8);
    __m128 c17_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a17_9 = _mm_load_ss(&A[231]);
    c17_9 = _mm_add_ss(c17_9, _mm_mul_ss(a17_9, b17));
    _mm_store_ss(&C[(l_n*56)+48], c17_9);
    __m128 c17_10 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a17_10 = _mm_load_ss(&A[232]);
    c17_10 = _mm_add_ss(c17_10, _mm_mul_ss(a17_10, b17));
    _mm_store_ss(&C[(l_n*56)+51], c17_10);
    __m128 c17_11 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a17_11 = _mm_load_ss(&A[233]);
    c17_11 = _mm_add_ss(c17_11, _mm_mul_ss(a17_11, b17));
    _mm_store_ss(&C[(l_n*56)+53], c17_11);
#else
    C[(l_n*56)+1] += A[222] * B[(l_n*56)+17];
    C[(l_n*56)+7] += A[223] * B[(l_n*56)+17];
    C[(l_n*56)+17] += A[224] * B[(l_n*56)+17];
    C[(l_n*56)+23] += A[225] * B[(l_n*56)+17];
    C[(l_n*56)+27] += A[226] * B[(l_n*56)+17];
    C[(l_n*56)+30] += A[227] * B[(l_n*56)+17];
    C[(l_n*56)+32] += A[228] * B[(l_n*56)+17];
    C[(l_n*56)+39] += A[229] * B[(l_n*56)+17];
    C[(l_n*56)+44] += A[230] * B[(l_n*56)+17];
    C[(l_n*56)+48] += A[231] * B[(l_n*56)+17];
    C[(l_n*56)+51] += A[232] * B[(l_n*56)+17];
    C[(l_n*56)+53] += A[233] * B[(l_n*56)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*56)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*56)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a18_0 = _mm_load_ss(&A[234]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*56)+0], c18_0);
    __m128 c18_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a18_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[235]));
    c18_1 = _mm_add_ps(c18_1, _mm_mul_ps(a18_1, b18));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c18_1));
    __m128 c18_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a18_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[237]));
    c18_3 = _mm_add_ps(c18_3, _mm_mul_ps(a18_3, b18));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c18_3));
    __m128 c18_5 = _mm_load_ss(&C[(l_n*56)+18]);
    __m128 a18_5 = _mm_load_ss(&A[239]);
    c18_5 = _mm_add_ss(c18_5, _mm_mul_ss(a18_5, b18));
    _mm_store_ss(&C[(l_n*56)+18], c18_5);
    __m128 c18_6 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a18_6 = _mm_load_ss(&A[240]);
    c18_6 = _mm_add_ss(c18_6, _mm_mul_ss(a18_6, b18));
    _mm_store_ss(&C[(l_n*56)+24], c18_6);
    __m128 c18_7 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a18_7 = _mm_load_ss(&A[241]);
    c18_7 = _mm_add_ss(c18_7, _mm_mul_ss(a18_7, b18));
    _mm_store_ss(&C[(l_n*56)+28], c18_7);
    __m128 c18_8 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a18_8 = _mm_load_ss(&A[242]);
    c18_8 = _mm_add_ss(c18_8, _mm_mul_ss(a18_8, b18));
    _mm_store_ss(&C[(l_n*56)+31], c18_8);
    __m128 c18_9 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a18_9 = _mm_load_ss(&A[243]);
    c18_9 = _mm_add_ss(c18_9, _mm_mul_ss(a18_9, b18));
    _mm_store_ss(&C[(l_n*56)+33], c18_9);
    __m128 c18_10 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a18_10 = _mm_load_ss(&A[244]);
    c18_10 = _mm_add_ss(c18_10, _mm_mul_ss(a18_10, b18));
    _mm_store_ss(&C[(l_n*56)+40], c18_10);
    __m128 c18_11 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a18_11 = _mm_load_ss(&A[245]);
    c18_11 = _mm_add_ss(c18_11, _mm_mul_ss(a18_11, b18));
    _mm_store_ss(&C[(l_n*56)+45], c18_11);
    __m128 c18_12 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a18_12 = _mm_load_ss(&A[246]);
    c18_12 = _mm_add_ss(c18_12, _mm_mul_ss(a18_12, b18));
    _mm_store_ss(&C[(l_n*56)+49], c18_12);
    __m128 c18_13 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a18_13 = _mm_load_ss(&A[247]);
    c18_13 = _mm_add_ss(c18_13, _mm_mul_ss(a18_13, b18));
    _mm_store_ss(&C[(l_n*56)+52], c18_13);
    __m128 c18_14 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a18_14 = _mm_load_ss(&A[248]);
    c18_14 = _mm_add_ss(c18_14, _mm_mul_ss(a18_14, b18));
    _mm_store_ss(&C[(l_n*56)+54], c18_14);
#else
    C[(l_n*56)+0] += A[234] * B[(l_n*56)+18];
    C[(l_n*56)+2] += A[235] * B[(l_n*56)+18];
    C[(l_n*56)+3] += A[236] * B[(l_n*56)+18];
    C[(l_n*56)+8] += A[237] * B[(l_n*56)+18];
    C[(l_n*56)+9] += A[238] * B[(l_n*56)+18];
    C[(l_n*56)+18] += A[239] * B[(l_n*56)+18];
    C[(l_n*56)+24] += A[240] * B[(l_n*56)+18];
    C[(l_n*56)+28] += A[241] * B[(l_n*56)+18];
    C[(l_n*56)+31] += A[242] * B[(l_n*56)+18];
    C[(l_n*56)+33] += A[243] * B[(l_n*56)+18];
    C[(l_n*56)+40] += A[244] * B[(l_n*56)+18];
    C[(l_n*56)+45] += A[245] * B[(l_n*56)+18];
    C[(l_n*56)+49] += A[246] * B[(l_n*56)+18];
    C[(l_n*56)+52] += A[247] * B[(l_n*56)+18];
    C[(l_n*56)+54] += A[248] * B[(l_n*56)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*56)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*56)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a19_0 = _mm_load_ss(&A[249]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*56)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a19_1 = _mm_load_ss(&A[250]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*56)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a19_2 = _mm_load_ss(&A[251]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*56)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a19_3 = _mm_load_ss(&A[252]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*56)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a19_4 = _mm_load_ss(&A[253]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*56)+24], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a19_5 = _mm_load_ss(&A[254]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*56)+28], c19_5);
    __m128 c19_6 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a19_6 = _mm_load_ss(&A[255]);
    c19_6 = _mm_add_ss(c19_6, _mm_mul_ss(a19_6, b19));
    _mm_store_ss(&C[(l_n*56)+31], c19_6);
    __m128 c19_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a19_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[256]));
    c19_7 = _mm_add_ps(c19_7, _mm_mul_ps(a19_7, b19));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c19_7));
    __m128 c19_9 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a19_9 = _mm_load_ss(&A[258]);
    c19_9 = _mm_add_ss(c19_9, _mm_mul_ss(a19_9, b19));
    _mm_store_ss(&C[(l_n*56)+40], c19_9);
    __m128 c19_10 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a19_10 = _mm_load_ss(&A[259]);
    c19_10 = _mm_add_ss(c19_10, _mm_mul_ss(a19_10, b19));
    _mm_store_ss(&C[(l_n*56)+45], c19_10);
    __m128 c19_11 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a19_11 = _mm_load_ss(&A[260]);
    c19_11 = _mm_add_ss(c19_11, _mm_mul_ss(a19_11, b19));
    _mm_store_ss(&C[(l_n*56)+49], c19_11);
    __m128 c19_12 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a19_12 = _mm_load_ss(&A[261]);
    c19_12 = _mm_add_ss(c19_12, _mm_mul_ss(a19_12, b19));
    _mm_store_ss(&C[(l_n*56)+52], c19_12);
    __m128 c19_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a19_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[262]));
    c19_13 = _mm_add_ps(c19_13, _mm_mul_ps(a19_13, b19));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c19_13));
#else
    C[(l_n*56)+0] += A[249] * B[(l_n*56)+19];
    C[(l_n*56)+3] += A[250] * B[(l_n*56)+19];
    C[(l_n*56)+9] += A[251] * B[(l_n*56)+19];
    C[(l_n*56)+19] += A[252] * B[(l_n*56)+19];
    C[(l_n*56)+24] += A[253] * B[(l_n*56)+19];
    C[(l_n*56)+28] += A[254] * B[(l_n*56)+19];
    C[(l_n*56)+31] += A[255] * B[(l_n*56)+19];
    C[(l_n*56)+33] += A[256] * B[(l_n*56)+19];
    C[(l_n*56)+34] += A[257] * B[(l_n*56)+19];
    C[(l_n*56)+40] += A[258] * B[(l_n*56)+19];
    C[(l_n*56)+45] += A[259] * B[(l_n*56)+19];
    C[(l_n*56)+49] += A[260] * B[(l_n*56)+19];
    C[(l_n*56)+52] += A[261] * B[(l_n*56)+19];
    C[(l_n*56)+54] += A[262] * B[(l_n*56)+19];
    C[(l_n*56)+55] += A[263] * B[(l_n*56)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*56)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*56)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a20_0 = _mm_load_ss(&A[264]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*56)+20], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*56)+36]);
    __m128 a20_1 = _mm_load_ss(&A[265]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*56)+36], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a20_2 = _mm_load_ss(&A[266]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*56)+41], c20_2);
#else
    C[(l_n*56)+20] += A[264] * B[(l_n*56)+20];
    C[(l_n*56)+36] += A[265] * B[(l_n*56)+20];
    C[(l_n*56)+41] += A[266] * B[(l_n*56)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*56)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*56)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a21_0 = _mm_load_ss(&A[267]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*56)+10], c21_0);
    __m128 c21_1 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a21_1 = _mm_load_ss(&A[268]);
    c21_1 = _mm_add_ss(c21_1, _mm_mul_ss(a21_1, b21));
    _mm_store_ss(&C[(l_n*56)+21], c21_1);
    __m128 c21_2 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a21_2 = _mm_load_ss(&A[269]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*56)+37], c21_2);
    __m128 c21_3 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a21_3 = _mm_load_ss(&A[270]);
    c21_3 = _mm_add_ss(c21_3, _mm_mul_ss(a21_3, b21));
    _mm_store_ss(&C[(l_n*56)+42], c21_3);
#else
    C[(l_n*56)+10] += A[267] * B[(l_n*56)+21];
    C[(l_n*56)+21] += A[268] * B[(l_n*56)+21];
    C[(l_n*56)+37] += A[269] * B[(l_n*56)+21];
    C[(l_n*56)+42] += A[270] * B[(l_n*56)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*56)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*56)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a22_0 = _mm_load_ss(&A[271]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*56)+4], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a22_1 = _mm_load_ss(&A[272]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*56)+11], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a22_2 = _mm_load_ss(&A[273]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*56)+14], c22_2);
    __m128 c22_3 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a22_3 = _mm_load_ss(&A[274]);
    c22_3 = _mm_add_ss(c22_3, _mm_mul_ss(a22_3, b22));
    _mm_store_ss(&C[(l_n*56)+22], c22_3);
    __m128 c22_4 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a22_4 = _mm_load_ss(&A[275]);
    c22_4 = _mm_add_ss(c22_4, _mm_mul_ss(a22_4, b22));
    _mm_store_ss(&C[(l_n*56)+38], c22_4);
    __m128 c22_5 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a22_5 = _mm_load_ss(&A[276]);
    c22_5 = _mm_add_ss(c22_5, _mm_mul_ss(a22_5, b22));
    _mm_store_ss(&C[(l_n*56)+43], c22_5);
#else
    C[(l_n*56)+4] += A[271] * B[(l_n*56)+22];
    C[(l_n*56)+11] += A[272] * B[(l_n*56)+22];
    C[(l_n*56)+14] += A[273] * B[(l_n*56)+22];
    C[(l_n*56)+22] += A[274] * B[(l_n*56)+22];
    C[(l_n*56)+38] += A[275] * B[(l_n*56)+22];
    C[(l_n*56)+43] += A[276] * B[(l_n*56)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*56)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*56)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a23_0 = _mm_load_ss(&A[277]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*56)+1], c23_0);
    __m128 c23_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a23_1 = _mm_load_ss(&A[278]);
    c23_1 = _mm_add_ss(c23_1, _mm_mul_ss(a23_1, b23));
    _mm_store_ss(&C[(l_n*56)+5], c23_1);
    __m128 c23_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a23_2 = _mm_load_ss(&A[279]);
    c23_2 = _mm_add_ss(c23_2, _mm_mul_ss(a23_2, b23));
    _mm_store_ss(&C[(l_n*56)+7], c23_2);
    __m128 c23_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a23_3 = _mm_load_ss(&A[280]);
    c23_3 = _mm_add_ss(c23_3, _mm_mul_ss(a23_3, b23));
    _mm_store_ss(&C[(l_n*56)+12], c23_3);
    __m128 c23_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a23_4 = _mm_load_ss(&A[281]);
    c23_4 = _mm_add_ss(c23_4, _mm_mul_ss(a23_4, b23));
    _mm_store_ss(&C[(l_n*56)+15], c23_4);
    __m128 c23_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a23_5 = _mm_load_ss(&A[282]);
    c23_5 = _mm_add_ss(c23_5, _mm_mul_ss(a23_5, b23));
    _mm_store_ss(&C[(l_n*56)+17], c23_5);
    __m128 c23_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a23_6 = _mm_load_ss(&A[283]);
    c23_6 = _mm_add_ss(c23_6, _mm_mul_ss(a23_6, b23));
    _mm_store_ss(&C[(l_n*56)+23], c23_6);
    __m128 c23_7 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a23_7 = _mm_load_ss(&A[284]);
    c23_7 = _mm_add_ss(c23_7, _mm_mul_ss(a23_7, b23));
    _mm_store_ss(&C[(l_n*56)+39], c23_7);
    __m128 c23_8 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a23_8 = _mm_load_ss(&A[285]);
    c23_8 = _mm_add_ss(c23_8, _mm_mul_ss(a23_8, b23));
    _mm_store_ss(&C[(l_n*56)+44], c23_8);
#else
    C[(l_n*56)+1] += A[277] * B[(l_n*56)+23];
    C[(l_n*56)+5] += A[278] * B[(l_n*56)+23];
    C[(l_n*56)+7] += A[279] * B[(l_n*56)+23];
    C[(l_n*56)+12] += A[280] * B[(l_n*56)+23];
    C[(l_n*56)+15] += A[281] * B[(l_n*56)+23];
    C[(l_n*56)+17] += A[282] * B[(l_n*56)+23];
    C[(l_n*56)+23] += A[283] * B[(l_n*56)+23];
    C[(l_n*56)+39] += A[284] * B[(l_n*56)+23];
    C[(l_n*56)+44] += A[285] * B[(l_n*56)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*56)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*56)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a24_0 = _mm_load_ss(&A[286]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*56)+0], c24_0);
    __m128 c24_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a24_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[287]));
    c24_1 = _mm_add_ps(c24_1, _mm_mul_ps(a24_1, b24));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c24_1));
    __m128 c24_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a24_3 = _mm_load_ss(&A[289]);
    c24_3 = _mm_add_ss(c24_3, _mm_mul_ss(a24_3, b24));
    _mm_store_ss(&C[(l_n*56)+6], c24_3);
    __m128 c24_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a24_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[290]));
    c24_4 = _mm_add_ps(c24_4, _mm_mul_ps(a24_4, b24));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c24_4));
    __m128 c24_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a24_6 = _mm_load_ss(&A[292]);
    c24_6 = _mm_add_ss(c24_6, _mm_mul_ss(a24_6, b24));
    _mm_store_ss(&C[(l_n*56)+13], c24_6);
    __m128 c24_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a24_7 = _mm_load_ss(&A[293]);
    c24_7 = _mm_add_ss(c24_7, _mm_mul_ss(a24_7, b24));
    _mm_store_ss(&C[(l_n*56)+16], c24_7);
    __m128 c24_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a24_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[294]));
    c24_8 = _mm_add_ps(c24_8, _mm_mul_ps(a24_8, b24));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c24_8));
    __m128 c24_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a24_10 = _mm_load_ss(&A[296]);
    c24_10 = _mm_add_ss(c24_10, _mm_mul_ss(a24_10, b24));
    _mm_store_ss(&C[(l_n*56)+24], c24_10);
    __m128 c24_11 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a24_11 = _mm_load_ss(&A[297]);
    c24_11 = _mm_add_ss(c24_11, _mm_mul_ss(a24_11, b24));
    _mm_store_ss(&C[(l_n*56)+40], c24_11);
    __m128 c24_12 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a24_12 = _mm_load_ss(&A[298]);
    c24_12 = _mm_add_ss(c24_12, _mm_mul_ss(a24_12, b24));
    _mm_store_ss(&C[(l_n*56)+45], c24_12);
#else
    C[(l_n*56)+0] += A[286] * B[(l_n*56)+24];
    C[(l_n*56)+2] += A[287] * B[(l_n*56)+24];
    C[(l_n*56)+3] += A[288] * B[(l_n*56)+24];
    C[(l_n*56)+6] += A[289] * B[(l_n*56)+24];
    C[(l_n*56)+8] += A[290] * B[(l_n*56)+24];
    C[(l_n*56)+9] += A[291] * B[(l_n*56)+24];
    C[(l_n*56)+13] += A[292] * B[(l_n*56)+24];
    C[(l_n*56)+16] += A[293] * B[(l_n*56)+24];
    C[(l_n*56)+18] += A[294] * B[(l_n*56)+24];
    C[(l_n*56)+19] += A[295] * B[(l_n*56)+24];
    C[(l_n*56)+24] += A[296] * B[(l_n*56)+24];
    C[(l_n*56)+40] += A[297] * B[(l_n*56)+24];
    C[(l_n*56)+45] += A[298] * B[(l_n*56)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*56)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*56)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a25_0 = _mm_load_ss(&A[299]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*56)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a25_1 = _mm_load_ss(&A[300]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*56)+25], c25_1);
    __m128 c25_2 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a25_2 = _mm_load_ss(&A[301]);
    c25_2 = _mm_add_ss(c25_2, _mm_mul_ss(a25_2, b25));
    _mm_store_ss(&C[(l_n*56)+37], c25_2);
    __m128 c25_3 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a25_3 = _mm_load_ss(&A[302]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*56)+42], c25_3);
    __m128 c25_4 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a25_4 = _mm_load_ss(&A[303]);
    c25_4 = _mm_add_ss(c25_4, _mm_mul_ss(a25_4, b25));
    _mm_store_ss(&C[(l_n*56)+46], c25_4);
#else
    C[(l_n*56)+10] += A[299] * B[(l_n*56)+25];
    C[(l_n*56)+25] += A[300] * B[(l_n*56)+25];
    C[(l_n*56)+37] += A[301] * B[(l_n*56)+25];
    C[(l_n*56)+42] += A[302] * B[(l_n*56)+25];
    C[(l_n*56)+46] += A[303] * B[(l_n*56)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*56)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*56)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a26_0 = _mm_load_ss(&A[304]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*56)+4], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a26_1 = _mm_load_ss(&A[305]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*56)+11], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a26_2 = _mm_load_ss(&A[306]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*56)+14], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a26_3 = _mm_load_ss(&A[307]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*56)+26], c26_3);
    __m128 c26_4 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a26_4 = _mm_load_ss(&A[308]);
    c26_4 = _mm_add_ss(c26_4, _mm_mul_ss(a26_4, b26));
    _mm_store_ss(&C[(l_n*56)+38], c26_4);
    __m128 c26_5 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a26_5 = _mm_load_ss(&A[309]);
    c26_5 = _mm_add_ss(c26_5, _mm_mul_ss(a26_5, b26));
    _mm_store_ss(&C[(l_n*56)+43], c26_5);
    __m128 c26_6 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a26_6 = _mm_load_ss(&A[310]);
    c26_6 = _mm_add_ss(c26_6, _mm_mul_ss(a26_6, b26));
    _mm_store_ss(&C[(l_n*56)+47], c26_6);
#else
    C[(l_n*56)+4] += A[304] * B[(l_n*56)+26];
    C[(l_n*56)+11] += A[305] * B[(l_n*56)+26];
    C[(l_n*56)+14] += A[306] * B[(l_n*56)+26];
    C[(l_n*56)+26] += A[307] * B[(l_n*56)+26];
    C[(l_n*56)+38] += A[308] * B[(l_n*56)+26];
    C[(l_n*56)+43] += A[309] * B[(l_n*56)+26];
    C[(l_n*56)+47] += A[310] * B[(l_n*56)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*56)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*56)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a27_0 = _mm_load_ss(&A[311]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*56)+1], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a27_1 = _mm_load_ss(&A[312]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*56)+5], c27_1);
    __m128 c27_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a27_2 = _mm_load_ss(&A[313]);
    c27_2 = _mm_add_ss(c27_2, _mm_mul_ss(a27_2, b27));
    _mm_store_ss(&C[(l_n*56)+7], c27_2);
    __m128 c27_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a27_3 = _mm_load_ss(&A[314]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*56)+12], c27_3);
    __m128 c27_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a27_4 = _mm_load_ss(&A[315]);
    c27_4 = _mm_add_ss(c27_4, _mm_mul_ss(a27_4, b27));
    _mm_store_ss(&C[(l_n*56)+15], c27_4);
    __m128 c27_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a27_5 = _mm_load_ss(&A[316]);
    c27_5 = _mm_add_ss(c27_5, _mm_mul_ss(a27_5, b27));
    _mm_store_ss(&C[(l_n*56)+17], c27_5);
    __m128 c27_6 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a27_6 = _mm_load_ss(&A[317]);
    c27_6 = _mm_add_ss(c27_6, _mm_mul_ss(a27_6, b27));
    _mm_store_ss(&C[(l_n*56)+27], c27_6);
    __m128 c27_7 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a27_7 = _mm_load_ss(&A[318]);
    c27_7 = _mm_add_ss(c27_7, _mm_mul_ss(a27_7, b27));
    _mm_store_ss(&C[(l_n*56)+39], c27_7);
    __m128 c27_8 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a27_8 = _mm_load_ss(&A[319]);
    c27_8 = _mm_add_ss(c27_8, _mm_mul_ss(a27_8, b27));
    _mm_store_ss(&C[(l_n*56)+44], c27_8);
    __m128 c27_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a27_9 = _mm_load_ss(&A[320]);
    c27_9 = _mm_add_ss(c27_9, _mm_mul_ss(a27_9, b27));
    _mm_store_ss(&C[(l_n*56)+48], c27_9);
#else
    C[(l_n*56)+1] += A[311] * B[(l_n*56)+27];
    C[(l_n*56)+5] += A[312] * B[(l_n*56)+27];
    C[(l_n*56)+7] += A[313] * B[(l_n*56)+27];
    C[(l_n*56)+12] += A[314] * B[(l_n*56)+27];
    C[(l_n*56)+15] += A[315] * B[(l_n*56)+27];
    C[(l_n*56)+17] += A[316] * B[(l_n*56)+27];
    C[(l_n*56)+27] += A[317] * B[(l_n*56)+27];
    C[(l_n*56)+39] += A[318] * B[(l_n*56)+27];
    C[(l_n*56)+44] += A[319] * B[(l_n*56)+27];
    C[(l_n*56)+48] += A[320] * B[(l_n*56)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*56)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*56)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a28_0 = _mm_load_ss(&A[321]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*56)+0], c28_0);
    __m128 c28_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a28_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[322]));
    c28_1 = _mm_add_ps(c28_1, _mm_mul_ps(a28_1, b28));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c28_1));
    __m128 c28_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a28_3 = _mm_load_ss(&A[324]);
    c28_3 = _mm_add_ss(c28_3, _mm_mul_ss(a28_3, b28));
    _mm_store_ss(&C[(l_n*56)+6], c28_3);
    __m128 c28_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a28_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[325]));
    c28_4 = _mm_add_ps(c28_4, _mm_mul_ps(a28_4, b28));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c28_4));
    __m128 c28_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a28_6 = _mm_load_ss(&A[327]);
    c28_6 = _mm_add_ss(c28_6, _mm_mul_ss(a28_6, b28));
    _mm_store_ss(&C[(l_n*56)+13], c28_6);
    __m128 c28_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a28_7 = _mm_load_ss(&A[328]);
    c28_7 = _mm_add_ss(c28_7, _mm_mul_ss(a28_7, b28));
    _mm_store_ss(&C[(l_n*56)+16], c28_7);
    __m128 c28_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a28_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[329]));
    c28_8 = _mm_add_ps(c28_8, _mm_mul_ps(a28_8, b28));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c28_8));
    __m128 c28_10 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a28_10 = _mm_load_ss(&A[331]);
    c28_10 = _mm_add_ss(c28_10, _mm_mul_ss(a28_10, b28));
    _mm_store_ss(&C[(l_n*56)+28], c28_10);
    __m128 c28_11 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a28_11 = _mm_load_ss(&A[332]);
    c28_11 = _mm_add_ss(c28_11, _mm_mul_ss(a28_11, b28));
    _mm_store_ss(&C[(l_n*56)+40], c28_11);
    __m128 c28_12 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a28_12 = _mm_load_ss(&A[333]);
    c28_12 = _mm_add_ss(c28_12, _mm_mul_ss(a28_12, b28));
    _mm_store_ss(&C[(l_n*56)+45], c28_12);
    __m128 c28_13 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a28_13 = _mm_load_ss(&A[334]);
    c28_13 = _mm_add_ss(c28_13, _mm_mul_ss(a28_13, b28));
    _mm_store_ss(&C[(l_n*56)+49], c28_13);
#else
    C[(l_n*56)+0] += A[321] * B[(l_n*56)+28];
    C[(l_n*56)+2] += A[322] * B[(l_n*56)+28];
    C[(l_n*56)+3] += A[323] * B[(l_n*56)+28];
    C[(l_n*56)+6] += A[324] * B[(l_n*56)+28];
    C[(l_n*56)+8] += A[325] * B[(l_n*56)+28];
    C[(l_n*56)+9] += A[326] * B[(l_n*56)+28];
    C[(l_n*56)+13] += A[327] * B[(l_n*56)+28];
    C[(l_n*56)+16] += A[328] * B[(l_n*56)+28];
    C[(l_n*56)+18] += A[329] * B[(l_n*56)+28];
    C[(l_n*56)+19] += A[330] * B[(l_n*56)+28];
    C[(l_n*56)+28] += A[331] * B[(l_n*56)+28];
    C[(l_n*56)+40] += A[332] * B[(l_n*56)+28];
    C[(l_n*56)+45] += A[333] * B[(l_n*56)+28];
    C[(l_n*56)+49] += A[334] * B[(l_n*56)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*56)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*56)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a29_0 = _mm_load_ss(&A[335]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*56)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a29_1 = _mm_load_ss(&A[336]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*56)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a29_2 = _mm_load_ss(&A[337]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*56)+29], c29_2);
    __m128 c29_3 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a29_3 = _mm_load_ss(&A[338]);
    c29_3 = _mm_add_ss(c29_3, _mm_mul_ss(a29_3, b29));
    _mm_store_ss(&C[(l_n*56)+38], c29_3);
    __m128 c29_4 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a29_4 = _mm_load_ss(&A[339]);
    c29_4 = _mm_add_ss(c29_4, _mm_mul_ss(a29_4, b29));
    _mm_store_ss(&C[(l_n*56)+43], c29_4);
    __m128 c29_5 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a29_5 = _mm_load_ss(&A[340]);
    c29_5 = _mm_add_ss(c29_5, _mm_mul_ss(a29_5, b29));
    _mm_store_ss(&C[(l_n*56)+47], c29_5);
    __m128 c29_6 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a29_6 = _mm_load_ss(&A[341]);
    c29_6 = _mm_add_ss(c29_6, _mm_mul_ss(a29_6, b29));
    _mm_store_ss(&C[(l_n*56)+50], c29_6);
#else
    C[(l_n*56)+4] += A[335] * B[(l_n*56)+29];
    C[(l_n*56)+14] += A[336] * B[(l_n*56)+29];
    C[(l_n*56)+29] += A[337] * B[(l_n*56)+29];
    C[(l_n*56)+38] += A[338] * B[(l_n*56)+29];
    C[(l_n*56)+43] += A[339] * B[(l_n*56)+29];
    C[(l_n*56)+47] += A[340] * B[(l_n*56)+29];
    C[(l_n*56)+50] += A[341] * B[(l_n*56)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*56)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*56)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a30_0 = _mm_load_ss(&A[342]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*56)+1], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a30_1 = _mm_load_ss(&A[343]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*56)+5], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a30_2 = _mm_load_ss(&A[344]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*56)+7], c30_2);
    __m128 c30_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a30_3 = _mm_load_ss(&A[345]);
    c30_3 = _mm_add_ss(c30_3, _mm_mul_ss(a30_3, b30));
    _mm_store_ss(&C[(l_n*56)+15], c30_3);
    __m128 c30_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a30_4 = _mm_load_ss(&A[346]);
    c30_4 = _mm_add_ss(c30_4, _mm_mul_ss(a30_4, b30));
    _mm_store_ss(&C[(l_n*56)+17], c30_4);
    __m128 c30_5 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a30_5 = _mm_load_ss(&A[347]);
    c30_5 = _mm_add_ss(c30_5, _mm_mul_ss(a30_5, b30));
    _mm_store_ss(&C[(l_n*56)+30], c30_5);
    __m128 c30_6 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a30_6 = _mm_load_ss(&A[348]);
    c30_6 = _mm_add_ss(c30_6, _mm_mul_ss(a30_6, b30));
    _mm_store_ss(&C[(l_n*56)+39], c30_6);
    __m128 c30_7 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a30_7 = _mm_load_ss(&A[349]);
    c30_7 = _mm_add_ss(c30_7, _mm_mul_ss(a30_7, b30));
    _mm_store_ss(&C[(l_n*56)+44], c30_7);
    __m128 c30_8 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a30_8 = _mm_load_ss(&A[350]);
    c30_8 = _mm_add_ss(c30_8, _mm_mul_ss(a30_8, b30));
    _mm_store_ss(&C[(l_n*56)+48], c30_8);
    __m128 c30_9 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a30_9 = _mm_load_ss(&A[351]);
    c30_9 = _mm_add_ss(c30_9, _mm_mul_ss(a30_9, b30));
    _mm_store_ss(&C[(l_n*56)+51], c30_9);
#else
    C[(l_n*56)+1] += A[342] * B[(l_n*56)+30];
    C[(l_n*56)+5] += A[343] * B[(l_n*56)+30];
    C[(l_n*56)+7] += A[344] * B[(l_n*56)+30];
    C[(l_n*56)+15] += A[345] * B[(l_n*56)+30];
    C[(l_n*56)+17] += A[346] * B[(l_n*56)+30];
    C[(l_n*56)+30] += A[347] * B[(l_n*56)+30];
    C[(l_n*56)+39] += A[348] * B[(l_n*56)+30];
    C[(l_n*56)+44] += A[349] * B[(l_n*56)+30];
    C[(l_n*56)+48] += A[350] * B[(l_n*56)+30];
    C[(l_n*56)+51] += A[351] * B[(l_n*56)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*56)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*56)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a31_0 = _mm_load_ss(&A[352]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*56)+0], c31_0);
    __m128 c31_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a31_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[353]));
    c31_1 = _mm_add_ps(c31_1, _mm_mul_ps(a31_1, b31));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c31_1));
    __m128 c31_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a31_3 = _mm_load_ss(&A[355]);
    c31_3 = _mm_add_ss(c31_3, _mm_mul_ss(a31_3, b31));
    _mm_store_ss(&C[(l_n*56)+6], c31_3);
    __m128 c31_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a31_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[356]));
    c31_4 = _mm_add_ps(c31_4, _mm_mul_ps(a31_4, b31));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c31_4));
    __m128 c31_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a31_6 = _mm_load_ss(&A[358]);
    c31_6 = _mm_add_ss(c31_6, _mm_mul_ss(a31_6, b31));
    _mm_store_ss(&C[(l_n*56)+16], c31_6);
    __m128 c31_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a31_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[359]));
    c31_7 = _mm_add_ps(c31_7, _mm_mul_ps(a31_7, b31));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c31_7));
    __m128 c31_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a31_9 = _mm_load_ss(&A[361]);
    c31_9 = _mm_add_ss(c31_9, _mm_mul_ss(a31_9, b31));
    _mm_store_ss(&C[(l_n*56)+31], c31_9);
    __m128 c31_10 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a31_10 = _mm_load_ss(&A[362]);
    c31_10 = _mm_add_ss(c31_10, _mm_mul_ss(a31_10, b31));
    _mm_store_ss(&C[(l_n*56)+40], c31_10);
    __m128 c31_11 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a31_11 = _mm_load_ss(&A[363]);
    c31_11 = _mm_add_ss(c31_11, _mm_mul_ss(a31_11, b31));
    _mm_store_ss(&C[(l_n*56)+45], c31_11);
    __m128 c31_12 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a31_12 = _mm_load_ss(&A[364]);
    c31_12 = _mm_add_ss(c31_12, _mm_mul_ss(a31_12, b31));
    _mm_store_ss(&C[(l_n*56)+49], c31_12);
    __m128 c31_13 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a31_13 = _mm_load_ss(&A[365]);
    c31_13 = _mm_add_ss(c31_13, _mm_mul_ss(a31_13, b31));
    _mm_store_ss(&C[(l_n*56)+52], c31_13);
#else
    C[(l_n*56)+0] += A[352] * B[(l_n*56)+31];
    C[(l_n*56)+2] += A[353] * B[(l_n*56)+31];
    C[(l_n*56)+3] += A[354] * B[(l_n*56)+31];
    C[(l_n*56)+6] += A[355] * B[(l_n*56)+31];
    C[(l_n*56)+8] += A[356] * B[(l_n*56)+31];
    C[(l_n*56)+9] += A[357] * B[(l_n*56)+31];
    C[(l_n*56)+16] += A[358] * B[(l_n*56)+31];
    C[(l_n*56)+18] += A[359] * B[(l_n*56)+31];
    C[(l_n*56)+19] += A[360] * B[(l_n*56)+31];
    C[(l_n*56)+31] += A[361] * B[(l_n*56)+31];
    C[(l_n*56)+40] += A[362] * B[(l_n*56)+31];
    C[(l_n*56)+45] += A[363] * B[(l_n*56)+31];
    C[(l_n*56)+49] += A[364] * B[(l_n*56)+31];
    C[(l_n*56)+52] += A[365] * B[(l_n*56)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*56)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*56)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a32_0 = _mm_load_ss(&A[366]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*56)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a32_1 = _mm_load_ss(&A[367]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*56)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a32_2 = _mm_load_ss(&A[368]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*56)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a32_3 = _mm_load_ss(&A[369]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*56)+32], c32_3);
    __m128 c32_4 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a32_4 = _mm_load_ss(&A[370]);
    c32_4 = _mm_add_ss(c32_4, _mm_mul_ss(a32_4, b32));
    _mm_store_ss(&C[(l_n*56)+39], c32_4);
    __m128 c32_5 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a32_5 = _mm_load_ss(&A[371]);
    c32_5 = _mm_add_ss(c32_5, _mm_mul_ss(a32_5, b32));
    _mm_store_ss(&C[(l_n*56)+44], c32_5);
    __m128 c32_6 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a32_6 = _mm_load_ss(&A[372]);
    c32_6 = _mm_add_ss(c32_6, _mm_mul_ss(a32_6, b32));
    _mm_store_ss(&C[(l_n*56)+48], c32_6);
    __m128 c32_7 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a32_7 = _mm_load_ss(&A[373]);
    c32_7 = _mm_add_ss(c32_7, _mm_mul_ss(a32_7, b32));
    _mm_store_ss(&C[(l_n*56)+51], c32_7);
    __m128 c32_8 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a32_8 = _mm_load_ss(&A[374]);
    c32_8 = _mm_add_ss(c32_8, _mm_mul_ss(a32_8, b32));
    _mm_store_ss(&C[(l_n*56)+53], c32_8);
#else
    C[(l_n*56)+1] += A[366] * B[(l_n*56)+32];
    C[(l_n*56)+7] += A[367] * B[(l_n*56)+32];
    C[(l_n*56)+17] += A[368] * B[(l_n*56)+32];
    C[(l_n*56)+32] += A[369] * B[(l_n*56)+32];
    C[(l_n*56)+39] += A[370] * B[(l_n*56)+32];
    C[(l_n*56)+44] += A[371] * B[(l_n*56)+32];
    C[(l_n*56)+48] += A[372] * B[(l_n*56)+32];
    C[(l_n*56)+51] += A[373] * B[(l_n*56)+32];
    C[(l_n*56)+53] += A[374] * B[(l_n*56)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*56)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*56)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a33_0 = _mm_load_ss(&A[375]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*56)+0], c33_0);
    __m128 c33_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a33_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[376]));
    c33_1 = _mm_add_ps(c33_1, _mm_mul_ps(a33_1, b33));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c33_1));
    __m128 c33_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a33_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[378]));
    c33_3 = _mm_add_ps(c33_3, _mm_mul_ps(a33_3, b33));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c33_3));
    __m128 c33_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a33_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[380]));
    c33_5 = _mm_add_ps(c33_5, _mm_mul_ps(a33_5, b33));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c33_5));
    __m128 c33_7 = _mm_load_ss(&C[(l_n*56)+33]);
    __m128 a33_7 = _mm_load_ss(&A[382]);
    c33_7 = _mm_add_ss(c33_7, _mm_mul_ss(a33_7, b33));
    _mm_store_ss(&C[(l_n*56)+33], c33_7);
    __m128 c33_8 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a33_8 = _mm_load_ss(&A[383]);
    c33_8 = _mm_add_ss(c33_8, _mm_mul_ss(a33_8, b33));
    _mm_store_ss(&C[(l_n*56)+40], c33_8);
    __m128 c33_9 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a33_9 = _mm_load_ss(&A[384]);
    c33_9 = _mm_add_ss(c33_9, _mm_mul_ss(a33_9, b33));
    _mm_store_ss(&C[(l_n*56)+45], c33_9);
    __m128 c33_10 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a33_10 = _mm_load_ss(&A[385]);
    c33_10 = _mm_add_ss(c33_10, _mm_mul_ss(a33_10, b33));
    _mm_store_ss(&C[(l_n*56)+49], c33_10);
    __m128 c33_11 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a33_11 = _mm_load_ss(&A[386]);
    c33_11 = _mm_add_ss(c33_11, _mm_mul_ss(a33_11, b33));
    _mm_store_ss(&C[(l_n*56)+52], c33_11);
    __m128 c33_12 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a33_12 = _mm_load_ss(&A[387]);
    c33_12 = _mm_add_ss(c33_12, _mm_mul_ss(a33_12, b33));
    _mm_store_ss(&C[(l_n*56)+54], c33_12);
#else
    C[(l_n*56)+0] += A[375] * B[(l_n*56)+33];
    C[(l_n*56)+2] += A[376] * B[(l_n*56)+33];
    C[(l_n*56)+3] += A[377] * B[(l_n*56)+33];
    C[(l_n*56)+8] += A[378] * B[(l_n*56)+33];
    C[(l_n*56)+9] += A[379] * B[(l_n*56)+33];
    C[(l_n*56)+18] += A[380] * B[(l_n*56)+33];
    C[(l_n*56)+19] += A[381] * B[(l_n*56)+33];
    C[(l_n*56)+33] += A[382] * B[(l_n*56)+33];
    C[(l_n*56)+40] += A[383] * B[(l_n*56)+33];
    C[(l_n*56)+45] += A[384] * B[(l_n*56)+33];
    C[(l_n*56)+49] += A[385] * B[(l_n*56)+33];
    C[(l_n*56)+52] += A[386] * B[(l_n*56)+33];
    C[(l_n*56)+54] += A[387] * B[(l_n*56)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*56)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*56)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a34_0 = _mm_load_ss(&A[388]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*56)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a34_1 = _mm_load_ss(&A[389]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*56)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a34_2 = _mm_load_ss(&A[390]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*56)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a34_3 = _mm_load_ss(&A[391]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*56)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a34_4 = _mm_load_ss(&A[392]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*56)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a34_5 = _mm_load_ss(&A[393]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*56)+40], c34_5);
    __m128 c34_6 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a34_6 = _mm_load_ss(&A[394]);
    c34_6 = _mm_add_ss(c34_6, _mm_mul_ss(a34_6, b34));
    _mm_store_ss(&C[(l_n*56)+45], c34_6);
    __m128 c34_7 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a34_7 = _mm_load_ss(&A[395]);
    c34_7 = _mm_add_ss(c34_7, _mm_mul_ss(a34_7, b34));
    _mm_store_ss(&C[(l_n*56)+49], c34_7);
    __m128 c34_8 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a34_8 = _mm_load_ss(&A[396]);
    c34_8 = _mm_add_ss(c34_8, _mm_mul_ss(a34_8, b34));
    _mm_store_ss(&C[(l_n*56)+52], c34_8);
    __m128 c34_9 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+54]));
    __m128 a34_9 = _mm_castpd_ps(_mm_load_sd((const double*)&A[397]));
    c34_9 = _mm_add_ps(c34_9, _mm_mul_ps(a34_9, b34));
    _mm_store_sd((double*)&C[(l_n*56)+54], _mm_castps_pd(c34_9));
#else
    C[(l_n*56)+0] += A[388] * B[(l_n*56)+34];
    C[(l_n*56)+3] += A[389] * B[(l_n*56)+34];
    C[(l_n*56)+9] += A[390] * B[(l_n*56)+34];
    C[(l_n*56)+19] += A[391] * B[(l_n*56)+34];
    C[(l_n*56)+34] += A[392] * B[(l_n*56)+34];
    C[(l_n*56)+40] += A[393] * B[(l_n*56)+34];
    C[(l_n*56)+45] += A[394] * B[(l_n*56)+34];
    C[(l_n*56)+49] += A[395] * B[(l_n*56)+34];
    C[(l_n*56)+52] += A[396] * B[(l_n*56)+34];
    C[(l_n*56)+54] += A[397] * B[(l_n*56)+34];
    C[(l_n*56)+55] += A[398] * B[(l_n*56)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*56)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*56)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*56)+35]);
    __m128 a35_0 = _mm_load_ss(&A[399]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*56)+35], c35_0);
#else
    C[(l_n*56)+35] += A[399] * B[(l_n*56)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*56)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*56)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a36_0 = _mm_load_ss(&A[400]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*56)+20], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*56)+36]);
    __m128 a36_1 = _mm_load_ss(&A[401]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*56)+36], c36_1);
#else
    C[(l_n*56)+20] += A[400] * B[(l_n*56)+36];
    C[(l_n*56)+36] += A[401] * B[(l_n*56)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*56)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*56)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a37_0 = _mm_load_ss(&A[402]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*56)+10], c37_0);
    __m128 c37_1 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a37_1 = _mm_load_ss(&A[403]);
    c37_1 = _mm_add_ss(c37_1, _mm_mul_ss(a37_1, b37));
    _mm_store_ss(&C[(l_n*56)+21], c37_1);
    __m128 c37_2 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a37_2 = _mm_load_ss(&A[404]);
    c37_2 = _mm_add_ss(c37_2, _mm_mul_ss(a37_2, b37));
    _mm_store_ss(&C[(l_n*56)+25], c37_2);
    __m128 c37_3 = _mm_load_ss(&C[(l_n*56)+37]);
    __m128 a37_3 = _mm_load_ss(&A[405]);
    c37_3 = _mm_add_ss(c37_3, _mm_mul_ss(a37_3, b37));
    _mm_store_ss(&C[(l_n*56)+37], c37_3);
#else
    C[(l_n*56)+10] += A[402] * B[(l_n*56)+37];
    C[(l_n*56)+21] += A[403] * B[(l_n*56)+37];
    C[(l_n*56)+25] += A[404] * B[(l_n*56)+37];
    C[(l_n*56)+37] += A[405] * B[(l_n*56)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*56)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*56)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a38_0 = _mm_load_ss(&A[406]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*56)+4], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a38_1 = _mm_load_ss(&A[407]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*56)+11], c38_1);
    __m128 c38_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a38_2 = _mm_load_ss(&A[408]);
    c38_2 = _mm_add_ss(c38_2, _mm_mul_ss(a38_2, b38));
    _mm_store_ss(&C[(l_n*56)+14], c38_2);
    __m128 c38_3 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a38_3 = _mm_load_ss(&A[409]);
    c38_3 = _mm_add_ss(c38_3, _mm_mul_ss(a38_3, b38));
    _mm_store_ss(&C[(l_n*56)+22], c38_3);
    __m128 c38_4 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a38_4 = _mm_load_ss(&A[410]);
    c38_4 = _mm_add_ss(c38_4, _mm_mul_ss(a38_4, b38));
    _mm_store_ss(&C[(l_n*56)+26], c38_4);
    __m128 c38_5 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a38_5 = _mm_load_ss(&A[411]);
    c38_5 = _mm_add_ss(c38_5, _mm_mul_ss(a38_5, b38));
    _mm_store_ss(&C[(l_n*56)+29], c38_5);
    __m128 c38_6 = _mm_load_ss(&C[(l_n*56)+38]);
    __m128 a38_6 = _mm_load_ss(&A[412]);
    c38_6 = _mm_add_ss(c38_6, _mm_mul_ss(a38_6, b38));
    _mm_store_ss(&C[(l_n*56)+38], c38_6);
#else
    C[(l_n*56)+4] += A[406] * B[(l_n*56)+38];
    C[(l_n*56)+11] += A[407] * B[(l_n*56)+38];
    C[(l_n*56)+14] += A[408] * B[(l_n*56)+38];
    C[(l_n*56)+22] += A[409] * B[(l_n*56)+38];
    C[(l_n*56)+26] += A[410] * B[(l_n*56)+38];
    C[(l_n*56)+29] += A[411] * B[(l_n*56)+38];
    C[(l_n*56)+38] += A[412] * B[(l_n*56)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*56)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*56)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a39_0 = _mm_load_ss(&A[413]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*56)+1], c39_0);
    __m128 c39_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a39_1 = _mm_load_ss(&A[414]);
    c39_1 = _mm_add_ss(c39_1, _mm_mul_ss(a39_1, b39));
    _mm_store_ss(&C[(l_n*56)+5], c39_1);
    __m128 c39_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a39_2 = _mm_load_ss(&A[415]);
    c39_2 = _mm_add_ss(c39_2, _mm_mul_ss(a39_2, b39));
    _mm_store_ss(&C[(l_n*56)+7], c39_2);
    __m128 c39_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a39_3 = _mm_load_ss(&A[416]);
    c39_3 = _mm_add_ss(c39_3, _mm_mul_ss(a39_3, b39));
    _mm_store_ss(&C[(l_n*56)+12], c39_3);
    __m128 c39_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a39_4 = _mm_load_ss(&A[417]);
    c39_4 = _mm_add_ss(c39_4, _mm_mul_ss(a39_4, b39));
    _mm_store_ss(&C[(l_n*56)+15], c39_4);
    __m128 c39_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a39_5 = _mm_load_ss(&A[418]);
    c39_5 = _mm_add_ss(c39_5, _mm_mul_ss(a39_5, b39));
    _mm_store_ss(&C[(l_n*56)+17], c39_5);
    __m128 c39_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a39_6 = _mm_load_ss(&A[419]);
    c39_6 = _mm_add_ss(c39_6, _mm_mul_ss(a39_6, b39));
    _mm_store_ss(&C[(l_n*56)+23], c39_6);
    __m128 c39_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a39_7 = _mm_load_ss(&A[420]);
    c39_7 = _mm_add_ss(c39_7, _mm_mul_ss(a39_7, b39));
    _mm_store_ss(&C[(l_n*56)+27], c39_7);
    __m128 c39_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a39_8 = _mm_load_ss(&A[421]);
    c39_8 = _mm_add_ss(c39_8, _mm_mul_ss(a39_8, b39));
    _mm_store_ss(&C[(l_n*56)+30], c39_8);
    __m128 c39_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a39_9 = _mm_load_ss(&A[422]);
    c39_9 = _mm_add_ss(c39_9, _mm_mul_ss(a39_9, b39));
    _mm_store_ss(&C[(l_n*56)+32], c39_9);
    __m128 c39_10 = _mm_load_ss(&C[(l_n*56)+39]);
    __m128 a39_10 = _mm_load_ss(&A[423]);
    c39_10 = _mm_add_ss(c39_10, _mm_mul_ss(a39_10, b39));
    _mm_store_ss(&C[(l_n*56)+39], c39_10);
#else
    C[(l_n*56)+1] += A[413] * B[(l_n*56)+39];
    C[(l_n*56)+5] += A[414] * B[(l_n*56)+39];
    C[(l_n*56)+7] += A[415] * B[(l_n*56)+39];
    C[(l_n*56)+12] += A[416] * B[(l_n*56)+39];
    C[(l_n*56)+15] += A[417] * B[(l_n*56)+39];
    C[(l_n*56)+17] += A[418] * B[(l_n*56)+39];
    C[(l_n*56)+23] += A[419] * B[(l_n*56)+39];
    C[(l_n*56)+27] += A[420] * B[(l_n*56)+39];
    C[(l_n*56)+30] += A[421] * B[(l_n*56)+39];
    C[(l_n*56)+32] += A[422] * B[(l_n*56)+39];
    C[(l_n*56)+39] += A[423] * B[(l_n*56)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*56)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*56)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a40_0 = _mm_load_ss(&A[424]);
    c40_0 = _mm_add_ss(c40_0, _mm_mul_ss(a40_0, b40));
    _mm_store_ss(&C[(l_n*56)+0], c40_0);
    __m128 c40_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a40_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[425]));
    c40_1 = _mm_add_ps(c40_1, _mm_mul_ps(a40_1, b40));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c40_1));
    __m128 c40_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a40_3 = _mm_load_ss(&A[427]);
    c40_3 = _mm_add_ss(c40_3, _mm_mul_ss(a40_3, b40));
    _mm_store_ss(&C[(l_n*56)+6], c40_3);
    __m128 c40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a40_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[428]));
    c40_4 = _mm_add_ps(c40_4, _mm_mul_ps(a40_4, b40));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c40_4));
    __m128 c40_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a40_6 = _mm_load_ss(&A[430]);
    c40_6 = _mm_add_ss(c40_6, _mm_mul_ss(a40_6, b40));
    _mm_store_ss(&C[(l_n*56)+13], c40_6);
    __m128 c40_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a40_7 = _mm_load_ss(&A[431]);
    c40_7 = _mm_add_ss(c40_7, _mm_mul_ss(a40_7, b40));
    _mm_store_ss(&C[(l_n*56)+16], c40_7);
    __m128 c40_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a40_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[432]));
    c40_8 = _mm_add_ps(c40_8, _mm_mul_ps(a40_8, b40));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c40_8));
    __m128 c40_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a40_10 = _mm_load_ss(&A[434]);
    c40_10 = _mm_add_ss(c40_10, _mm_mul_ss(a40_10, b40));
    _mm_store_ss(&C[(l_n*56)+24], c40_10);
    __m128 c40_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a40_11 = _mm_load_ss(&A[435]);
    c40_11 = _mm_add_ss(c40_11, _mm_mul_ss(a40_11, b40));
    _mm_store_ss(&C[(l_n*56)+28], c40_11);
    __m128 c40_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a40_12 = _mm_load_ss(&A[436]);
    c40_12 = _mm_add_ss(c40_12, _mm_mul_ss(a40_12, b40));
    _mm_store_ss(&C[(l_n*56)+31], c40_12);
    __m128 c40_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a40_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[437]));
    c40_13 = _mm_add_ps(c40_13, _mm_mul_ps(a40_13, b40));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c40_13));
    __m128 c40_15 = _mm_load_ss(&C[(l_n*56)+40]);
    __m128 a40_15 = _mm_load_ss(&A[439]);
    c40_15 = _mm_add_ss(c40_15, _mm_mul_ss(a40_15, b40));
    _mm_store_ss(&C[(l_n*56)+40], c40_15);
#else
    C[(l_n*56)+0] += A[424] * B[(l_n*56)+40];
    C[(l_n*56)+2] += A[425] * B[(l_n*56)+40];
    C[(l_n*56)+3] += A[426] * B[(l_n*56)+40];
    C[(l_n*56)+6] += A[427] * B[(l_n*56)+40];
    C[(l_n*56)+8] += A[428] * B[(l_n*56)+40];
    C[(l_n*56)+9] += A[429] * B[(l_n*56)+40];
    C[(l_n*56)+13] += A[430] * B[(l_n*56)+40];
    C[(l_n*56)+16] += A[431] * B[(l_n*56)+40];
    C[(l_n*56)+18] += A[432] * B[(l_n*56)+40];
    C[(l_n*56)+19] += A[433] * B[(l_n*56)+40];
    C[(l_n*56)+24] += A[434] * B[(l_n*56)+40];
    C[(l_n*56)+28] += A[435] * B[(l_n*56)+40];
    C[(l_n*56)+31] += A[436] * B[(l_n*56)+40];
    C[(l_n*56)+33] += A[437] * B[(l_n*56)+40];
    C[(l_n*56)+34] += A[438] * B[(l_n*56)+40];
    C[(l_n*56)+40] += A[439] * B[(l_n*56)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*56)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*56)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*56)+20]);
    __m128 a41_0 = _mm_load_ss(&A[440]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*56)+20], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*56)+41]);
    __m128 a41_1 = _mm_load_ss(&A[441]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*56)+41], c41_1);
#else
    C[(l_n*56)+20] += A[440] * B[(l_n*56)+41];
    C[(l_n*56)+41] += A[441] * B[(l_n*56)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*56)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*56)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a42_0 = _mm_load_ss(&A[442]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*56)+10], c42_0);
    __m128 c42_1 = _mm_load_ss(&C[(l_n*56)+21]);
    __m128 a42_1 = _mm_load_ss(&A[443]);
    c42_1 = _mm_add_ss(c42_1, _mm_mul_ss(a42_1, b42));
    _mm_store_ss(&C[(l_n*56)+21], c42_1);
    __m128 c42_2 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a42_2 = _mm_load_ss(&A[444]);
    c42_2 = _mm_add_ss(c42_2, _mm_mul_ss(a42_2, b42));
    _mm_store_ss(&C[(l_n*56)+25], c42_2);
    __m128 c42_3 = _mm_load_ss(&C[(l_n*56)+42]);
    __m128 a42_3 = _mm_load_ss(&A[445]);
    c42_3 = _mm_add_ss(c42_3, _mm_mul_ss(a42_3, b42));
    _mm_store_ss(&C[(l_n*56)+42], c42_3);
#else
    C[(l_n*56)+10] += A[442] * B[(l_n*56)+42];
    C[(l_n*56)+21] += A[443] * B[(l_n*56)+42];
    C[(l_n*56)+25] += A[444] * B[(l_n*56)+42];
    C[(l_n*56)+42] += A[445] * B[(l_n*56)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*56)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*56)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a43_0 = _mm_load_ss(&A[446]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*56)+4], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a43_1 = _mm_load_ss(&A[447]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*56)+11], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a43_2 = _mm_load_ss(&A[448]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*56)+14], c43_2);
    __m128 c43_3 = _mm_load_ss(&C[(l_n*56)+22]);
    __m128 a43_3 = _mm_load_ss(&A[449]);
    c43_3 = _mm_add_ss(c43_3, _mm_mul_ss(a43_3, b43));
    _mm_store_ss(&C[(l_n*56)+22], c43_3);
    __m128 c43_4 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a43_4 = _mm_load_ss(&A[450]);
    c43_4 = _mm_add_ss(c43_4, _mm_mul_ss(a43_4, b43));
    _mm_store_ss(&C[(l_n*56)+26], c43_4);
    __m128 c43_5 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a43_5 = _mm_load_ss(&A[451]);
    c43_5 = _mm_add_ss(c43_5, _mm_mul_ss(a43_5, b43));
    _mm_store_ss(&C[(l_n*56)+29], c43_5);
    __m128 c43_6 = _mm_load_ss(&C[(l_n*56)+43]);
    __m128 a43_6 = _mm_load_ss(&A[452]);
    c43_6 = _mm_add_ss(c43_6, _mm_mul_ss(a43_6, b43));
    _mm_store_ss(&C[(l_n*56)+43], c43_6);
#else
    C[(l_n*56)+4] += A[446] * B[(l_n*56)+43];
    C[(l_n*56)+11] += A[447] * B[(l_n*56)+43];
    C[(l_n*56)+14] += A[448] * B[(l_n*56)+43];
    C[(l_n*56)+22] += A[449] * B[(l_n*56)+43];
    C[(l_n*56)+26] += A[450] * B[(l_n*56)+43];
    C[(l_n*56)+29] += A[451] * B[(l_n*56)+43];
    C[(l_n*56)+43] += A[452] * B[(l_n*56)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*56)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*56)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a44_0 = _mm_load_ss(&A[453]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*56)+1], c44_0);
    __m128 c44_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a44_1 = _mm_load_ss(&A[454]);
    c44_1 = _mm_add_ss(c44_1, _mm_mul_ss(a44_1, b44));
    _mm_store_ss(&C[(l_n*56)+5], c44_1);
    __m128 c44_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a44_2 = _mm_load_ss(&A[455]);
    c44_2 = _mm_add_ss(c44_2, _mm_mul_ss(a44_2, b44));
    _mm_store_ss(&C[(l_n*56)+7], c44_2);
    __m128 c44_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a44_3 = _mm_load_ss(&A[456]);
    c44_3 = _mm_add_ss(c44_3, _mm_mul_ss(a44_3, b44));
    _mm_store_ss(&C[(l_n*56)+12], c44_3);
    __m128 c44_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a44_4 = _mm_load_ss(&A[457]);
    c44_4 = _mm_add_ss(c44_4, _mm_mul_ss(a44_4, b44));
    _mm_store_ss(&C[(l_n*56)+15], c44_4);
    __m128 c44_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a44_5 = _mm_load_ss(&A[458]);
    c44_5 = _mm_add_ss(c44_5, _mm_mul_ss(a44_5, b44));
    _mm_store_ss(&C[(l_n*56)+17], c44_5);
    __m128 c44_6 = _mm_load_ss(&C[(l_n*56)+23]);
    __m128 a44_6 = _mm_load_ss(&A[459]);
    c44_6 = _mm_add_ss(c44_6, _mm_mul_ss(a44_6, b44));
    _mm_store_ss(&C[(l_n*56)+23], c44_6);
    __m128 c44_7 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a44_7 = _mm_load_ss(&A[460]);
    c44_7 = _mm_add_ss(c44_7, _mm_mul_ss(a44_7, b44));
    _mm_store_ss(&C[(l_n*56)+27], c44_7);
    __m128 c44_8 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a44_8 = _mm_load_ss(&A[461]);
    c44_8 = _mm_add_ss(c44_8, _mm_mul_ss(a44_8, b44));
    _mm_store_ss(&C[(l_n*56)+30], c44_8);
    __m128 c44_9 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a44_9 = _mm_load_ss(&A[462]);
    c44_9 = _mm_add_ss(c44_9, _mm_mul_ss(a44_9, b44));
    _mm_store_ss(&C[(l_n*56)+32], c44_9);
    __m128 c44_10 = _mm_load_ss(&C[(l_n*56)+44]);
    __m128 a44_10 = _mm_load_ss(&A[463]);
    c44_10 = _mm_add_ss(c44_10, _mm_mul_ss(a44_10, b44));
    _mm_store_ss(&C[(l_n*56)+44], c44_10);
#else
    C[(l_n*56)+1] += A[453] * B[(l_n*56)+44];
    C[(l_n*56)+5] += A[454] * B[(l_n*56)+44];
    C[(l_n*56)+7] += A[455] * B[(l_n*56)+44];
    C[(l_n*56)+12] += A[456] * B[(l_n*56)+44];
    C[(l_n*56)+15] += A[457] * B[(l_n*56)+44];
    C[(l_n*56)+17] += A[458] * B[(l_n*56)+44];
    C[(l_n*56)+23] += A[459] * B[(l_n*56)+44];
    C[(l_n*56)+27] += A[460] * B[(l_n*56)+44];
    C[(l_n*56)+30] += A[461] * B[(l_n*56)+44];
    C[(l_n*56)+32] += A[462] * B[(l_n*56)+44];
    C[(l_n*56)+44] += A[463] * B[(l_n*56)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*56)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*56)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a45_0 = _mm_load_ss(&A[464]);
    c45_0 = _mm_add_ss(c45_0, _mm_mul_ss(a45_0, b45));
    _mm_store_ss(&C[(l_n*56)+0], c45_0);
    __m128 c45_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a45_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[465]));
    c45_1 = _mm_add_ps(c45_1, _mm_mul_ps(a45_1, b45));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c45_1));
    __m128 c45_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a45_3 = _mm_load_ss(&A[467]);
    c45_3 = _mm_add_ss(c45_3, _mm_mul_ss(a45_3, b45));
    _mm_store_ss(&C[(l_n*56)+6], c45_3);
    __m128 c45_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a45_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[468]));
    c45_4 = _mm_add_ps(c45_4, _mm_mul_ps(a45_4, b45));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c45_4));
    __m128 c45_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a45_6 = _mm_load_ss(&A[470]);
    c45_6 = _mm_add_ss(c45_6, _mm_mul_ss(a45_6, b45));
    _mm_store_ss(&C[(l_n*56)+13], c45_6);
    __m128 c45_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a45_7 = _mm_load_ss(&A[471]);
    c45_7 = _mm_add_ss(c45_7, _mm_mul_ss(a45_7, b45));
    _mm_store_ss(&C[(l_n*56)+16], c45_7);
    __m128 c45_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a45_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[472]));
    c45_8 = _mm_add_ps(c45_8, _mm_mul_ps(a45_8, b45));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c45_8));
    __m128 c45_10 = _mm_load_ss(&C[(l_n*56)+24]);
    __m128 a45_10 = _mm_load_ss(&A[474]);
    c45_10 = _mm_add_ss(c45_10, _mm_mul_ss(a45_10, b45));
    _mm_store_ss(&C[(l_n*56)+24], c45_10);
    __m128 c45_11 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a45_11 = _mm_load_ss(&A[475]);
    c45_11 = _mm_add_ss(c45_11, _mm_mul_ss(a45_11, b45));
    _mm_store_ss(&C[(l_n*56)+28], c45_11);
    __m128 c45_12 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a45_12 = _mm_load_ss(&A[476]);
    c45_12 = _mm_add_ss(c45_12, _mm_mul_ss(a45_12, b45));
    _mm_store_ss(&C[(l_n*56)+31], c45_12);
    __m128 c45_13 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a45_13 = _mm_castpd_ps(_mm_load_sd((const double*)&A[477]));
    c45_13 = _mm_add_ps(c45_13, _mm_mul_ps(a45_13, b45));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c45_13));
    __m128 c45_15 = _mm_load_ss(&C[(l_n*56)+45]);
    __m128 a45_15 = _mm_load_ss(&A[479]);
    c45_15 = _mm_add_ss(c45_15, _mm_mul_ss(a45_15, b45));
    _mm_store_ss(&C[(l_n*56)+45], c45_15);
#else
    C[(l_n*56)+0] += A[464] * B[(l_n*56)+45];
    C[(l_n*56)+2] += A[465] * B[(l_n*56)+45];
    C[(l_n*56)+3] += A[466] * B[(l_n*56)+45];
    C[(l_n*56)+6] += A[467] * B[(l_n*56)+45];
    C[(l_n*56)+8] += A[468] * B[(l_n*56)+45];
    C[(l_n*56)+9] += A[469] * B[(l_n*56)+45];
    C[(l_n*56)+13] += A[470] * B[(l_n*56)+45];
    C[(l_n*56)+16] += A[471] * B[(l_n*56)+45];
    C[(l_n*56)+18] += A[472] * B[(l_n*56)+45];
    C[(l_n*56)+19] += A[473] * B[(l_n*56)+45];
    C[(l_n*56)+24] += A[474] * B[(l_n*56)+45];
    C[(l_n*56)+28] += A[475] * B[(l_n*56)+45];
    C[(l_n*56)+31] += A[476] * B[(l_n*56)+45];
    C[(l_n*56)+33] += A[477] * B[(l_n*56)+45];
    C[(l_n*56)+34] += A[478] * B[(l_n*56)+45];
    C[(l_n*56)+45] += A[479] * B[(l_n*56)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*56)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*56)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*56)+10]);
    __m128 a46_0 = _mm_load_ss(&A[480]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*56)+10], c46_0);
    __m128 c46_1 = _mm_load_ss(&C[(l_n*56)+25]);
    __m128 a46_1 = _mm_load_ss(&A[481]);
    c46_1 = _mm_add_ss(c46_1, _mm_mul_ss(a46_1, b46));
    _mm_store_ss(&C[(l_n*56)+25], c46_1);
    __m128 c46_2 = _mm_load_ss(&C[(l_n*56)+46]);
    __m128 a46_2 = _mm_load_ss(&A[482]);
    c46_2 = _mm_add_ss(c46_2, _mm_mul_ss(a46_2, b46));
    _mm_store_ss(&C[(l_n*56)+46], c46_2);
#else
    C[(l_n*56)+10] += A[480] * B[(l_n*56)+46];
    C[(l_n*56)+25] += A[481] * B[(l_n*56)+46];
    C[(l_n*56)+46] += A[482] * B[(l_n*56)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*56)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*56)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a47_0 = _mm_load_ss(&A[483]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*56)+4], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*56)+11]);
    __m128 a47_1 = _mm_load_ss(&A[484]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*56)+11], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a47_2 = _mm_load_ss(&A[485]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*56)+14], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*56)+26]);
    __m128 a47_3 = _mm_load_ss(&A[486]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*56)+26], c47_3);
    __m128 c47_4 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a47_4 = _mm_load_ss(&A[487]);
    c47_4 = _mm_add_ss(c47_4, _mm_mul_ss(a47_4, b47));
    _mm_store_ss(&C[(l_n*56)+29], c47_4);
    __m128 c47_5 = _mm_load_ss(&C[(l_n*56)+47]);
    __m128 a47_5 = _mm_load_ss(&A[488]);
    c47_5 = _mm_add_ss(c47_5, _mm_mul_ss(a47_5, b47));
    _mm_store_ss(&C[(l_n*56)+47], c47_5);
#else
    C[(l_n*56)+4] += A[483] * B[(l_n*56)+47];
    C[(l_n*56)+11] += A[484] * B[(l_n*56)+47];
    C[(l_n*56)+14] += A[485] * B[(l_n*56)+47];
    C[(l_n*56)+26] += A[486] * B[(l_n*56)+47];
    C[(l_n*56)+29] += A[487] * B[(l_n*56)+47];
    C[(l_n*56)+47] += A[488] * B[(l_n*56)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*56)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*56)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a48_0 = _mm_load_ss(&A[489]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*56)+1], c48_0);
    __m128 c48_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a48_1 = _mm_load_ss(&A[490]);
    c48_1 = _mm_add_ss(c48_1, _mm_mul_ss(a48_1, b48));
    _mm_store_ss(&C[(l_n*56)+5], c48_1);
    __m128 c48_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a48_2 = _mm_load_ss(&A[491]);
    c48_2 = _mm_add_ss(c48_2, _mm_mul_ss(a48_2, b48));
    _mm_store_ss(&C[(l_n*56)+7], c48_2);
    __m128 c48_3 = _mm_load_ss(&C[(l_n*56)+12]);
    __m128 a48_3 = _mm_load_ss(&A[492]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*56)+12], c48_3);
    __m128 c48_4 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a48_4 = _mm_load_ss(&A[493]);
    c48_4 = _mm_add_ss(c48_4, _mm_mul_ss(a48_4, b48));
    _mm_store_ss(&C[(l_n*56)+15], c48_4);
    __m128 c48_5 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a48_5 = _mm_load_ss(&A[494]);
    c48_5 = _mm_add_ss(c48_5, _mm_mul_ss(a48_5, b48));
    _mm_store_ss(&C[(l_n*56)+17], c48_5);
    __m128 c48_6 = _mm_load_ss(&C[(l_n*56)+27]);
    __m128 a48_6 = _mm_load_ss(&A[495]);
    c48_6 = _mm_add_ss(c48_6, _mm_mul_ss(a48_6, b48));
    _mm_store_ss(&C[(l_n*56)+27], c48_6);
    __m128 c48_7 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a48_7 = _mm_load_ss(&A[496]);
    c48_7 = _mm_add_ss(c48_7, _mm_mul_ss(a48_7, b48));
    _mm_store_ss(&C[(l_n*56)+30], c48_7);
    __m128 c48_8 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a48_8 = _mm_load_ss(&A[497]);
    c48_8 = _mm_add_ss(c48_8, _mm_mul_ss(a48_8, b48));
    _mm_store_ss(&C[(l_n*56)+32], c48_8);
    __m128 c48_9 = _mm_load_ss(&C[(l_n*56)+48]);
    __m128 a48_9 = _mm_load_ss(&A[498]);
    c48_9 = _mm_add_ss(c48_9, _mm_mul_ss(a48_9, b48));
    _mm_store_ss(&C[(l_n*56)+48], c48_9);
#else
    C[(l_n*56)+1] += A[489] * B[(l_n*56)+48];
    C[(l_n*56)+5] += A[490] * B[(l_n*56)+48];
    C[(l_n*56)+7] += A[491] * B[(l_n*56)+48];
    C[(l_n*56)+12] += A[492] * B[(l_n*56)+48];
    C[(l_n*56)+15] += A[493] * B[(l_n*56)+48];
    C[(l_n*56)+17] += A[494] * B[(l_n*56)+48];
    C[(l_n*56)+27] += A[495] * B[(l_n*56)+48];
    C[(l_n*56)+30] += A[496] * B[(l_n*56)+48];
    C[(l_n*56)+32] += A[497] * B[(l_n*56)+48];
    C[(l_n*56)+48] += A[498] * B[(l_n*56)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*56)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*56)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a49_0 = _mm_load_ss(&A[499]);
    c49_0 = _mm_add_ss(c49_0, _mm_mul_ss(a49_0, b49));
    _mm_store_ss(&C[(l_n*56)+0], c49_0);
    __m128 c49_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a49_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[500]));
    c49_1 = _mm_add_ps(c49_1, _mm_mul_ps(a49_1, b49));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c49_1));
    __m128 c49_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a49_3 = _mm_load_ss(&A[502]);
    c49_3 = _mm_add_ss(c49_3, _mm_mul_ss(a49_3, b49));
    _mm_store_ss(&C[(l_n*56)+6], c49_3);
    __m128 c49_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a49_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[503]));
    c49_4 = _mm_add_ps(c49_4, _mm_mul_ps(a49_4, b49));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c49_4));
    __m128 c49_6 = _mm_load_ss(&C[(l_n*56)+13]);
    __m128 a49_6 = _mm_load_ss(&A[505]);
    c49_6 = _mm_add_ss(c49_6, _mm_mul_ss(a49_6, b49));
    _mm_store_ss(&C[(l_n*56)+13], c49_6);
    __m128 c49_7 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a49_7 = _mm_load_ss(&A[506]);
    c49_7 = _mm_add_ss(c49_7, _mm_mul_ss(a49_7, b49));
    _mm_store_ss(&C[(l_n*56)+16], c49_7);
    __m128 c49_8 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a49_8 = _mm_castpd_ps(_mm_load_sd((const double*)&A[507]));
    c49_8 = _mm_add_ps(c49_8, _mm_mul_ps(a49_8, b49));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c49_8));
    __m128 c49_10 = _mm_load_ss(&C[(l_n*56)+28]);
    __m128 a49_10 = _mm_load_ss(&A[509]);
    c49_10 = _mm_add_ss(c49_10, _mm_mul_ss(a49_10, b49));
    _mm_store_ss(&C[(l_n*56)+28], c49_10);
    __m128 c49_11 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a49_11 = _mm_load_ss(&A[510]);
    c49_11 = _mm_add_ss(c49_11, _mm_mul_ss(a49_11, b49));
    _mm_store_ss(&C[(l_n*56)+31], c49_11);
    __m128 c49_12 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a49_12 = _mm_castpd_ps(_mm_load_sd((const double*)&A[511]));
    c49_12 = _mm_add_ps(c49_12, _mm_mul_ps(a49_12, b49));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c49_12));
    __m128 c49_14 = _mm_load_ss(&C[(l_n*56)+49]);
    __m128 a49_14 = _mm_load_ss(&A[513]);
    c49_14 = _mm_add_ss(c49_14, _mm_mul_ss(a49_14, b49));
    _mm_store_ss(&C[(l_n*56)+49], c49_14);
#else
    C[(l_n*56)+0] += A[499] * B[(l_n*56)+49];
    C[(l_n*56)+2] += A[500] * B[(l_n*56)+49];
    C[(l_n*56)+3] += A[501] * B[(l_n*56)+49];
    C[(l_n*56)+6] += A[502] * B[(l_n*56)+49];
    C[(l_n*56)+8] += A[503] * B[(l_n*56)+49];
    C[(l_n*56)+9] += A[504] * B[(l_n*56)+49];
    C[(l_n*56)+13] += A[505] * B[(l_n*56)+49];
    C[(l_n*56)+16] += A[506] * B[(l_n*56)+49];
    C[(l_n*56)+18] += A[507] * B[(l_n*56)+49];
    C[(l_n*56)+19] += A[508] * B[(l_n*56)+49];
    C[(l_n*56)+28] += A[509] * B[(l_n*56)+49];
    C[(l_n*56)+31] += A[510] * B[(l_n*56)+49];
    C[(l_n*56)+33] += A[511] * B[(l_n*56)+49];
    C[(l_n*56)+34] += A[512] * B[(l_n*56)+49];
    C[(l_n*56)+49] += A[513] * B[(l_n*56)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*56)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*56)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*56)+4]);
    __m128 a50_0 = _mm_load_ss(&A[514]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*56)+4], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*56)+14]);
    __m128 a50_1 = _mm_load_ss(&A[515]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*56)+14], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*56)+29]);
    __m128 a50_2 = _mm_load_ss(&A[516]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*56)+29], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*56)+50]);
    __m128 a50_3 = _mm_load_ss(&A[517]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*56)+50], c50_3);
#else
    C[(l_n*56)+4] += A[514] * B[(l_n*56)+50];
    C[(l_n*56)+14] += A[515] * B[(l_n*56)+50];
    C[(l_n*56)+29] += A[516] * B[(l_n*56)+50];
    C[(l_n*56)+50] += A[517] * B[(l_n*56)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*56)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*56)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a51_0 = _mm_load_ss(&A[518]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*56)+1], c51_0);
    __m128 c51_1 = _mm_load_ss(&C[(l_n*56)+5]);
    __m128 a51_1 = _mm_load_ss(&A[519]);
    c51_1 = _mm_add_ss(c51_1, _mm_mul_ss(a51_1, b51));
    _mm_store_ss(&C[(l_n*56)+5], c51_1);
    __m128 c51_2 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a51_2 = _mm_load_ss(&A[520]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*56)+7], c51_2);
    __m128 c51_3 = _mm_load_ss(&C[(l_n*56)+15]);
    __m128 a51_3 = _mm_load_ss(&A[521]);
    c51_3 = _mm_add_ss(c51_3, _mm_mul_ss(a51_3, b51));
    _mm_store_ss(&C[(l_n*56)+15], c51_3);
    __m128 c51_4 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a51_4 = _mm_load_ss(&A[522]);
    c51_4 = _mm_add_ss(c51_4, _mm_mul_ss(a51_4, b51));
    _mm_store_ss(&C[(l_n*56)+17], c51_4);
    __m128 c51_5 = _mm_load_ss(&C[(l_n*56)+30]);
    __m128 a51_5 = _mm_load_ss(&A[523]);
    c51_5 = _mm_add_ss(c51_5, _mm_mul_ss(a51_5, b51));
    _mm_store_ss(&C[(l_n*56)+30], c51_5);
    __m128 c51_6 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a51_6 = _mm_load_ss(&A[524]);
    c51_6 = _mm_add_ss(c51_6, _mm_mul_ss(a51_6, b51));
    _mm_store_ss(&C[(l_n*56)+32], c51_6);
    __m128 c51_7 = _mm_load_ss(&C[(l_n*56)+51]);
    __m128 a51_7 = _mm_load_ss(&A[525]);
    c51_7 = _mm_add_ss(c51_7, _mm_mul_ss(a51_7, b51));
    _mm_store_ss(&C[(l_n*56)+51], c51_7);
#else
    C[(l_n*56)+1] += A[518] * B[(l_n*56)+51];
    C[(l_n*56)+5] += A[519] * B[(l_n*56)+51];
    C[(l_n*56)+7] += A[520] * B[(l_n*56)+51];
    C[(l_n*56)+15] += A[521] * B[(l_n*56)+51];
    C[(l_n*56)+17] += A[522] * B[(l_n*56)+51];
    C[(l_n*56)+30] += A[523] * B[(l_n*56)+51];
    C[(l_n*56)+32] += A[524] * B[(l_n*56)+51];
    C[(l_n*56)+51] += A[525] * B[(l_n*56)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*56)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*56)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a52_0 = _mm_load_ss(&A[526]);
    c52_0 = _mm_add_ss(c52_0, _mm_mul_ss(a52_0, b52));
    _mm_store_ss(&C[(l_n*56)+0], c52_0);
    __m128 c52_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a52_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[527]));
    c52_1 = _mm_add_ps(c52_1, _mm_mul_ps(a52_1, b52));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c52_1));
    __m128 c52_3 = _mm_load_ss(&C[(l_n*56)+6]);
    __m128 a52_3 = _mm_load_ss(&A[529]);
    c52_3 = _mm_add_ss(c52_3, _mm_mul_ss(a52_3, b52));
    _mm_store_ss(&C[(l_n*56)+6], c52_3);
    __m128 c52_4 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a52_4 = _mm_castpd_ps(_mm_load_sd((const double*)&A[530]));
    c52_4 = _mm_add_ps(c52_4, _mm_mul_ps(a52_4, b52));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c52_4));
    __m128 c52_6 = _mm_load_ss(&C[(l_n*56)+16]);
    __m128 a52_6 = _mm_load_ss(&A[532]);
    c52_6 = _mm_add_ss(c52_6, _mm_mul_ss(a52_6, b52));
    _mm_store_ss(&C[(l_n*56)+16], c52_6);
    __m128 c52_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a52_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[533]));
    c52_7 = _mm_add_ps(c52_7, _mm_mul_ps(a52_7, b52));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c52_7));
    __m128 c52_9 = _mm_load_ss(&C[(l_n*56)+31]);
    __m128 a52_9 = _mm_load_ss(&A[535]);
    c52_9 = _mm_add_ss(c52_9, _mm_mul_ss(a52_9, b52));
    _mm_store_ss(&C[(l_n*56)+31], c52_9);
    __m128 c52_10 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a52_10 = _mm_castpd_ps(_mm_load_sd((const double*)&A[536]));
    c52_10 = _mm_add_ps(c52_10, _mm_mul_ps(a52_10, b52));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c52_10));
    __m128 c52_12 = _mm_load_ss(&C[(l_n*56)+52]);
    __m128 a52_12 = _mm_load_ss(&A[538]);
    c52_12 = _mm_add_ss(c52_12, _mm_mul_ss(a52_12, b52));
    _mm_store_ss(&C[(l_n*56)+52], c52_12);
#else
    C[(l_n*56)+0] += A[526] * B[(l_n*56)+52];
    C[(l_n*56)+2] += A[527] * B[(l_n*56)+52];
    C[(l_n*56)+3] += A[528] * B[(l_n*56)+52];
    C[(l_n*56)+6] += A[529] * B[(l_n*56)+52];
    C[(l_n*56)+8] += A[530] * B[(l_n*56)+52];
    C[(l_n*56)+9] += A[531] * B[(l_n*56)+52];
    C[(l_n*56)+16] += A[532] * B[(l_n*56)+52];
    C[(l_n*56)+18] += A[533] * B[(l_n*56)+52];
    C[(l_n*56)+19] += A[534] * B[(l_n*56)+52];
    C[(l_n*56)+31] += A[535] * B[(l_n*56)+52];
    C[(l_n*56)+33] += A[536] * B[(l_n*56)+52];
    C[(l_n*56)+34] += A[537] * B[(l_n*56)+52];
    C[(l_n*56)+52] += A[538] * B[(l_n*56)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*56)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*56)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*56)+1]);
    __m128 a53_0 = _mm_load_ss(&A[539]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*56)+1], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*56)+7]);
    __m128 a53_1 = _mm_load_ss(&A[540]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*56)+7], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*56)+17]);
    __m128 a53_2 = _mm_load_ss(&A[541]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*56)+17], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*56)+32]);
    __m128 a53_3 = _mm_load_ss(&A[542]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*56)+32], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*56)+53]);
    __m128 a53_4 = _mm_load_ss(&A[543]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*56)+53], c53_4);
#else
    C[(l_n*56)+1] += A[539] * B[(l_n*56)+53];
    C[(l_n*56)+7] += A[540] * B[(l_n*56)+53];
    C[(l_n*56)+17] += A[541] * B[(l_n*56)+53];
    C[(l_n*56)+32] += A[542] * B[(l_n*56)+53];
    C[(l_n*56)+53] += A[543] * B[(l_n*56)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*56)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*56)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a54_0 = _mm_load_ss(&A[544]);
    c54_0 = _mm_add_ss(c54_0, _mm_mul_ss(a54_0, b54));
    _mm_store_ss(&C[(l_n*56)+0], c54_0);
    __m128 c54_1 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+2]));
    __m128 a54_1 = _mm_castpd_ps(_mm_load_sd((const double*)&A[545]));
    c54_1 = _mm_add_ps(c54_1, _mm_mul_ps(a54_1, b54));
    _mm_store_sd((double*)&C[(l_n*56)+2], _mm_castps_pd(c54_1));
    __m128 c54_3 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+8]));
    __m128 a54_3 = _mm_castpd_ps(_mm_load_sd((const double*)&A[547]));
    c54_3 = _mm_add_ps(c54_3, _mm_mul_ps(a54_3, b54));
    _mm_store_sd((double*)&C[(l_n*56)+8], _mm_castps_pd(c54_3));
    __m128 c54_5 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+18]));
    __m128 a54_5 = _mm_castpd_ps(_mm_load_sd((const double*)&A[549]));
    c54_5 = _mm_add_ps(c54_5, _mm_mul_ps(a54_5, b54));
    _mm_store_sd((double*)&C[(l_n*56)+18], _mm_castps_pd(c54_5));
    __m128 c54_7 = _mm_castpd_ps(_mm_load_sd((const double*)&C[(l_n*56)+33]));
    __m128 a54_7 = _mm_castpd_ps(_mm_load_sd((const double*)&A[551]));
    c54_7 = _mm_add_ps(c54_7, _mm_mul_ps(a54_7, b54));
    _mm_store_sd((double*)&C[(l_n*56)+33], _mm_castps_pd(c54_7));
    __m128 c54_9 = _mm_load_ss(&C[(l_n*56)+54]);
    __m128 a54_9 = _mm_load_ss(&A[553]);
    c54_9 = _mm_add_ss(c54_9, _mm_mul_ss(a54_9, b54));
    _mm_store_ss(&C[(l_n*56)+54], c54_9);
#else
    C[(l_n*56)+0] += A[544] * B[(l_n*56)+54];
    C[(l_n*56)+2] += A[545] * B[(l_n*56)+54];
    C[(l_n*56)+3] += A[546] * B[(l_n*56)+54];
    C[(l_n*56)+8] += A[547] * B[(l_n*56)+54];
    C[(l_n*56)+9] += A[548] * B[(l_n*56)+54];
    C[(l_n*56)+18] += A[549] * B[(l_n*56)+54];
    C[(l_n*56)+19] += A[550] * B[(l_n*56)+54];
    C[(l_n*56)+33] += A[551] * B[(l_n*56)+54];
    C[(l_n*56)+34] += A[552] * B[(l_n*56)+54];
    C[(l_n*56)+54] += A[553] * B[(l_n*56)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*56)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*56)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*56)+0]);
    __m128 a55_0 = _mm_load_ss(&A[554]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*56)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*56)+3]);
    __m128 a55_1 = _mm_load_ss(&A[555]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*56)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*56)+9]);
    __m128 a55_2 = _mm_load_ss(&A[556]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*56)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*56)+19]);
    __m128 a55_3 = _mm_load_ss(&A[557]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*56)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*56)+34]);
    __m128 a55_4 = _mm_load_ss(&A[558]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*56)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*56)+55]);
    __m128 a55_5 = _mm_load_ss(&A[559]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*56)+55], c55_5);
#else
    C[(l_n*56)+0] += A[554] * B[(l_n*56)+55];
    C[(l_n*56)+3] += A[555] * B[(l_n*56)+55];
    C[(l_n*56)+9] += A[556] * B[(l_n*56)+55];
    C[(l_n*56)+19] += A[557] * B[(l_n*56)+55];
    C[(l_n*56)+34] += A[558] * B[(l_n*56)+55];
    C[(l_n*56)+55] += A[559] * B[(l_n*56)+55];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 10080;
#endif
}

void ssparse_starMatrix_m84_n9_k9_ldA84_ldBna7_ldC84_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 84; l_m++) {
    C[0+l_m] += A[504+l_m] * B[0];
    C[0+l_m] += A[588+l_m] * B[1];
    C[0+l_m] += A[672+l_m] * B[2];
    C[84+l_m] += A[504+l_m] * B[3];
    C[84+l_m] += A[588+l_m] * B[4];
    C[84+l_m] += A[672+l_m] * B[5];
    C[168+l_m] += A[504+l_m] * B[6];
    C[168+l_m] += A[588+l_m] * B[7];
    C[168+l_m] += A[672+l_m] * B[8];
    C[252+l_m] += A[504+l_m] * B[9];
    C[252+l_m] += A[588+l_m] * B[10];
    C[336+l_m] += A[588+l_m] * B[11];
    C[336+l_m] += A[672+l_m] * B[12];
    C[420+l_m] += A[504+l_m] * B[13];
    C[420+l_m] += A[672+l_m] * B[14];
    C[504+l_m] += A[0+l_m] * B[15];
    C[504+l_m] += A[252+l_m] * B[16];
    C[504+l_m] += A[420+l_m] * B[17];
    C[588+l_m] += A[84+l_m] * B[18];
    C[588+l_m] += A[252+l_m] * B[19];
    C[588+l_m] += A[336+l_m] * B[20];
    C[672+l_m] += A[168+l_m] * B[21];
    C[672+l_m] += A[336+l_m] * B[22];
    C[672+l_m] += A[420+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 4032;
#endif
}

void ssparse_fM1DivM_m84_n9_k84_ldAna7_ldB84_ldC84_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 84; l_m++) {
      C[(l_n*84)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*84)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*84)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*84)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*84)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*84)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*84)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*84)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*84)+55], c0_5);
    __m128 c0_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a0_6 = _mm_load_ss(&A[6]);
    c0_6 = _mm_add_ss(c0_6, _mm_mul_ss(a0_6, b0));
    _mm_store_ss(&C[(l_n*84)+83], c0_6);
#else
    C[(l_n*84)+0] += A[0] * B[(l_n*84)+0];
    C[(l_n*84)+3] += A[1] * B[(l_n*84)+0];
    C[(l_n*84)+9] += A[2] * B[(l_n*84)+0];
    C[(l_n*84)+19] += A[3] * B[(l_n*84)+0];
    C[(l_n*84)+34] += A[4] * B[(l_n*84)+0];
    C[(l_n*84)+55] += A[5] * B[(l_n*84)+0];
    C[(l_n*84)+83] += A[6] * B[(l_n*84)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*84)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*84)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a1_0 = _mm_load_ss(&A[7]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*84)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a1_1 = _mm_load_ss(&A[8]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*84)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a1_2 = _mm_load_ss(&A[9]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*84)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a1_3 = _mm_load_ss(&A[10]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*84)+32], c1_3);
    __m128 c1_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a1_4 = _mm_load_ss(&A[11]);
    c1_4 = _mm_add_ss(c1_4, _mm_mul_ss(a1_4, b1));
    _mm_store_ss(&C[(l_n*84)+53], c1_4);
    __m128 c1_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a1_5 = _mm_load_ss(&A[12]);
    c1_5 = _mm_add_ss(c1_5, _mm_mul_ss(a1_5, b1));
    _mm_store_ss(&C[(l_n*84)+81], c1_5);
#else
    C[(l_n*84)+1] += A[7] * B[(l_n*84)+1];
    C[(l_n*84)+7] += A[8] * B[(l_n*84)+1];
    C[(l_n*84)+17] += A[9] * B[(l_n*84)+1];
    C[(l_n*84)+32] += A[10] * B[(l_n*84)+1];
    C[(l_n*84)+53] += A[11] * B[(l_n*84)+1];
    C[(l_n*84)+81] += A[12] * B[(l_n*84)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*84)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*84)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a2_0 = _mm_load_ss(&A[13]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*84)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a2_1 = _mm_load_ss(&A[14]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*84)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a2_2 = _mm_load_ss(&A[15]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*84)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a2_3 = _mm_load_ss(&A[16]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*84)+33], c2_3);
    __m128 c2_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a2_4 = _mm_load_ss(&A[17]);
    c2_4 = _mm_add_ss(c2_4, _mm_mul_ss(a2_4, b2));
    _mm_store_ss(&C[(l_n*84)+54], c2_4);
    __m128 c2_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a2_5 = _mm_load_ss(&A[18]);
    c2_5 = _mm_add_ss(c2_5, _mm_mul_ss(a2_5, b2));
    _mm_store_ss(&C[(l_n*84)+82], c2_5);
#else
    C[(l_n*84)+2] += A[13] * B[(l_n*84)+2];
    C[(l_n*84)+8] += A[14] * B[(l_n*84)+2];
    C[(l_n*84)+18] += A[15] * B[(l_n*84)+2];
    C[(l_n*84)+33] += A[16] * B[(l_n*84)+2];
    C[(l_n*84)+54] += A[17] * B[(l_n*84)+2];
    C[(l_n*84)+82] += A[18] * B[(l_n*84)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*84)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*84)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a3_0 = _mm_load_ss(&A[19]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*84)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a3_1 = _mm_load_ss(&A[20]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*84)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a3_2 = _mm_load_ss(&A[21]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*84)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a3_3 = _mm_load_ss(&A[22]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*84)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a3_4 = _mm_load_ss(&A[23]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*84)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a3_5 = _mm_load_ss(&A[24]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*84)+55], c3_5);
    __m128 c3_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a3_6 = _mm_load_ss(&A[25]);
    c3_6 = _mm_add_ss(c3_6, _mm_mul_ss(a3_6, b3));
    _mm_store_ss(&C[(l_n*84)+83], c3_6);
#else
    C[(l_n*84)+0] += A[19] * B[(l_n*84)+3];
    C[(l_n*84)+3] += A[20] * B[(l_n*84)+3];
    C[(l_n*84)+9] += A[21] * B[(l_n*84)+3];
    C[(l_n*84)+19] += A[22] * B[(l_n*84)+3];
    C[(l_n*84)+34] += A[23] * B[(l_n*84)+3];
    C[(l_n*84)+55] += A[24] * B[(l_n*84)+3];
    C[(l_n*84)+83] += A[25] * B[(l_n*84)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*84)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*84)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a4_0 = _mm_load_ss(&A[26]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*84)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a4_1 = _mm_load_ss(&A[27]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*84)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a4_2 = _mm_load_ss(&A[28]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*84)+29], c4_2);
    __m128 c4_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a4_3 = _mm_load_ss(&A[29]);
    c4_3 = _mm_add_ss(c4_3, _mm_mul_ss(a4_3, b4));
    _mm_store_ss(&C[(l_n*84)+50], c4_3);
    __m128 c4_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a4_4 = _mm_load_ss(&A[30]);
    c4_4 = _mm_add_ss(c4_4, _mm_mul_ss(a4_4, b4));
    _mm_store_ss(&C[(l_n*84)+78], c4_4);
#else
    C[(l_n*84)+4] += A[26] * B[(l_n*84)+4];
    C[(l_n*84)+14] += A[27] * B[(l_n*84)+4];
    C[(l_n*84)+29] += A[28] * B[(l_n*84)+4];
    C[(l_n*84)+50] += A[29] * B[(l_n*84)+4];
    C[(l_n*84)+78] += A[30] * B[(l_n*84)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*84)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*84)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a5_0 = _mm_load_ss(&A[31]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*84)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a5_1 = _mm_load_ss(&A[32]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*84)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a5_2 = _mm_load_ss(&A[33]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*84)+30], c5_2);
    __m128 c5_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a5_3 = _mm_load_ss(&A[34]);
    c5_3 = _mm_add_ss(c5_3, _mm_mul_ss(a5_3, b5));
    _mm_store_ss(&C[(l_n*84)+51], c5_3);
    __m128 c5_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a5_4 = _mm_load_ss(&A[35]);
    c5_4 = _mm_add_ss(c5_4, _mm_mul_ss(a5_4, b5));
    _mm_store_ss(&C[(l_n*84)+79], c5_4);
#else
    C[(l_n*84)+5] += A[31] * B[(l_n*84)+5];
    C[(l_n*84)+15] += A[32] * B[(l_n*84)+5];
    C[(l_n*84)+30] += A[33] * B[(l_n*84)+5];
    C[(l_n*84)+51] += A[34] * B[(l_n*84)+5];
    C[(l_n*84)+79] += A[35] * B[(l_n*84)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*84)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*84)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a6_0 = _mm_load_ss(&A[36]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*84)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a6_1 = _mm_load_ss(&A[37]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*84)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a6_2 = _mm_load_ss(&A[38]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*84)+31], c6_2);
    __m128 c6_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a6_3 = _mm_load_ss(&A[39]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*84)+52], c6_3);
    __m128 c6_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a6_4 = _mm_load_ss(&A[40]);
    c6_4 = _mm_add_ss(c6_4, _mm_mul_ss(a6_4, b6));
    _mm_store_ss(&C[(l_n*84)+80], c6_4);
#else
    C[(l_n*84)+6] += A[36] * B[(l_n*84)+6];
    C[(l_n*84)+16] += A[37] * B[(l_n*84)+6];
    C[(l_n*84)+31] += A[38] * B[(l_n*84)+6];
    C[(l_n*84)+52] += A[39] * B[(l_n*84)+6];
    C[(l_n*84)+80] += A[40] * B[(l_n*84)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*84)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*84)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a7_0 = _mm_load_ss(&A[41]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*84)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a7_1 = _mm_load_ss(&A[42]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*84)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a7_2 = _mm_load_ss(&A[43]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*84)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a7_3 = _mm_load_ss(&A[44]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*84)+32], c7_3);
    __m128 c7_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a7_4 = _mm_load_ss(&A[45]);
    c7_4 = _mm_add_ss(c7_4, _mm_mul_ss(a7_4, b7));
    _mm_store_ss(&C[(l_n*84)+53], c7_4);
    __m128 c7_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a7_5 = _mm_load_ss(&A[46]);
    c7_5 = _mm_add_ss(c7_5, _mm_mul_ss(a7_5, b7));
    _mm_store_ss(&C[(l_n*84)+81], c7_5);
#else
    C[(l_n*84)+1] += A[41] * B[(l_n*84)+7];
    C[(l_n*84)+7] += A[42] * B[(l_n*84)+7];
    C[(l_n*84)+17] += A[43] * B[(l_n*84)+7];
    C[(l_n*84)+32] += A[44] * B[(l_n*84)+7];
    C[(l_n*84)+53] += A[45] * B[(l_n*84)+7];
    C[(l_n*84)+81] += A[46] * B[(l_n*84)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*84)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*84)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a8_0 = _mm_load_ss(&A[47]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*84)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a8_1 = _mm_load_ss(&A[48]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*84)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a8_2 = _mm_load_ss(&A[49]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*84)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a8_3 = _mm_load_ss(&A[50]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*84)+33], c8_3);
    __m128 c8_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a8_4 = _mm_load_ss(&A[51]);
    c8_4 = _mm_add_ss(c8_4, _mm_mul_ss(a8_4, b8));
    _mm_store_ss(&C[(l_n*84)+54], c8_4);
    __m128 c8_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a8_5 = _mm_load_ss(&A[52]);
    c8_5 = _mm_add_ss(c8_5, _mm_mul_ss(a8_5, b8));
    _mm_store_ss(&C[(l_n*84)+82], c8_5);
#else
    C[(l_n*84)+2] += A[47] * B[(l_n*84)+8];
    C[(l_n*84)+8] += A[48] * B[(l_n*84)+8];
    C[(l_n*84)+18] += A[49] * B[(l_n*84)+8];
    C[(l_n*84)+33] += A[50] * B[(l_n*84)+8];
    C[(l_n*84)+54] += A[51] * B[(l_n*84)+8];
    C[(l_n*84)+82] += A[52] * B[(l_n*84)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*84)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*84)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a9_0 = _mm_load_ss(&A[53]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*84)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a9_1 = _mm_load_ss(&A[54]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*84)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a9_2 = _mm_load_ss(&A[55]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*84)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a9_3 = _mm_load_ss(&A[56]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*84)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a9_4 = _mm_load_ss(&A[57]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*84)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a9_5 = _mm_load_ss(&A[58]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*84)+55], c9_5);
    __m128 c9_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a9_6 = _mm_load_ss(&A[59]);
    c9_6 = _mm_add_ss(c9_6, _mm_mul_ss(a9_6, b9));
    _mm_store_ss(&C[(l_n*84)+83], c9_6);
#else
    C[(l_n*84)+0] += A[53] * B[(l_n*84)+9];
    C[(l_n*84)+3] += A[54] * B[(l_n*84)+9];
    C[(l_n*84)+9] += A[55] * B[(l_n*84)+9];
    C[(l_n*84)+19] += A[56] * B[(l_n*84)+9];
    C[(l_n*84)+34] += A[57] * B[(l_n*84)+9];
    C[(l_n*84)+55] += A[58] * B[(l_n*84)+9];
    C[(l_n*84)+83] += A[59] * B[(l_n*84)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*84)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*84)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a10_0 = _mm_load_ss(&A[60]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*84)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a10_1 = _mm_load_ss(&A[61]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*84)+25], c10_1);
    __m128 c10_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a10_2 = _mm_load_ss(&A[62]);
    c10_2 = _mm_add_ss(c10_2, _mm_mul_ss(a10_2, b10));
    _mm_store_ss(&C[(l_n*84)+46], c10_2);
    __m128 c10_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a10_3 = _mm_load_ss(&A[63]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*84)+74], c10_3);
#else
    C[(l_n*84)+10] += A[60] * B[(l_n*84)+10];
    C[(l_n*84)+25] += A[61] * B[(l_n*84)+10];
    C[(l_n*84)+46] += A[62] * B[(l_n*84)+10];
    C[(l_n*84)+74] += A[63] * B[(l_n*84)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*84)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*84)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a11_0 = _mm_load_ss(&A[64]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*84)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a11_1 = _mm_load_ss(&A[65]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*84)+26], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a11_2 = _mm_load_ss(&A[66]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*84)+47], c11_2);
    __m128 c11_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a11_3 = _mm_load_ss(&A[67]);
    c11_3 = _mm_add_ss(c11_3, _mm_mul_ss(a11_3, b11));
    _mm_store_ss(&C[(l_n*84)+75], c11_3);
#else
    C[(l_n*84)+11] += A[64] * B[(l_n*84)+11];
    C[(l_n*84)+26] += A[65] * B[(l_n*84)+11];
    C[(l_n*84)+47] += A[66] * B[(l_n*84)+11];
    C[(l_n*84)+75] += A[67] * B[(l_n*84)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*84)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*84)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a12_0 = _mm_load_ss(&A[68]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*84)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a12_1 = _mm_load_ss(&A[69]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*84)+27], c12_1);
    __m128 c12_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a12_2 = _mm_load_ss(&A[70]);
    c12_2 = _mm_add_ss(c12_2, _mm_mul_ss(a12_2, b12));
    _mm_store_ss(&C[(l_n*84)+48], c12_2);
    __m128 c12_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a12_3 = _mm_load_ss(&A[71]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*84)+76], c12_3);
#else
    C[(l_n*84)+12] += A[68] * B[(l_n*84)+12];
    C[(l_n*84)+27] += A[69] * B[(l_n*84)+12];
    C[(l_n*84)+48] += A[70] * B[(l_n*84)+12];
    C[(l_n*84)+76] += A[71] * B[(l_n*84)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*84)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*84)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a13_0 = _mm_load_ss(&A[72]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*84)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a13_1 = _mm_load_ss(&A[73]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*84)+28], c13_1);
    __m128 c13_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a13_2 = _mm_load_ss(&A[74]);
    c13_2 = _mm_add_ss(c13_2, _mm_mul_ss(a13_2, b13));
    _mm_store_ss(&C[(l_n*84)+49], c13_2);
    __m128 c13_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a13_3 = _mm_load_ss(&A[75]);
    c13_3 = _mm_add_ss(c13_3, _mm_mul_ss(a13_3, b13));
    _mm_store_ss(&C[(l_n*84)+77], c13_3);
#else
    C[(l_n*84)+13] += A[72] * B[(l_n*84)+13];
    C[(l_n*84)+28] += A[73] * B[(l_n*84)+13];
    C[(l_n*84)+49] += A[74] * B[(l_n*84)+13];
    C[(l_n*84)+77] += A[75] * B[(l_n*84)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*84)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*84)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a14_0 = _mm_load_ss(&A[76]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*84)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a14_1 = _mm_load_ss(&A[77]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*84)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a14_2 = _mm_load_ss(&A[78]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*84)+29], c14_2);
    __m128 c14_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a14_3 = _mm_load_ss(&A[79]);
    c14_3 = _mm_add_ss(c14_3, _mm_mul_ss(a14_3, b14));
    _mm_store_ss(&C[(l_n*84)+50], c14_3);
    __m128 c14_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a14_4 = _mm_load_ss(&A[80]);
    c14_4 = _mm_add_ss(c14_4, _mm_mul_ss(a14_4, b14));
    _mm_store_ss(&C[(l_n*84)+78], c14_4);
#else
    C[(l_n*84)+4] += A[76] * B[(l_n*84)+14];
    C[(l_n*84)+14] += A[77] * B[(l_n*84)+14];
    C[(l_n*84)+29] += A[78] * B[(l_n*84)+14];
    C[(l_n*84)+50] += A[79] * B[(l_n*84)+14];
    C[(l_n*84)+78] += A[80] * B[(l_n*84)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*84)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*84)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a15_0 = _mm_load_ss(&A[81]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*84)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a15_1 = _mm_load_ss(&A[82]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*84)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a15_2 = _mm_load_ss(&A[83]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*84)+30], c15_2);
    __m128 c15_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a15_3 = _mm_load_ss(&A[84]);
    c15_3 = _mm_add_ss(c15_3, _mm_mul_ss(a15_3, b15));
    _mm_store_ss(&C[(l_n*84)+51], c15_3);
    __m128 c15_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a15_4 = _mm_load_ss(&A[85]);
    c15_4 = _mm_add_ss(c15_4, _mm_mul_ss(a15_4, b15));
    _mm_store_ss(&C[(l_n*84)+79], c15_4);
#else
    C[(l_n*84)+5] += A[81] * B[(l_n*84)+15];
    C[(l_n*84)+15] += A[82] * B[(l_n*84)+15];
    C[(l_n*84)+30] += A[83] * B[(l_n*84)+15];
    C[(l_n*84)+51] += A[84] * B[(l_n*84)+15];
    C[(l_n*84)+79] += A[85] * B[(l_n*84)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*84)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*84)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a16_0 = _mm_load_ss(&A[86]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*84)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a16_1 = _mm_load_ss(&A[87]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*84)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a16_2 = _mm_load_ss(&A[88]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*84)+31], c16_2);
    __m128 c16_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a16_3 = _mm_load_ss(&A[89]);
    c16_3 = _mm_add_ss(c16_3, _mm_mul_ss(a16_3, b16));
    _mm_store_ss(&C[(l_n*84)+52], c16_3);
    __m128 c16_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a16_4 = _mm_load_ss(&A[90]);
    c16_4 = _mm_add_ss(c16_4, _mm_mul_ss(a16_4, b16));
    _mm_store_ss(&C[(l_n*84)+80], c16_4);
#else
    C[(l_n*84)+6] += A[86] * B[(l_n*84)+16];
    C[(l_n*84)+16] += A[87] * B[(l_n*84)+16];
    C[(l_n*84)+31] += A[88] * B[(l_n*84)+16];
    C[(l_n*84)+52] += A[89] * B[(l_n*84)+16];
    C[(l_n*84)+80] += A[90] * B[(l_n*84)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*84)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*84)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a17_0 = _mm_load_ss(&A[91]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*84)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a17_1 = _mm_load_ss(&A[92]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*84)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a17_2 = _mm_load_ss(&A[93]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*84)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a17_3 = _mm_load_ss(&A[94]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*84)+32], c17_3);
    __m128 c17_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a17_4 = _mm_load_ss(&A[95]);
    c17_4 = _mm_add_ss(c17_4, _mm_mul_ss(a17_4, b17));
    _mm_store_ss(&C[(l_n*84)+53], c17_4);
    __m128 c17_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a17_5 = _mm_load_ss(&A[96]);
    c17_5 = _mm_add_ss(c17_5, _mm_mul_ss(a17_5, b17));
    _mm_store_ss(&C[(l_n*84)+81], c17_5);
#else
    C[(l_n*84)+1] += A[91] * B[(l_n*84)+17];
    C[(l_n*84)+7] += A[92] * B[(l_n*84)+17];
    C[(l_n*84)+17] += A[93] * B[(l_n*84)+17];
    C[(l_n*84)+32] += A[94] * B[(l_n*84)+17];
    C[(l_n*84)+53] += A[95] * B[(l_n*84)+17];
    C[(l_n*84)+81] += A[96] * B[(l_n*84)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*84)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*84)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a18_0 = _mm_load_ss(&A[97]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*84)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a18_1 = _mm_load_ss(&A[98]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*84)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a18_2 = _mm_load_ss(&A[99]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*84)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a18_3 = _mm_load_ss(&A[100]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*84)+33], c18_3);
    __m128 c18_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a18_4 = _mm_load_ss(&A[101]);
    c18_4 = _mm_add_ss(c18_4, _mm_mul_ss(a18_4, b18));
    _mm_store_ss(&C[(l_n*84)+54], c18_4);
    __m128 c18_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a18_5 = _mm_load_ss(&A[102]);
    c18_5 = _mm_add_ss(c18_5, _mm_mul_ss(a18_5, b18));
    _mm_store_ss(&C[(l_n*84)+82], c18_5);
#else
    C[(l_n*84)+2] += A[97] * B[(l_n*84)+18];
    C[(l_n*84)+8] += A[98] * B[(l_n*84)+18];
    C[(l_n*84)+18] += A[99] * B[(l_n*84)+18];
    C[(l_n*84)+33] += A[100] * B[(l_n*84)+18];
    C[(l_n*84)+54] += A[101] * B[(l_n*84)+18];
    C[(l_n*84)+82] += A[102] * B[(l_n*84)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*84)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*84)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a19_0 = _mm_load_ss(&A[103]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*84)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a19_1 = _mm_load_ss(&A[104]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*84)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a19_2 = _mm_load_ss(&A[105]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*84)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a19_3 = _mm_load_ss(&A[106]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*84)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a19_4 = _mm_load_ss(&A[107]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*84)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a19_5 = _mm_load_ss(&A[108]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*84)+55], c19_5);
    __m128 c19_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a19_6 = _mm_load_ss(&A[109]);
    c19_6 = _mm_add_ss(c19_6, _mm_mul_ss(a19_6, b19));
    _mm_store_ss(&C[(l_n*84)+83], c19_6);
#else
    C[(l_n*84)+0] += A[103] * B[(l_n*84)+19];
    C[(l_n*84)+3] += A[104] * B[(l_n*84)+19];
    C[(l_n*84)+9] += A[105] * B[(l_n*84)+19];
    C[(l_n*84)+19] += A[106] * B[(l_n*84)+19];
    C[(l_n*84)+34] += A[107] * B[(l_n*84)+19];
    C[(l_n*84)+55] += A[108] * B[(l_n*84)+19];
    C[(l_n*84)+83] += A[109] * B[(l_n*84)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*84)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*84)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a20_0 = _mm_load_ss(&A[110]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*84)+20], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a20_1 = _mm_load_ss(&A[111]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*84)+41], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a20_2 = _mm_load_ss(&A[112]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*84)+69], c20_2);
#else
    C[(l_n*84)+20] += A[110] * B[(l_n*84)+20];
    C[(l_n*84)+41] += A[111] * B[(l_n*84)+20];
    C[(l_n*84)+69] += A[112] * B[(l_n*84)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*84)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*84)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a21_0 = _mm_load_ss(&A[113]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*84)+21], c21_0);
    __m128 c21_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a21_1 = _mm_load_ss(&A[114]);
    c21_1 = _mm_add_ss(c21_1, _mm_mul_ss(a21_1, b21));
    _mm_store_ss(&C[(l_n*84)+42], c21_1);
    __m128 c21_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a21_2 = _mm_load_ss(&A[115]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*84)+70], c21_2);
#else
    C[(l_n*84)+21] += A[113] * B[(l_n*84)+21];
    C[(l_n*84)+42] += A[114] * B[(l_n*84)+21];
    C[(l_n*84)+70] += A[115] * B[(l_n*84)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*84)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*84)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a22_0 = _mm_load_ss(&A[116]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*84)+22], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a22_1 = _mm_load_ss(&A[117]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*84)+43], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a22_2 = _mm_load_ss(&A[118]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*84)+71], c22_2);
#else
    C[(l_n*84)+22] += A[116] * B[(l_n*84)+22];
    C[(l_n*84)+43] += A[117] * B[(l_n*84)+22];
    C[(l_n*84)+71] += A[118] * B[(l_n*84)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*84)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*84)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a23_0 = _mm_load_ss(&A[119]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*84)+23], c23_0);
    __m128 c23_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a23_1 = _mm_load_ss(&A[120]);
    c23_1 = _mm_add_ss(c23_1, _mm_mul_ss(a23_1, b23));
    _mm_store_ss(&C[(l_n*84)+44], c23_1);
    __m128 c23_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a23_2 = _mm_load_ss(&A[121]);
    c23_2 = _mm_add_ss(c23_2, _mm_mul_ss(a23_2, b23));
    _mm_store_ss(&C[(l_n*84)+72], c23_2);
#else
    C[(l_n*84)+23] += A[119] * B[(l_n*84)+23];
    C[(l_n*84)+44] += A[120] * B[(l_n*84)+23];
    C[(l_n*84)+72] += A[121] * B[(l_n*84)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*84)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*84)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a24_0 = _mm_load_ss(&A[122]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*84)+24], c24_0);
    __m128 c24_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a24_1 = _mm_load_ss(&A[123]);
    c24_1 = _mm_add_ss(c24_1, _mm_mul_ss(a24_1, b24));
    _mm_store_ss(&C[(l_n*84)+45], c24_1);
    __m128 c24_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a24_2 = _mm_load_ss(&A[124]);
    c24_2 = _mm_add_ss(c24_2, _mm_mul_ss(a24_2, b24));
    _mm_store_ss(&C[(l_n*84)+73], c24_2);
#else
    C[(l_n*84)+24] += A[122] * B[(l_n*84)+24];
    C[(l_n*84)+45] += A[123] * B[(l_n*84)+24];
    C[(l_n*84)+73] += A[124] * B[(l_n*84)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*84)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*84)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a25_0 = _mm_load_ss(&A[125]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*84)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a25_1 = _mm_load_ss(&A[126]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*84)+25], c25_1);
    __m128 c25_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a25_2 = _mm_load_ss(&A[127]);
    c25_2 = _mm_add_ss(c25_2, _mm_mul_ss(a25_2, b25));
    _mm_store_ss(&C[(l_n*84)+46], c25_2);
    __m128 c25_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a25_3 = _mm_load_ss(&A[128]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*84)+74], c25_3);
#else
    C[(l_n*84)+10] += A[125] * B[(l_n*84)+25];
    C[(l_n*84)+25] += A[126] * B[(l_n*84)+25];
    C[(l_n*84)+46] += A[127] * B[(l_n*84)+25];
    C[(l_n*84)+74] += A[128] * B[(l_n*84)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*84)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*84)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a26_0 = _mm_load_ss(&A[129]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*84)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a26_1 = _mm_load_ss(&A[130]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*84)+26], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a26_2 = _mm_load_ss(&A[131]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*84)+47], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a26_3 = _mm_load_ss(&A[132]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*84)+75], c26_3);
#else
    C[(l_n*84)+11] += A[129] * B[(l_n*84)+26];
    C[(l_n*84)+26] += A[130] * B[(l_n*84)+26];
    C[(l_n*84)+47] += A[131] * B[(l_n*84)+26];
    C[(l_n*84)+75] += A[132] * B[(l_n*84)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*84)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*84)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a27_0 = _mm_load_ss(&A[133]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*84)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a27_1 = _mm_load_ss(&A[134]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*84)+27], c27_1);
    __m128 c27_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a27_2 = _mm_load_ss(&A[135]);
    c27_2 = _mm_add_ss(c27_2, _mm_mul_ss(a27_2, b27));
    _mm_store_ss(&C[(l_n*84)+48], c27_2);
    __m128 c27_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a27_3 = _mm_load_ss(&A[136]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*84)+76], c27_3);
#else
    C[(l_n*84)+12] += A[133] * B[(l_n*84)+27];
    C[(l_n*84)+27] += A[134] * B[(l_n*84)+27];
    C[(l_n*84)+48] += A[135] * B[(l_n*84)+27];
    C[(l_n*84)+76] += A[136] * B[(l_n*84)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*84)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*84)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a28_0 = _mm_load_ss(&A[137]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*84)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a28_1 = _mm_load_ss(&A[138]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*84)+28], c28_1);
    __m128 c28_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a28_2 = _mm_load_ss(&A[139]);
    c28_2 = _mm_add_ss(c28_2, _mm_mul_ss(a28_2, b28));
    _mm_store_ss(&C[(l_n*84)+49], c28_2);
    __m128 c28_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a28_3 = _mm_load_ss(&A[140]);
    c28_3 = _mm_add_ss(c28_3, _mm_mul_ss(a28_3, b28));
    _mm_store_ss(&C[(l_n*84)+77], c28_3);
#else
    C[(l_n*84)+13] += A[137] * B[(l_n*84)+28];
    C[(l_n*84)+28] += A[138] * B[(l_n*84)+28];
    C[(l_n*84)+49] += A[139] * B[(l_n*84)+28];
    C[(l_n*84)+77] += A[140] * B[(l_n*84)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*84)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*84)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a29_0 = _mm_load_ss(&A[141]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*84)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a29_1 = _mm_load_ss(&A[142]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*84)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a29_2 = _mm_load_ss(&A[143]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*84)+29], c29_2);
    __m128 c29_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a29_3 = _mm_load_ss(&A[144]);
    c29_3 = _mm_add_ss(c29_3, _mm_mul_ss(a29_3, b29));
    _mm_store_ss(&C[(l_n*84)+50], c29_3);
    __m128 c29_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a29_4 = _mm_load_ss(&A[145]);
    c29_4 = _mm_add_ss(c29_4, _mm_mul_ss(a29_4, b29));
    _mm_store_ss(&C[(l_n*84)+78], c29_4);
#else
    C[(l_n*84)+4] += A[141] * B[(l_n*84)+29];
    C[(l_n*84)+14] += A[142] * B[(l_n*84)+29];
    C[(l_n*84)+29] += A[143] * B[(l_n*84)+29];
    C[(l_n*84)+50] += A[144] * B[(l_n*84)+29];
    C[(l_n*84)+78] += A[145] * B[(l_n*84)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*84)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*84)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a30_0 = _mm_load_ss(&A[146]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*84)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a30_1 = _mm_load_ss(&A[147]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*84)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a30_2 = _mm_load_ss(&A[148]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*84)+30], c30_2);
    __m128 c30_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a30_3 = _mm_load_ss(&A[149]);
    c30_3 = _mm_add_ss(c30_3, _mm_mul_ss(a30_3, b30));
    _mm_store_ss(&C[(l_n*84)+51], c30_3);
    __m128 c30_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a30_4 = _mm_load_ss(&A[150]);
    c30_4 = _mm_add_ss(c30_4, _mm_mul_ss(a30_4, b30));
    _mm_store_ss(&C[(l_n*84)+79], c30_4);
#else
    C[(l_n*84)+5] += A[146] * B[(l_n*84)+30];
    C[(l_n*84)+15] += A[147] * B[(l_n*84)+30];
    C[(l_n*84)+30] += A[148] * B[(l_n*84)+30];
    C[(l_n*84)+51] += A[149] * B[(l_n*84)+30];
    C[(l_n*84)+79] += A[150] * B[(l_n*84)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*84)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*84)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a31_0 = _mm_load_ss(&A[151]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*84)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a31_1 = _mm_load_ss(&A[152]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*84)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a31_2 = _mm_load_ss(&A[153]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*84)+31], c31_2);
    __m128 c31_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a31_3 = _mm_load_ss(&A[154]);
    c31_3 = _mm_add_ss(c31_3, _mm_mul_ss(a31_3, b31));
    _mm_store_ss(&C[(l_n*84)+52], c31_3);
    __m128 c31_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a31_4 = _mm_load_ss(&A[155]);
    c31_4 = _mm_add_ss(c31_4, _mm_mul_ss(a31_4, b31));
    _mm_store_ss(&C[(l_n*84)+80], c31_4);
#else
    C[(l_n*84)+6] += A[151] * B[(l_n*84)+31];
    C[(l_n*84)+16] += A[152] * B[(l_n*84)+31];
    C[(l_n*84)+31] += A[153] * B[(l_n*84)+31];
    C[(l_n*84)+52] += A[154] * B[(l_n*84)+31];
    C[(l_n*84)+80] += A[155] * B[(l_n*84)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*84)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*84)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a32_0 = _mm_load_ss(&A[156]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*84)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a32_1 = _mm_load_ss(&A[157]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*84)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a32_2 = _mm_load_ss(&A[158]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*84)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a32_3 = _mm_load_ss(&A[159]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*84)+32], c32_3);
    __m128 c32_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a32_4 = _mm_load_ss(&A[160]);
    c32_4 = _mm_add_ss(c32_4, _mm_mul_ss(a32_4, b32));
    _mm_store_ss(&C[(l_n*84)+53], c32_4);
    __m128 c32_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a32_5 = _mm_load_ss(&A[161]);
    c32_5 = _mm_add_ss(c32_5, _mm_mul_ss(a32_5, b32));
    _mm_store_ss(&C[(l_n*84)+81], c32_5);
#else
    C[(l_n*84)+1] += A[156] * B[(l_n*84)+32];
    C[(l_n*84)+7] += A[157] * B[(l_n*84)+32];
    C[(l_n*84)+17] += A[158] * B[(l_n*84)+32];
    C[(l_n*84)+32] += A[159] * B[(l_n*84)+32];
    C[(l_n*84)+53] += A[160] * B[(l_n*84)+32];
    C[(l_n*84)+81] += A[161] * B[(l_n*84)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*84)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*84)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a33_0 = _mm_load_ss(&A[162]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*84)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a33_1 = _mm_load_ss(&A[163]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*84)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a33_2 = _mm_load_ss(&A[164]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*84)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a33_3 = _mm_load_ss(&A[165]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*84)+33], c33_3);
    __m128 c33_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a33_4 = _mm_load_ss(&A[166]);
    c33_4 = _mm_add_ss(c33_4, _mm_mul_ss(a33_4, b33));
    _mm_store_ss(&C[(l_n*84)+54], c33_4);
    __m128 c33_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a33_5 = _mm_load_ss(&A[167]);
    c33_5 = _mm_add_ss(c33_5, _mm_mul_ss(a33_5, b33));
    _mm_store_ss(&C[(l_n*84)+82], c33_5);
#else
    C[(l_n*84)+2] += A[162] * B[(l_n*84)+33];
    C[(l_n*84)+8] += A[163] * B[(l_n*84)+33];
    C[(l_n*84)+18] += A[164] * B[(l_n*84)+33];
    C[(l_n*84)+33] += A[165] * B[(l_n*84)+33];
    C[(l_n*84)+54] += A[166] * B[(l_n*84)+33];
    C[(l_n*84)+82] += A[167] * B[(l_n*84)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*84)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*84)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a34_0 = _mm_load_ss(&A[168]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*84)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a34_1 = _mm_load_ss(&A[169]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*84)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a34_2 = _mm_load_ss(&A[170]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*84)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a34_3 = _mm_load_ss(&A[171]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*84)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a34_4 = _mm_load_ss(&A[172]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*84)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a34_5 = _mm_load_ss(&A[173]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*84)+55], c34_5);
    __m128 c34_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a34_6 = _mm_load_ss(&A[174]);
    c34_6 = _mm_add_ss(c34_6, _mm_mul_ss(a34_6, b34));
    _mm_store_ss(&C[(l_n*84)+83], c34_6);
#else
    C[(l_n*84)+0] += A[168] * B[(l_n*84)+34];
    C[(l_n*84)+3] += A[169] * B[(l_n*84)+34];
    C[(l_n*84)+9] += A[170] * B[(l_n*84)+34];
    C[(l_n*84)+19] += A[171] * B[(l_n*84)+34];
    C[(l_n*84)+34] += A[172] * B[(l_n*84)+34];
    C[(l_n*84)+55] += A[173] * B[(l_n*84)+34];
    C[(l_n*84)+83] += A[174] * B[(l_n*84)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*84)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*84)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*84)+35]);
    __m128 a35_0 = _mm_load_ss(&A[175]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*84)+35], c35_0);
    __m128 c35_1 = _mm_load_ss(&C[(l_n*84)+63]);
    __m128 a35_1 = _mm_load_ss(&A[176]);
    c35_1 = _mm_add_ss(c35_1, _mm_mul_ss(a35_1, b35));
    _mm_store_ss(&C[(l_n*84)+63], c35_1);
#else
    C[(l_n*84)+35] += A[175] * B[(l_n*84)+35];
    C[(l_n*84)+63] += A[176] * B[(l_n*84)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*84)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*84)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*84)+36]);
    __m128 a36_0 = _mm_load_ss(&A[177]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*84)+36], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*84)+64]);
    __m128 a36_1 = _mm_load_ss(&A[178]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*84)+64], c36_1);
#else
    C[(l_n*84)+36] += A[177] * B[(l_n*84)+36];
    C[(l_n*84)+64] += A[178] * B[(l_n*84)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*84)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*84)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*84)+37]);
    __m128 a37_0 = _mm_load_ss(&A[179]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*84)+37], c37_0);
    __m128 c37_1 = _mm_load_ss(&C[(l_n*84)+65]);
    __m128 a37_1 = _mm_load_ss(&A[180]);
    c37_1 = _mm_add_ss(c37_1, _mm_mul_ss(a37_1, b37));
    _mm_store_ss(&C[(l_n*84)+65], c37_1);
#else
    C[(l_n*84)+37] += A[179] * B[(l_n*84)+37];
    C[(l_n*84)+65] += A[180] * B[(l_n*84)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*84)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*84)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*84)+38]);
    __m128 a38_0 = _mm_load_ss(&A[181]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*84)+38], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*84)+66]);
    __m128 a38_1 = _mm_load_ss(&A[182]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*84)+66], c38_1);
#else
    C[(l_n*84)+38] += A[181] * B[(l_n*84)+38];
    C[(l_n*84)+66] += A[182] * B[(l_n*84)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*84)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*84)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*84)+39]);
    __m128 a39_0 = _mm_load_ss(&A[183]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*84)+39], c39_0);
    __m128 c39_1 = _mm_load_ss(&C[(l_n*84)+67]);
    __m128 a39_1 = _mm_load_ss(&A[184]);
    c39_1 = _mm_add_ss(c39_1, _mm_mul_ss(a39_1, b39));
    _mm_store_ss(&C[(l_n*84)+67], c39_1);
#else
    C[(l_n*84)+39] += A[183] * B[(l_n*84)+39];
    C[(l_n*84)+67] += A[184] * B[(l_n*84)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*84)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*84)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_load_ss(&C[(l_n*84)+40]);
    __m128 a40_0 = _mm_load_ss(&A[185]);
    c40_0 = _mm_add_ss(c40_0, _mm_mul_ss(a40_0, b40));
    _mm_store_ss(&C[(l_n*84)+40], c40_0);
    __m128 c40_1 = _mm_load_ss(&C[(l_n*84)+68]);
    __m128 a40_1 = _mm_load_ss(&A[186]);
    c40_1 = _mm_add_ss(c40_1, _mm_mul_ss(a40_1, b40));
    _mm_store_ss(&C[(l_n*84)+68], c40_1);
#else
    C[(l_n*84)+40] += A[185] * B[(l_n*84)+40];
    C[(l_n*84)+68] += A[186] * B[(l_n*84)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*84)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*84)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a41_0 = _mm_load_ss(&A[187]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*84)+20], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a41_1 = _mm_load_ss(&A[188]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*84)+41], c41_1);
    __m128 c41_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a41_2 = _mm_load_ss(&A[189]);
    c41_2 = _mm_add_ss(c41_2, _mm_mul_ss(a41_2, b41));
    _mm_store_ss(&C[(l_n*84)+69], c41_2);
#else
    C[(l_n*84)+20] += A[187] * B[(l_n*84)+41];
    C[(l_n*84)+41] += A[188] * B[(l_n*84)+41];
    C[(l_n*84)+69] += A[189] * B[(l_n*84)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*84)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*84)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a42_0 = _mm_load_ss(&A[190]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*84)+21], c42_0);
    __m128 c42_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a42_1 = _mm_load_ss(&A[191]);
    c42_1 = _mm_add_ss(c42_1, _mm_mul_ss(a42_1, b42));
    _mm_store_ss(&C[(l_n*84)+42], c42_1);
    __m128 c42_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a42_2 = _mm_load_ss(&A[192]);
    c42_2 = _mm_add_ss(c42_2, _mm_mul_ss(a42_2, b42));
    _mm_store_ss(&C[(l_n*84)+70], c42_2);
#else
    C[(l_n*84)+21] += A[190] * B[(l_n*84)+42];
    C[(l_n*84)+42] += A[191] * B[(l_n*84)+42];
    C[(l_n*84)+70] += A[192] * B[(l_n*84)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*84)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*84)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a43_0 = _mm_load_ss(&A[193]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*84)+22], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a43_1 = _mm_load_ss(&A[194]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*84)+43], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a43_2 = _mm_load_ss(&A[195]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*84)+71], c43_2);
#else
    C[(l_n*84)+22] += A[193] * B[(l_n*84)+43];
    C[(l_n*84)+43] += A[194] * B[(l_n*84)+43];
    C[(l_n*84)+71] += A[195] * B[(l_n*84)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*84)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*84)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a44_0 = _mm_load_ss(&A[196]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*84)+23], c44_0);
    __m128 c44_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a44_1 = _mm_load_ss(&A[197]);
    c44_1 = _mm_add_ss(c44_1, _mm_mul_ss(a44_1, b44));
    _mm_store_ss(&C[(l_n*84)+44], c44_1);
    __m128 c44_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a44_2 = _mm_load_ss(&A[198]);
    c44_2 = _mm_add_ss(c44_2, _mm_mul_ss(a44_2, b44));
    _mm_store_ss(&C[(l_n*84)+72], c44_2);
#else
    C[(l_n*84)+23] += A[196] * B[(l_n*84)+44];
    C[(l_n*84)+44] += A[197] * B[(l_n*84)+44];
    C[(l_n*84)+72] += A[198] * B[(l_n*84)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*84)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*84)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a45_0 = _mm_load_ss(&A[199]);
    c45_0 = _mm_add_ss(c45_0, _mm_mul_ss(a45_0, b45));
    _mm_store_ss(&C[(l_n*84)+24], c45_0);
    __m128 c45_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a45_1 = _mm_load_ss(&A[200]);
    c45_1 = _mm_add_ss(c45_1, _mm_mul_ss(a45_1, b45));
    _mm_store_ss(&C[(l_n*84)+45], c45_1);
    __m128 c45_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a45_2 = _mm_load_ss(&A[201]);
    c45_2 = _mm_add_ss(c45_2, _mm_mul_ss(a45_2, b45));
    _mm_store_ss(&C[(l_n*84)+73], c45_2);
#else
    C[(l_n*84)+24] += A[199] * B[(l_n*84)+45];
    C[(l_n*84)+45] += A[200] * B[(l_n*84)+45];
    C[(l_n*84)+73] += A[201] * B[(l_n*84)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*84)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*84)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a46_0 = _mm_load_ss(&A[202]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*84)+10], c46_0);
    __m128 c46_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a46_1 = _mm_load_ss(&A[203]);
    c46_1 = _mm_add_ss(c46_1, _mm_mul_ss(a46_1, b46));
    _mm_store_ss(&C[(l_n*84)+25], c46_1);
    __m128 c46_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a46_2 = _mm_load_ss(&A[204]);
    c46_2 = _mm_add_ss(c46_2, _mm_mul_ss(a46_2, b46));
    _mm_store_ss(&C[(l_n*84)+46], c46_2);
    __m128 c46_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a46_3 = _mm_load_ss(&A[205]);
    c46_3 = _mm_add_ss(c46_3, _mm_mul_ss(a46_3, b46));
    _mm_store_ss(&C[(l_n*84)+74], c46_3);
#else
    C[(l_n*84)+10] += A[202] * B[(l_n*84)+46];
    C[(l_n*84)+25] += A[203] * B[(l_n*84)+46];
    C[(l_n*84)+46] += A[204] * B[(l_n*84)+46];
    C[(l_n*84)+74] += A[205] * B[(l_n*84)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*84)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*84)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a47_0 = _mm_load_ss(&A[206]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*84)+11], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a47_1 = _mm_load_ss(&A[207]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*84)+26], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a47_2 = _mm_load_ss(&A[208]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*84)+47], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a47_3 = _mm_load_ss(&A[209]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*84)+75], c47_3);
#else
    C[(l_n*84)+11] += A[206] * B[(l_n*84)+47];
    C[(l_n*84)+26] += A[207] * B[(l_n*84)+47];
    C[(l_n*84)+47] += A[208] * B[(l_n*84)+47];
    C[(l_n*84)+75] += A[209] * B[(l_n*84)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*84)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*84)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a48_0 = _mm_load_ss(&A[210]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*84)+12], c48_0);
    __m128 c48_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a48_1 = _mm_load_ss(&A[211]);
    c48_1 = _mm_add_ss(c48_1, _mm_mul_ss(a48_1, b48));
    _mm_store_ss(&C[(l_n*84)+27], c48_1);
    __m128 c48_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a48_2 = _mm_load_ss(&A[212]);
    c48_2 = _mm_add_ss(c48_2, _mm_mul_ss(a48_2, b48));
    _mm_store_ss(&C[(l_n*84)+48], c48_2);
    __m128 c48_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a48_3 = _mm_load_ss(&A[213]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*84)+76], c48_3);
#else
    C[(l_n*84)+12] += A[210] * B[(l_n*84)+48];
    C[(l_n*84)+27] += A[211] * B[(l_n*84)+48];
    C[(l_n*84)+48] += A[212] * B[(l_n*84)+48];
    C[(l_n*84)+76] += A[213] * B[(l_n*84)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*84)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*84)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a49_0 = _mm_load_ss(&A[214]);
    c49_0 = _mm_add_ss(c49_0, _mm_mul_ss(a49_0, b49));
    _mm_store_ss(&C[(l_n*84)+13], c49_0);
    __m128 c49_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a49_1 = _mm_load_ss(&A[215]);
    c49_1 = _mm_add_ss(c49_1, _mm_mul_ss(a49_1, b49));
    _mm_store_ss(&C[(l_n*84)+28], c49_1);
    __m128 c49_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a49_2 = _mm_load_ss(&A[216]);
    c49_2 = _mm_add_ss(c49_2, _mm_mul_ss(a49_2, b49));
    _mm_store_ss(&C[(l_n*84)+49], c49_2);
    __m128 c49_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a49_3 = _mm_load_ss(&A[217]);
    c49_3 = _mm_add_ss(c49_3, _mm_mul_ss(a49_3, b49));
    _mm_store_ss(&C[(l_n*84)+77], c49_3);
#else
    C[(l_n*84)+13] += A[214] * B[(l_n*84)+49];
    C[(l_n*84)+28] += A[215] * B[(l_n*84)+49];
    C[(l_n*84)+49] += A[216] * B[(l_n*84)+49];
    C[(l_n*84)+77] += A[217] * B[(l_n*84)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*84)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*84)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a50_0 = _mm_load_ss(&A[218]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*84)+4], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a50_1 = _mm_load_ss(&A[219]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*84)+14], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a50_2 = _mm_load_ss(&A[220]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*84)+29], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a50_3 = _mm_load_ss(&A[221]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*84)+50], c50_3);
    __m128 c50_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a50_4 = _mm_load_ss(&A[222]);
    c50_4 = _mm_add_ss(c50_4, _mm_mul_ss(a50_4, b50));
    _mm_store_ss(&C[(l_n*84)+78], c50_4);
#else
    C[(l_n*84)+4] += A[218] * B[(l_n*84)+50];
    C[(l_n*84)+14] += A[219] * B[(l_n*84)+50];
    C[(l_n*84)+29] += A[220] * B[(l_n*84)+50];
    C[(l_n*84)+50] += A[221] * B[(l_n*84)+50];
    C[(l_n*84)+78] += A[222] * B[(l_n*84)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*84)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*84)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a51_0 = _mm_load_ss(&A[223]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*84)+5], c51_0);
    __m128 c51_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a51_1 = _mm_load_ss(&A[224]);
    c51_1 = _mm_add_ss(c51_1, _mm_mul_ss(a51_1, b51));
    _mm_store_ss(&C[(l_n*84)+15], c51_1);
    __m128 c51_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a51_2 = _mm_load_ss(&A[225]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*84)+30], c51_2);
    __m128 c51_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a51_3 = _mm_load_ss(&A[226]);
    c51_3 = _mm_add_ss(c51_3, _mm_mul_ss(a51_3, b51));
    _mm_store_ss(&C[(l_n*84)+51], c51_3);
    __m128 c51_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a51_4 = _mm_load_ss(&A[227]);
    c51_4 = _mm_add_ss(c51_4, _mm_mul_ss(a51_4, b51));
    _mm_store_ss(&C[(l_n*84)+79], c51_4);
#else
    C[(l_n*84)+5] += A[223] * B[(l_n*84)+51];
    C[(l_n*84)+15] += A[224] * B[(l_n*84)+51];
    C[(l_n*84)+30] += A[225] * B[(l_n*84)+51];
    C[(l_n*84)+51] += A[226] * B[(l_n*84)+51];
    C[(l_n*84)+79] += A[227] * B[(l_n*84)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*84)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*84)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a52_0 = _mm_load_ss(&A[228]);
    c52_0 = _mm_add_ss(c52_0, _mm_mul_ss(a52_0, b52));
    _mm_store_ss(&C[(l_n*84)+6], c52_0);
    __m128 c52_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a52_1 = _mm_load_ss(&A[229]);
    c52_1 = _mm_add_ss(c52_1, _mm_mul_ss(a52_1, b52));
    _mm_store_ss(&C[(l_n*84)+16], c52_1);
    __m128 c52_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a52_2 = _mm_load_ss(&A[230]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*84)+31], c52_2);
    __m128 c52_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a52_3 = _mm_load_ss(&A[231]);
    c52_3 = _mm_add_ss(c52_3, _mm_mul_ss(a52_3, b52));
    _mm_store_ss(&C[(l_n*84)+52], c52_3);
    __m128 c52_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a52_4 = _mm_load_ss(&A[232]);
    c52_4 = _mm_add_ss(c52_4, _mm_mul_ss(a52_4, b52));
    _mm_store_ss(&C[(l_n*84)+80], c52_4);
#else
    C[(l_n*84)+6] += A[228] * B[(l_n*84)+52];
    C[(l_n*84)+16] += A[229] * B[(l_n*84)+52];
    C[(l_n*84)+31] += A[230] * B[(l_n*84)+52];
    C[(l_n*84)+52] += A[231] * B[(l_n*84)+52];
    C[(l_n*84)+80] += A[232] * B[(l_n*84)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*84)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*84)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a53_0 = _mm_load_ss(&A[233]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*84)+1], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a53_1 = _mm_load_ss(&A[234]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*84)+7], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a53_2 = _mm_load_ss(&A[235]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*84)+17], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a53_3 = _mm_load_ss(&A[236]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*84)+32], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a53_4 = _mm_load_ss(&A[237]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*84)+53], c53_4);
    __m128 c53_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a53_5 = _mm_load_ss(&A[238]);
    c53_5 = _mm_add_ss(c53_5, _mm_mul_ss(a53_5, b53));
    _mm_store_ss(&C[(l_n*84)+81], c53_5);
#else
    C[(l_n*84)+1] += A[233] * B[(l_n*84)+53];
    C[(l_n*84)+7] += A[234] * B[(l_n*84)+53];
    C[(l_n*84)+17] += A[235] * B[(l_n*84)+53];
    C[(l_n*84)+32] += A[236] * B[(l_n*84)+53];
    C[(l_n*84)+53] += A[237] * B[(l_n*84)+53];
    C[(l_n*84)+81] += A[238] * B[(l_n*84)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*84)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*84)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a54_0 = _mm_load_ss(&A[239]);
    c54_0 = _mm_add_ss(c54_0, _mm_mul_ss(a54_0, b54));
    _mm_store_ss(&C[(l_n*84)+2], c54_0);
    __m128 c54_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a54_1 = _mm_load_ss(&A[240]);
    c54_1 = _mm_add_ss(c54_1, _mm_mul_ss(a54_1, b54));
    _mm_store_ss(&C[(l_n*84)+8], c54_1);
    __m128 c54_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a54_2 = _mm_load_ss(&A[241]);
    c54_2 = _mm_add_ss(c54_2, _mm_mul_ss(a54_2, b54));
    _mm_store_ss(&C[(l_n*84)+18], c54_2);
    __m128 c54_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a54_3 = _mm_load_ss(&A[242]);
    c54_3 = _mm_add_ss(c54_3, _mm_mul_ss(a54_3, b54));
    _mm_store_ss(&C[(l_n*84)+33], c54_3);
    __m128 c54_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a54_4 = _mm_load_ss(&A[243]);
    c54_4 = _mm_add_ss(c54_4, _mm_mul_ss(a54_4, b54));
    _mm_store_ss(&C[(l_n*84)+54], c54_4);
    __m128 c54_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a54_5 = _mm_load_ss(&A[244]);
    c54_5 = _mm_add_ss(c54_5, _mm_mul_ss(a54_5, b54));
    _mm_store_ss(&C[(l_n*84)+82], c54_5);
#else
    C[(l_n*84)+2] += A[239] * B[(l_n*84)+54];
    C[(l_n*84)+8] += A[240] * B[(l_n*84)+54];
    C[(l_n*84)+18] += A[241] * B[(l_n*84)+54];
    C[(l_n*84)+33] += A[242] * B[(l_n*84)+54];
    C[(l_n*84)+54] += A[243] * B[(l_n*84)+54];
    C[(l_n*84)+82] += A[244] * B[(l_n*84)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*84)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*84)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a55_0 = _mm_load_ss(&A[245]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*84)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a55_1 = _mm_load_ss(&A[246]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*84)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a55_2 = _mm_load_ss(&A[247]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*84)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a55_3 = _mm_load_ss(&A[248]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*84)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a55_4 = _mm_load_ss(&A[249]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*84)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a55_5 = _mm_load_ss(&A[250]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*84)+55], c55_5);
    __m128 c55_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a55_6 = _mm_load_ss(&A[251]);
    c55_6 = _mm_add_ss(c55_6, _mm_mul_ss(a55_6, b55));
    _mm_store_ss(&C[(l_n*84)+83], c55_6);
#else
    C[(l_n*84)+0] += A[245] * B[(l_n*84)+55];
    C[(l_n*84)+3] += A[246] * B[(l_n*84)+55];
    C[(l_n*84)+9] += A[247] * B[(l_n*84)+55];
    C[(l_n*84)+19] += A[248] * B[(l_n*84)+55];
    C[(l_n*84)+34] += A[249] * B[(l_n*84)+55];
    C[(l_n*84)+55] += A[250] * B[(l_n*84)+55];
    C[(l_n*84)+83] += A[251] * B[(l_n*84)+55];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b56 = _mm_broadcast_ss(&B[(l_n*84)+56]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b56 = _mm_load_ss(&B[(l_n*84)+56]);    b56 = _mm_shuffle_ps(b56, b56, 0x00);
#endif
    __m128 c56_0 = _mm_load_ss(&C[(l_n*84)+56]);
    __m128 a56_0 = _mm_load_ss(&A[252]);
    c56_0 = _mm_add_ss(c56_0, _mm_mul_ss(a56_0, b56));
    _mm_store_ss(&C[(l_n*84)+56], c56_0);
#else
    C[(l_n*84)+56] += A[252] * B[(l_n*84)+56];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b57 = _mm_broadcast_ss(&B[(l_n*84)+57]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b57 = _mm_load_ss(&B[(l_n*84)+57]);    b57 = _mm_shuffle_ps(b57, b57, 0x00);
#endif
    __m128 c57_0 = _mm_load_ss(&C[(l_n*84)+57]);
    __m128 a57_0 = _mm_load_ss(&A[253]);
    c57_0 = _mm_add_ss(c57_0, _mm_mul_ss(a57_0, b57));
    _mm_store_ss(&C[(l_n*84)+57], c57_0);
#else
    C[(l_n*84)+57] += A[253] * B[(l_n*84)+57];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b58 = _mm_broadcast_ss(&B[(l_n*84)+58]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b58 = _mm_load_ss(&B[(l_n*84)+58]);    b58 = _mm_shuffle_ps(b58, b58, 0x00);
#endif
    __m128 c58_0 = _mm_load_ss(&C[(l_n*84)+58]);
    __m128 a58_0 = _mm_load_ss(&A[254]);
    c58_0 = _mm_add_ss(c58_0, _mm_mul_ss(a58_0, b58));
    _mm_store_ss(&C[(l_n*84)+58], c58_0);
#else
    C[(l_n*84)+58] += A[254] * B[(l_n*84)+58];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b59 = _mm_broadcast_ss(&B[(l_n*84)+59]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b59 = _mm_load_ss(&B[(l_n*84)+59]);    b59 = _mm_shuffle_ps(b59, b59, 0x00);
#endif
    __m128 c59_0 = _mm_load_ss(&C[(l_n*84)+59]);
    __m128 a59_0 = _mm_load_ss(&A[255]);
    c59_0 = _mm_add_ss(c59_0, _mm_mul_ss(a59_0, b59));
    _mm_store_ss(&C[(l_n*84)+59], c59_0);
#else
    C[(l_n*84)+59] += A[255] * B[(l_n*84)+59];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b60 = _mm_broadcast_ss(&B[(l_n*84)+60]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b60 = _mm_load_ss(&B[(l_n*84)+60]);    b60 = _mm_shuffle_ps(b60, b60, 0x00);
#endif
    __m128 c60_0 = _mm_load_ss(&C[(l_n*84)+60]);
    __m128 a60_0 = _mm_load_ss(&A[256]);
    c60_0 = _mm_add_ss(c60_0, _mm_mul_ss(a60_0, b60));
    _mm_store_ss(&C[(l_n*84)+60], c60_0);
#else
    C[(l_n*84)+60] += A[256] * B[(l_n*84)+60];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b61 = _mm_broadcast_ss(&B[(l_n*84)+61]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b61 = _mm_load_ss(&B[(l_n*84)+61]);    b61 = _mm_shuffle_ps(b61, b61, 0x00);
#endif
    __m128 c61_0 = _mm_load_ss(&C[(l_n*84)+61]);
    __m128 a61_0 = _mm_load_ss(&A[257]);
    c61_0 = _mm_add_ss(c61_0, _mm_mul_ss(a61_0, b61));
    _mm_store_ss(&C[(l_n*84)+61], c61_0);
#else
    C[(l_n*84)+61] += A[257] * B[(l_n*84)+61];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b62 = _mm_broadcast_ss(&B[(l_n*84)+62]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b62 = _mm_load_ss(&B[(l_n*84)+62]);    b62 = _mm_shuffle_ps(b62, b62, 0x00);
#endif
    __m128 c62_0 = _mm_load_ss(&C[(l_n*84)+62]);
    __m128 a62_0 = _mm_load_ss(&A[258]);
    c62_0 = _mm_add_ss(c62_0, _mm_mul_ss(a62_0, b62));
    _mm_store_ss(&C[(l_n*84)+62], c62_0);
#else
    C[(l_n*84)+62] += A[258] * B[(l_n*84)+62];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b63 = _mm_broadcast_ss(&B[(l_n*84)+63]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b63 = _mm_load_ss(&B[(l_n*84)+63]);    b63 = _mm_shuffle_ps(b63, b63, 0x00);
#endif
    __m128 c63_0 = _mm_load_ss(&C[(l_n*84)+35]);
    __m128 a63_0 = _mm_load_ss(&A[259]);
    c63_0 = _mm_add_ss(c63_0, _mm_mul_ss(a63_0, b63));
    _mm_store_ss(&C[(l_n*84)+35], c63_0);
    __m128 c63_1 = _mm_load_ss(&C[(l_n*84)+63]);
    __m128 a63_1 = _mm_load_ss(&A[260]);
    c63_1 = _mm_add_ss(c63_1, _mm_mul_ss(a63_1, b63));
    _mm_store_ss(&C[(l_n*84)+63], c63_1);
#else
    C[(l_n*84)+35] += A[259] * B[(l_n*84)+63];
    C[(l_n*84)+63] += A[260] * B[(l_n*84)+63];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b64 = _mm_broadcast_ss(&B[(l_n*84)+64]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b64 = _mm_load_ss(&B[(l_n*84)+64]);    b64 = _mm_shuffle_ps(b64, b64, 0x00);
#endif
    __m128 c64_0 = _mm_load_ss(&C[(l_n*84)+36]);
    __m128 a64_0 = _mm_load_ss(&A[261]);
    c64_0 = _mm_add_ss(c64_0, _mm_mul_ss(a64_0, b64));
    _mm_store_ss(&C[(l_n*84)+36], c64_0);
    __m128 c64_1 = _mm_load_ss(&C[(l_n*84)+64]);
    __m128 a64_1 = _mm_load_ss(&A[262]);
    c64_1 = _mm_add_ss(c64_1, _mm_mul_ss(a64_1, b64));
    _mm_store_ss(&C[(l_n*84)+64], c64_1);
#else
    C[(l_n*84)+36] += A[261] * B[(l_n*84)+64];
    C[(l_n*84)+64] += A[262] * B[(l_n*84)+64];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b65 = _mm_broadcast_ss(&B[(l_n*84)+65]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b65 = _mm_load_ss(&B[(l_n*84)+65]);    b65 = _mm_shuffle_ps(b65, b65, 0x00);
#endif
    __m128 c65_0 = _mm_load_ss(&C[(l_n*84)+37]);
    __m128 a65_0 = _mm_load_ss(&A[263]);
    c65_0 = _mm_add_ss(c65_0, _mm_mul_ss(a65_0, b65));
    _mm_store_ss(&C[(l_n*84)+37], c65_0);
    __m128 c65_1 = _mm_load_ss(&C[(l_n*84)+65]);
    __m128 a65_1 = _mm_load_ss(&A[264]);
    c65_1 = _mm_add_ss(c65_1, _mm_mul_ss(a65_1, b65));
    _mm_store_ss(&C[(l_n*84)+65], c65_1);
#else
    C[(l_n*84)+37] += A[263] * B[(l_n*84)+65];
    C[(l_n*84)+65] += A[264] * B[(l_n*84)+65];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b66 = _mm_broadcast_ss(&B[(l_n*84)+66]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b66 = _mm_load_ss(&B[(l_n*84)+66]);    b66 = _mm_shuffle_ps(b66, b66, 0x00);
#endif
    __m128 c66_0 = _mm_load_ss(&C[(l_n*84)+38]);
    __m128 a66_0 = _mm_load_ss(&A[265]);
    c66_0 = _mm_add_ss(c66_0, _mm_mul_ss(a66_0, b66));
    _mm_store_ss(&C[(l_n*84)+38], c66_0);
    __m128 c66_1 = _mm_load_ss(&C[(l_n*84)+66]);
    __m128 a66_1 = _mm_load_ss(&A[266]);
    c66_1 = _mm_add_ss(c66_1, _mm_mul_ss(a66_1, b66));
    _mm_store_ss(&C[(l_n*84)+66], c66_1);
#else
    C[(l_n*84)+38] += A[265] * B[(l_n*84)+66];
    C[(l_n*84)+66] += A[266] * B[(l_n*84)+66];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b67 = _mm_broadcast_ss(&B[(l_n*84)+67]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b67 = _mm_load_ss(&B[(l_n*84)+67]);    b67 = _mm_shuffle_ps(b67, b67, 0x00);
#endif
    __m128 c67_0 = _mm_load_ss(&C[(l_n*84)+39]);
    __m128 a67_0 = _mm_load_ss(&A[267]);
    c67_0 = _mm_add_ss(c67_0, _mm_mul_ss(a67_0, b67));
    _mm_store_ss(&C[(l_n*84)+39], c67_0);
    __m128 c67_1 = _mm_load_ss(&C[(l_n*84)+67]);
    __m128 a67_1 = _mm_load_ss(&A[268]);
    c67_1 = _mm_add_ss(c67_1, _mm_mul_ss(a67_1, b67));
    _mm_store_ss(&C[(l_n*84)+67], c67_1);
#else
    C[(l_n*84)+39] += A[267] * B[(l_n*84)+67];
    C[(l_n*84)+67] += A[268] * B[(l_n*84)+67];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b68 = _mm_broadcast_ss(&B[(l_n*84)+68]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b68 = _mm_load_ss(&B[(l_n*84)+68]);    b68 = _mm_shuffle_ps(b68, b68, 0x00);
#endif
    __m128 c68_0 = _mm_load_ss(&C[(l_n*84)+40]);
    __m128 a68_0 = _mm_load_ss(&A[269]);
    c68_0 = _mm_add_ss(c68_0, _mm_mul_ss(a68_0, b68));
    _mm_store_ss(&C[(l_n*84)+40], c68_0);
    __m128 c68_1 = _mm_load_ss(&C[(l_n*84)+68]);
    __m128 a68_1 = _mm_load_ss(&A[270]);
    c68_1 = _mm_add_ss(c68_1, _mm_mul_ss(a68_1, b68));
    _mm_store_ss(&C[(l_n*84)+68], c68_1);
#else
    C[(l_n*84)+40] += A[269] * B[(l_n*84)+68];
    C[(l_n*84)+68] += A[270] * B[(l_n*84)+68];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b69 = _mm_broadcast_ss(&B[(l_n*84)+69]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b69 = _mm_load_ss(&B[(l_n*84)+69]);    b69 = _mm_shuffle_ps(b69, b69, 0x00);
#endif
    __m128 c69_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a69_0 = _mm_load_ss(&A[271]);
    c69_0 = _mm_add_ss(c69_0, _mm_mul_ss(a69_0, b69));
    _mm_store_ss(&C[(l_n*84)+20], c69_0);
    __m128 c69_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a69_1 = _mm_load_ss(&A[272]);
    c69_1 = _mm_add_ss(c69_1, _mm_mul_ss(a69_1, b69));
    _mm_store_ss(&C[(l_n*84)+41], c69_1);
    __m128 c69_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a69_2 = _mm_load_ss(&A[273]);
    c69_2 = _mm_add_ss(c69_2, _mm_mul_ss(a69_2, b69));
    _mm_store_ss(&C[(l_n*84)+69], c69_2);
#else
    C[(l_n*84)+20] += A[271] * B[(l_n*84)+69];
    C[(l_n*84)+41] += A[272] * B[(l_n*84)+69];
    C[(l_n*84)+69] += A[273] * B[(l_n*84)+69];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b70 = _mm_broadcast_ss(&B[(l_n*84)+70]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b70 = _mm_load_ss(&B[(l_n*84)+70]);    b70 = _mm_shuffle_ps(b70, b70, 0x00);
#endif
    __m128 c70_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a70_0 = _mm_load_ss(&A[274]);
    c70_0 = _mm_add_ss(c70_0, _mm_mul_ss(a70_0, b70));
    _mm_store_ss(&C[(l_n*84)+21], c70_0);
    __m128 c70_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a70_1 = _mm_load_ss(&A[275]);
    c70_1 = _mm_add_ss(c70_1, _mm_mul_ss(a70_1, b70));
    _mm_store_ss(&C[(l_n*84)+42], c70_1);
    __m128 c70_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a70_2 = _mm_load_ss(&A[276]);
    c70_2 = _mm_add_ss(c70_2, _mm_mul_ss(a70_2, b70));
    _mm_store_ss(&C[(l_n*84)+70], c70_2);
#else
    C[(l_n*84)+21] += A[274] * B[(l_n*84)+70];
    C[(l_n*84)+42] += A[275] * B[(l_n*84)+70];
    C[(l_n*84)+70] += A[276] * B[(l_n*84)+70];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b71 = _mm_broadcast_ss(&B[(l_n*84)+71]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b71 = _mm_load_ss(&B[(l_n*84)+71]);    b71 = _mm_shuffle_ps(b71, b71, 0x00);
#endif
    __m128 c71_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a71_0 = _mm_load_ss(&A[277]);
    c71_0 = _mm_add_ss(c71_0, _mm_mul_ss(a71_0, b71));
    _mm_store_ss(&C[(l_n*84)+22], c71_0);
    __m128 c71_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a71_1 = _mm_load_ss(&A[278]);
    c71_1 = _mm_add_ss(c71_1, _mm_mul_ss(a71_1, b71));
    _mm_store_ss(&C[(l_n*84)+43], c71_1);
    __m128 c71_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a71_2 = _mm_load_ss(&A[279]);
    c71_2 = _mm_add_ss(c71_2, _mm_mul_ss(a71_2, b71));
    _mm_store_ss(&C[(l_n*84)+71], c71_2);
#else
    C[(l_n*84)+22] += A[277] * B[(l_n*84)+71];
    C[(l_n*84)+43] += A[278] * B[(l_n*84)+71];
    C[(l_n*84)+71] += A[279] * B[(l_n*84)+71];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b72 = _mm_broadcast_ss(&B[(l_n*84)+72]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b72 = _mm_load_ss(&B[(l_n*84)+72]);    b72 = _mm_shuffle_ps(b72, b72, 0x00);
#endif
    __m128 c72_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a72_0 = _mm_load_ss(&A[280]);
    c72_0 = _mm_add_ss(c72_0, _mm_mul_ss(a72_0, b72));
    _mm_store_ss(&C[(l_n*84)+23], c72_0);
    __m128 c72_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a72_1 = _mm_load_ss(&A[281]);
    c72_1 = _mm_add_ss(c72_1, _mm_mul_ss(a72_1, b72));
    _mm_store_ss(&C[(l_n*84)+44], c72_1);
    __m128 c72_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a72_2 = _mm_load_ss(&A[282]);
    c72_2 = _mm_add_ss(c72_2, _mm_mul_ss(a72_2, b72));
    _mm_store_ss(&C[(l_n*84)+72], c72_2);
#else
    C[(l_n*84)+23] += A[280] * B[(l_n*84)+72];
    C[(l_n*84)+44] += A[281] * B[(l_n*84)+72];
    C[(l_n*84)+72] += A[282] * B[(l_n*84)+72];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b73 = _mm_broadcast_ss(&B[(l_n*84)+73]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b73 = _mm_load_ss(&B[(l_n*84)+73]);    b73 = _mm_shuffle_ps(b73, b73, 0x00);
#endif
    __m128 c73_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a73_0 = _mm_load_ss(&A[283]);
    c73_0 = _mm_add_ss(c73_0, _mm_mul_ss(a73_0, b73));
    _mm_store_ss(&C[(l_n*84)+24], c73_0);
    __m128 c73_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a73_1 = _mm_load_ss(&A[284]);
    c73_1 = _mm_add_ss(c73_1, _mm_mul_ss(a73_1, b73));
    _mm_store_ss(&C[(l_n*84)+45], c73_1);
    __m128 c73_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a73_2 = _mm_load_ss(&A[285]);
    c73_2 = _mm_add_ss(c73_2, _mm_mul_ss(a73_2, b73));
    _mm_store_ss(&C[(l_n*84)+73], c73_2);
#else
    C[(l_n*84)+24] += A[283] * B[(l_n*84)+73];
    C[(l_n*84)+45] += A[284] * B[(l_n*84)+73];
    C[(l_n*84)+73] += A[285] * B[(l_n*84)+73];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b74 = _mm_broadcast_ss(&B[(l_n*84)+74]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b74 = _mm_load_ss(&B[(l_n*84)+74]);    b74 = _mm_shuffle_ps(b74, b74, 0x00);
#endif
    __m128 c74_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a74_0 = _mm_load_ss(&A[286]);
    c74_0 = _mm_add_ss(c74_0, _mm_mul_ss(a74_0, b74));
    _mm_store_ss(&C[(l_n*84)+10], c74_0);
    __m128 c74_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a74_1 = _mm_load_ss(&A[287]);
    c74_1 = _mm_add_ss(c74_1, _mm_mul_ss(a74_1, b74));
    _mm_store_ss(&C[(l_n*84)+25], c74_1);
    __m128 c74_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a74_2 = _mm_load_ss(&A[288]);
    c74_2 = _mm_add_ss(c74_2, _mm_mul_ss(a74_2, b74));
    _mm_store_ss(&C[(l_n*84)+46], c74_2);
    __m128 c74_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a74_3 = _mm_load_ss(&A[289]);
    c74_3 = _mm_add_ss(c74_3, _mm_mul_ss(a74_3, b74));
    _mm_store_ss(&C[(l_n*84)+74], c74_3);
#else
    C[(l_n*84)+10] += A[286] * B[(l_n*84)+74];
    C[(l_n*84)+25] += A[287] * B[(l_n*84)+74];
    C[(l_n*84)+46] += A[288] * B[(l_n*84)+74];
    C[(l_n*84)+74] += A[289] * B[(l_n*84)+74];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b75 = _mm_broadcast_ss(&B[(l_n*84)+75]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b75 = _mm_load_ss(&B[(l_n*84)+75]);    b75 = _mm_shuffle_ps(b75, b75, 0x00);
#endif
    __m128 c75_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a75_0 = _mm_load_ss(&A[290]);
    c75_0 = _mm_add_ss(c75_0, _mm_mul_ss(a75_0, b75));
    _mm_store_ss(&C[(l_n*84)+11], c75_0);
    __m128 c75_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a75_1 = _mm_load_ss(&A[291]);
    c75_1 = _mm_add_ss(c75_1, _mm_mul_ss(a75_1, b75));
    _mm_store_ss(&C[(l_n*84)+26], c75_1);
    __m128 c75_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a75_2 = _mm_load_ss(&A[292]);
    c75_2 = _mm_add_ss(c75_2, _mm_mul_ss(a75_2, b75));
    _mm_store_ss(&C[(l_n*84)+47], c75_2);
    __m128 c75_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a75_3 = _mm_load_ss(&A[293]);
    c75_3 = _mm_add_ss(c75_3, _mm_mul_ss(a75_3, b75));
    _mm_store_ss(&C[(l_n*84)+75], c75_3);
#else
    C[(l_n*84)+11] += A[290] * B[(l_n*84)+75];
    C[(l_n*84)+26] += A[291] * B[(l_n*84)+75];
    C[(l_n*84)+47] += A[292] * B[(l_n*84)+75];
    C[(l_n*84)+75] += A[293] * B[(l_n*84)+75];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b76 = _mm_broadcast_ss(&B[(l_n*84)+76]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b76 = _mm_load_ss(&B[(l_n*84)+76]);    b76 = _mm_shuffle_ps(b76, b76, 0x00);
#endif
    __m128 c76_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a76_0 = _mm_load_ss(&A[294]);
    c76_0 = _mm_add_ss(c76_0, _mm_mul_ss(a76_0, b76));
    _mm_store_ss(&C[(l_n*84)+12], c76_0);
    __m128 c76_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a76_1 = _mm_load_ss(&A[295]);
    c76_1 = _mm_add_ss(c76_1, _mm_mul_ss(a76_1, b76));
    _mm_store_ss(&C[(l_n*84)+27], c76_1);
    __m128 c76_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a76_2 = _mm_load_ss(&A[296]);
    c76_2 = _mm_add_ss(c76_2, _mm_mul_ss(a76_2, b76));
    _mm_store_ss(&C[(l_n*84)+48], c76_2);
    __m128 c76_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a76_3 = _mm_load_ss(&A[297]);
    c76_3 = _mm_add_ss(c76_3, _mm_mul_ss(a76_3, b76));
    _mm_store_ss(&C[(l_n*84)+76], c76_3);
#else
    C[(l_n*84)+12] += A[294] * B[(l_n*84)+76];
    C[(l_n*84)+27] += A[295] * B[(l_n*84)+76];
    C[(l_n*84)+48] += A[296] * B[(l_n*84)+76];
    C[(l_n*84)+76] += A[297] * B[(l_n*84)+76];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b77 = _mm_broadcast_ss(&B[(l_n*84)+77]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b77 = _mm_load_ss(&B[(l_n*84)+77]);    b77 = _mm_shuffle_ps(b77, b77, 0x00);
#endif
    __m128 c77_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a77_0 = _mm_load_ss(&A[298]);
    c77_0 = _mm_add_ss(c77_0, _mm_mul_ss(a77_0, b77));
    _mm_store_ss(&C[(l_n*84)+13], c77_0);
    __m128 c77_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a77_1 = _mm_load_ss(&A[299]);
    c77_1 = _mm_add_ss(c77_1, _mm_mul_ss(a77_1, b77));
    _mm_store_ss(&C[(l_n*84)+28], c77_1);
    __m128 c77_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a77_2 = _mm_load_ss(&A[300]);
    c77_2 = _mm_add_ss(c77_2, _mm_mul_ss(a77_2, b77));
    _mm_store_ss(&C[(l_n*84)+49], c77_2);
    __m128 c77_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a77_3 = _mm_load_ss(&A[301]);
    c77_3 = _mm_add_ss(c77_3, _mm_mul_ss(a77_3, b77));
    _mm_store_ss(&C[(l_n*84)+77], c77_3);
#else
    C[(l_n*84)+13] += A[298] * B[(l_n*84)+77];
    C[(l_n*84)+28] += A[299] * B[(l_n*84)+77];
    C[(l_n*84)+49] += A[300] * B[(l_n*84)+77];
    C[(l_n*84)+77] += A[301] * B[(l_n*84)+77];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b78 = _mm_broadcast_ss(&B[(l_n*84)+78]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b78 = _mm_load_ss(&B[(l_n*84)+78]);    b78 = _mm_shuffle_ps(b78, b78, 0x00);
#endif
    __m128 c78_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a78_0 = _mm_load_ss(&A[302]);
    c78_0 = _mm_add_ss(c78_0, _mm_mul_ss(a78_0, b78));
    _mm_store_ss(&C[(l_n*84)+4], c78_0);
    __m128 c78_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a78_1 = _mm_load_ss(&A[303]);
    c78_1 = _mm_add_ss(c78_1, _mm_mul_ss(a78_1, b78));
    _mm_store_ss(&C[(l_n*84)+14], c78_1);
    __m128 c78_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a78_2 = _mm_load_ss(&A[304]);
    c78_2 = _mm_add_ss(c78_2, _mm_mul_ss(a78_2, b78));
    _mm_store_ss(&C[(l_n*84)+29], c78_2);
    __m128 c78_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a78_3 = _mm_load_ss(&A[305]);
    c78_3 = _mm_add_ss(c78_3, _mm_mul_ss(a78_3, b78));
    _mm_store_ss(&C[(l_n*84)+50], c78_3);
    __m128 c78_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a78_4 = _mm_load_ss(&A[306]);
    c78_4 = _mm_add_ss(c78_4, _mm_mul_ss(a78_4, b78));
    _mm_store_ss(&C[(l_n*84)+78], c78_4);
#else
    C[(l_n*84)+4] += A[302] * B[(l_n*84)+78];
    C[(l_n*84)+14] += A[303] * B[(l_n*84)+78];
    C[(l_n*84)+29] += A[304] * B[(l_n*84)+78];
    C[(l_n*84)+50] += A[305] * B[(l_n*84)+78];
    C[(l_n*84)+78] += A[306] * B[(l_n*84)+78];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b79 = _mm_broadcast_ss(&B[(l_n*84)+79]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b79 = _mm_load_ss(&B[(l_n*84)+79]);    b79 = _mm_shuffle_ps(b79, b79, 0x00);
#endif
    __m128 c79_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a79_0 = _mm_load_ss(&A[307]);
    c79_0 = _mm_add_ss(c79_0, _mm_mul_ss(a79_0, b79));
    _mm_store_ss(&C[(l_n*84)+5], c79_0);
    __m128 c79_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a79_1 = _mm_load_ss(&A[308]);
    c79_1 = _mm_add_ss(c79_1, _mm_mul_ss(a79_1, b79));
    _mm_store_ss(&C[(l_n*84)+15], c79_1);
    __m128 c79_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a79_2 = _mm_load_ss(&A[309]);
    c79_2 = _mm_add_ss(c79_2, _mm_mul_ss(a79_2, b79));
    _mm_store_ss(&C[(l_n*84)+30], c79_2);
    __m128 c79_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a79_3 = _mm_load_ss(&A[310]);
    c79_3 = _mm_add_ss(c79_3, _mm_mul_ss(a79_3, b79));
    _mm_store_ss(&C[(l_n*84)+51], c79_3);
    __m128 c79_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a79_4 = _mm_load_ss(&A[311]);
    c79_4 = _mm_add_ss(c79_4, _mm_mul_ss(a79_4, b79));
    _mm_store_ss(&C[(l_n*84)+79], c79_4);
#else
    C[(l_n*84)+5] += A[307] * B[(l_n*84)+79];
    C[(l_n*84)+15] += A[308] * B[(l_n*84)+79];
    C[(l_n*84)+30] += A[309] * B[(l_n*84)+79];
    C[(l_n*84)+51] += A[310] * B[(l_n*84)+79];
    C[(l_n*84)+79] += A[311] * B[(l_n*84)+79];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b80 = _mm_broadcast_ss(&B[(l_n*84)+80]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b80 = _mm_load_ss(&B[(l_n*84)+80]);    b80 = _mm_shuffle_ps(b80, b80, 0x00);
#endif
    __m128 c80_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a80_0 = _mm_load_ss(&A[312]);
    c80_0 = _mm_add_ss(c80_0, _mm_mul_ss(a80_0, b80));
    _mm_store_ss(&C[(l_n*84)+6], c80_0);
    __m128 c80_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a80_1 = _mm_load_ss(&A[313]);
    c80_1 = _mm_add_ss(c80_1, _mm_mul_ss(a80_1, b80));
    _mm_store_ss(&C[(l_n*84)+16], c80_1);
    __m128 c80_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a80_2 = _mm_load_ss(&A[314]);
    c80_2 = _mm_add_ss(c80_2, _mm_mul_ss(a80_2, b80));
    _mm_store_ss(&C[(l_n*84)+31], c80_2);
    __m128 c80_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a80_3 = _mm_load_ss(&A[315]);
    c80_3 = _mm_add_ss(c80_3, _mm_mul_ss(a80_3, b80));
    _mm_store_ss(&C[(l_n*84)+52], c80_3);
    __m128 c80_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a80_4 = _mm_load_ss(&A[316]);
    c80_4 = _mm_add_ss(c80_4, _mm_mul_ss(a80_4, b80));
    _mm_store_ss(&C[(l_n*84)+80], c80_4);
#else
    C[(l_n*84)+6] += A[312] * B[(l_n*84)+80];
    C[(l_n*84)+16] += A[313] * B[(l_n*84)+80];
    C[(l_n*84)+31] += A[314] * B[(l_n*84)+80];
    C[(l_n*84)+52] += A[315] * B[(l_n*84)+80];
    C[(l_n*84)+80] += A[316] * B[(l_n*84)+80];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b81 = _mm_broadcast_ss(&B[(l_n*84)+81]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b81 = _mm_load_ss(&B[(l_n*84)+81]);    b81 = _mm_shuffle_ps(b81, b81, 0x00);
#endif
    __m128 c81_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a81_0 = _mm_load_ss(&A[317]);
    c81_0 = _mm_add_ss(c81_0, _mm_mul_ss(a81_0, b81));
    _mm_store_ss(&C[(l_n*84)+1], c81_0);
    __m128 c81_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a81_1 = _mm_load_ss(&A[318]);
    c81_1 = _mm_add_ss(c81_1, _mm_mul_ss(a81_1, b81));
    _mm_store_ss(&C[(l_n*84)+7], c81_1);
    __m128 c81_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a81_2 = _mm_load_ss(&A[319]);
    c81_2 = _mm_add_ss(c81_2, _mm_mul_ss(a81_2, b81));
    _mm_store_ss(&C[(l_n*84)+17], c81_2);
    __m128 c81_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a81_3 = _mm_load_ss(&A[320]);
    c81_3 = _mm_add_ss(c81_3, _mm_mul_ss(a81_3, b81));
    _mm_store_ss(&C[(l_n*84)+32], c81_3);
    __m128 c81_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a81_4 = _mm_load_ss(&A[321]);
    c81_4 = _mm_add_ss(c81_4, _mm_mul_ss(a81_4, b81));
    _mm_store_ss(&C[(l_n*84)+53], c81_4);
    __m128 c81_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a81_5 = _mm_load_ss(&A[322]);
    c81_5 = _mm_add_ss(c81_5, _mm_mul_ss(a81_5, b81));
    _mm_store_ss(&C[(l_n*84)+81], c81_5);
#else
    C[(l_n*84)+1] += A[317] * B[(l_n*84)+81];
    C[(l_n*84)+7] += A[318] * B[(l_n*84)+81];
    C[(l_n*84)+17] += A[319] * B[(l_n*84)+81];
    C[(l_n*84)+32] += A[320] * B[(l_n*84)+81];
    C[(l_n*84)+53] += A[321] * B[(l_n*84)+81];
    C[(l_n*84)+81] += A[322] * B[(l_n*84)+81];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b82 = _mm_broadcast_ss(&B[(l_n*84)+82]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b82 = _mm_load_ss(&B[(l_n*84)+82]);    b82 = _mm_shuffle_ps(b82, b82, 0x00);
#endif
    __m128 c82_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a82_0 = _mm_load_ss(&A[323]);
    c82_0 = _mm_add_ss(c82_0, _mm_mul_ss(a82_0, b82));
    _mm_store_ss(&C[(l_n*84)+2], c82_0);
    __m128 c82_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a82_1 = _mm_load_ss(&A[324]);
    c82_1 = _mm_add_ss(c82_1, _mm_mul_ss(a82_1, b82));
    _mm_store_ss(&C[(l_n*84)+8], c82_1);
    __m128 c82_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a82_2 = _mm_load_ss(&A[325]);
    c82_2 = _mm_add_ss(c82_2, _mm_mul_ss(a82_2, b82));
    _mm_store_ss(&C[(l_n*84)+18], c82_2);
    __m128 c82_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a82_3 = _mm_load_ss(&A[326]);
    c82_3 = _mm_add_ss(c82_3, _mm_mul_ss(a82_3, b82));
    _mm_store_ss(&C[(l_n*84)+33], c82_3);
    __m128 c82_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a82_4 = _mm_load_ss(&A[327]);
    c82_4 = _mm_add_ss(c82_4, _mm_mul_ss(a82_4, b82));
    _mm_store_ss(&C[(l_n*84)+54], c82_4);
    __m128 c82_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a82_5 = _mm_load_ss(&A[328]);
    c82_5 = _mm_add_ss(c82_5, _mm_mul_ss(a82_5, b82));
    _mm_store_ss(&C[(l_n*84)+82], c82_5);
#else
    C[(l_n*84)+2] += A[323] * B[(l_n*84)+82];
    C[(l_n*84)+8] += A[324] * B[(l_n*84)+82];
    C[(l_n*84)+18] += A[325] * B[(l_n*84)+82];
    C[(l_n*84)+33] += A[326] * B[(l_n*84)+82];
    C[(l_n*84)+54] += A[327] * B[(l_n*84)+82];
    C[(l_n*84)+82] += A[328] * B[(l_n*84)+82];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b83 = _mm_broadcast_ss(&B[(l_n*84)+83]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b83 = _mm_load_ss(&B[(l_n*84)+83]);    b83 = _mm_shuffle_ps(b83, b83, 0x00);
#endif
    __m128 c83_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a83_0 = _mm_load_ss(&A[329]);
    c83_0 = _mm_add_ss(c83_0, _mm_mul_ss(a83_0, b83));
    _mm_store_ss(&C[(l_n*84)+0], c83_0);
    __m128 c83_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a83_1 = _mm_load_ss(&A[330]);
    c83_1 = _mm_add_ss(c83_1, _mm_mul_ss(a83_1, b83));
    _mm_store_ss(&C[(l_n*84)+3], c83_1);
    __m128 c83_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a83_2 = _mm_load_ss(&A[331]);
    c83_2 = _mm_add_ss(c83_2, _mm_mul_ss(a83_2, b83));
    _mm_store_ss(&C[(l_n*84)+9], c83_2);
    __m128 c83_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a83_3 = _mm_load_ss(&A[332]);
    c83_3 = _mm_add_ss(c83_3, _mm_mul_ss(a83_3, b83));
    _mm_store_ss(&C[(l_n*84)+19], c83_3);
    __m128 c83_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a83_4 = _mm_load_ss(&A[333]);
    c83_4 = _mm_add_ss(c83_4, _mm_mul_ss(a83_4, b83));
    _mm_store_ss(&C[(l_n*84)+34], c83_4);
    __m128 c83_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a83_5 = _mm_load_ss(&A[334]);
    c83_5 = _mm_add_ss(c83_5, _mm_mul_ss(a83_5, b83));
    _mm_store_ss(&C[(l_n*84)+55], c83_5);
    __m128 c83_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a83_6 = _mm_load_ss(&A[335]);
    c83_6 = _mm_add_ss(c83_6, _mm_mul_ss(a83_6, b83));
    _mm_store_ss(&C[(l_n*84)+83], c83_6);
#else
    C[(l_n*84)+0] += A[329] * B[(l_n*84)+83];
    C[(l_n*84)+3] += A[330] * B[(l_n*84)+83];
    C[(l_n*84)+9] += A[331] * B[(l_n*84)+83];
    C[(l_n*84)+19] += A[332] * B[(l_n*84)+83];
    C[(l_n*84)+34] += A[333] * B[(l_n*84)+83];
    C[(l_n*84)+55] += A[334] * B[(l_n*84)+83];
    C[(l_n*84)+83] += A[335] * B[(l_n*84)+83];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 6048;
#endif
}

void ssparse_fP113DivM_m84_n9_k84_ldAna7_ldB84_ldC84_beta0_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_n = 0;
  #pragma nounroll_and_jam
  for ( l_n = 0; l_n < 9; l_n++) {
    unsigned int l_m = 0;
   #pragma simd
    for ( l_m = 0; l_m < 84; l_m++) {
      C[(l_n*84)+l_m] = 0.0f;
    }
#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b0 = _mm_broadcast_ss(&B[(l_n*84)+0]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b0 = _mm_load_ss(&B[(l_n*84)+0]);    b0 = _mm_shuffle_ps(b0, b0, 0x00);
#endif
    __m128 c0_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a0_0 = _mm_load_ss(&A[0]);
    c0_0 = _mm_add_ss(c0_0, _mm_mul_ss(a0_0, b0));
    _mm_store_ss(&C[(l_n*84)+0], c0_0);
    __m128 c0_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a0_1 = _mm_load_ss(&A[1]);
    c0_1 = _mm_add_ss(c0_1, _mm_mul_ss(a0_1, b0));
    _mm_store_ss(&C[(l_n*84)+3], c0_1);
    __m128 c0_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a0_2 = _mm_load_ss(&A[2]);
    c0_2 = _mm_add_ss(c0_2, _mm_mul_ss(a0_2, b0));
    _mm_store_ss(&C[(l_n*84)+9], c0_2);
    __m128 c0_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a0_3 = _mm_load_ss(&A[3]);
    c0_3 = _mm_add_ss(c0_3, _mm_mul_ss(a0_3, b0));
    _mm_store_ss(&C[(l_n*84)+19], c0_3);
    __m128 c0_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a0_4 = _mm_load_ss(&A[4]);
    c0_4 = _mm_add_ss(c0_4, _mm_mul_ss(a0_4, b0));
    _mm_store_ss(&C[(l_n*84)+34], c0_4);
    __m128 c0_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a0_5 = _mm_load_ss(&A[5]);
    c0_5 = _mm_add_ss(c0_5, _mm_mul_ss(a0_5, b0));
    _mm_store_ss(&C[(l_n*84)+55], c0_5);
    __m128 c0_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a0_6 = _mm_load_ss(&A[6]);
    c0_6 = _mm_add_ss(c0_6, _mm_mul_ss(a0_6, b0));
    _mm_store_ss(&C[(l_n*84)+83], c0_6);
#else
    C[(l_n*84)+0] += A[0] * B[(l_n*84)+0];
    C[(l_n*84)+3] += A[1] * B[(l_n*84)+0];
    C[(l_n*84)+9] += A[2] * B[(l_n*84)+0];
    C[(l_n*84)+19] += A[3] * B[(l_n*84)+0];
    C[(l_n*84)+34] += A[4] * B[(l_n*84)+0];
    C[(l_n*84)+55] += A[5] * B[(l_n*84)+0];
    C[(l_n*84)+83] += A[6] * B[(l_n*84)+0];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b1 = _mm_broadcast_ss(&B[(l_n*84)+1]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b1 = _mm_load_ss(&B[(l_n*84)+1]);    b1 = _mm_shuffle_ps(b1, b1, 0x00);
#endif
    __m128 c1_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a1_0 = _mm_load_ss(&A[7]);
    c1_0 = _mm_add_ss(c1_0, _mm_mul_ss(a1_0, b1));
    _mm_store_ss(&C[(l_n*84)+1], c1_0);
    __m128 c1_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a1_1 = _mm_load_ss(&A[8]);
    c1_1 = _mm_add_ss(c1_1, _mm_mul_ss(a1_1, b1));
    _mm_store_ss(&C[(l_n*84)+7], c1_1);
    __m128 c1_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a1_2 = _mm_load_ss(&A[9]);
    c1_2 = _mm_add_ss(c1_2, _mm_mul_ss(a1_2, b1));
    _mm_store_ss(&C[(l_n*84)+17], c1_2);
    __m128 c1_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a1_3 = _mm_load_ss(&A[10]);
    c1_3 = _mm_add_ss(c1_3, _mm_mul_ss(a1_3, b1));
    _mm_store_ss(&C[(l_n*84)+32], c1_3);
    __m128 c1_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a1_4 = _mm_load_ss(&A[11]);
    c1_4 = _mm_add_ss(c1_4, _mm_mul_ss(a1_4, b1));
    _mm_store_ss(&C[(l_n*84)+53], c1_4);
    __m128 c1_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a1_5 = _mm_load_ss(&A[12]);
    c1_5 = _mm_add_ss(c1_5, _mm_mul_ss(a1_5, b1));
    _mm_store_ss(&C[(l_n*84)+81], c1_5);
#else
    C[(l_n*84)+1] += A[7] * B[(l_n*84)+1];
    C[(l_n*84)+7] += A[8] * B[(l_n*84)+1];
    C[(l_n*84)+17] += A[9] * B[(l_n*84)+1];
    C[(l_n*84)+32] += A[10] * B[(l_n*84)+1];
    C[(l_n*84)+53] += A[11] * B[(l_n*84)+1];
    C[(l_n*84)+81] += A[12] * B[(l_n*84)+1];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b2 = _mm_broadcast_ss(&B[(l_n*84)+2]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b2 = _mm_load_ss(&B[(l_n*84)+2]);    b2 = _mm_shuffle_ps(b2, b2, 0x00);
#endif
    __m128 c2_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a2_0 = _mm_load_ss(&A[13]);
    c2_0 = _mm_add_ss(c2_0, _mm_mul_ss(a2_0, b2));
    _mm_store_ss(&C[(l_n*84)+2], c2_0);
    __m128 c2_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a2_1 = _mm_load_ss(&A[14]);
    c2_1 = _mm_add_ss(c2_1, _mm_mul_ss(a2_1, b2));
    _mm_store_ss(&C[(l_n*84)+8], c2_1);
    __m128 c2_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a2_2 = _mm_load_ss(&A[15]);
    c2_2 = _mm_add_ss(c2_2, _mm_mul_ss(a2_2, b2));
    _mm_store_ss(&C[(l_n*84)+18], c2_2);
    __m128 c2_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a2_3 = _mm_load_ss(&A[16]);
    c2_3 = _mm_add_ss(c2_3, _mm_mul_ss(a2_3, b2));
    _mm_store_ss(&C[(l_n*84)+33], c2_3);
    __m128 c2_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a2_4 = _mm_load_ss(&A[17]);
    c2_4 = _mm_add_ss(c2_4, _mm_mul_ss(a2_4, b2));
    _mm_store_ss(&C[(l_n*84)+54], c2_4);
    __m128 c2_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a2_5 = _mm_load_ss(&A[18]);
    c2_5 = _mm_add_ss(c2_5, _mm_mul_ss(a2_5, b2));
    _mm_store_ss(&C[(l_n*84)+82], c2_5);
#else
    C[(l_n*84)+2] += A[13] * B[(l_n*84)+2];
    C[(l_n*84)+8] += A[14] * B[(l_n*84)+2];
    C[(l_n*84)+18] += A[15] * B[(l_n*84)+2];
    C[(l_n*84)+33] += A[16] * B[(l_n*84)+2];
    C[(l_n*84)+54] += A[17] * B[(l_n*84)+2];
    C[(l_n*84)+82] += A[18] * B[(l_n*84)+2];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b3 = _mm_broadcast_ss(&B[(l_n*84)+3]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b3 = _mm_load_ss(&B[(l_n*84)+3]);    b3 = _mm_shuffle_ps(b3, b3, 0x00);
#endif
    __m128 c3_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a3_0 = _mm_load_ss(&A[19]);
    c3_0 = _mm_add_ss(c3_0, _mm_mul_ss(a3_0, b3));
    _mm_store_ss(&C[(l_n*84)+0], c3_0);
    __m128 c3_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a3_1 = _mm_load_ss(&A[20]);
    c3_1 = _mm_add_ss(c3_1, _mm_mul_ss(a3_1, b3));
    _mm_store_ss(&C[(l_n*84)+3], c3_1);
    __m128 c3_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a3_2 = _mm_load_ss(&A[21]);
    c3_2 = _mm_add_ss(c3_2, _mm_mul_ss(a3_2, b3));
    _mm_store_ss(&C[(l_n*84)+9], c3_2);
    __m128 c3_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a3_3 = _mm_load_ss(&A[22]);
    c3_3 = _mm_add_ss(c3_3, _mm_mul_ss(a3_3, b3));
    _mm_store_ss(&C[(l_n*84)+19], c3_3);
    __m128 c3_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a3_4 = _mm_load_ss(&A[23]);
    c3_4 = _mm_add_ss(c3_4, _mm_mul_ss(a3_4, b3));
    _mm_store_ss(&C[(l_n*84)+34], c3_4);
    __m128 c3_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a3_5 = _mm_load_ss(&A[24]);
    c3_5 = _mm_add_ss(c3_5, _mm_mul_ss(a3_5, b3));
    _mm_store_ss(&C[(l_n*84)+55], c3_5);
    __m128 c3_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a3_6 = _mm_load_ss(&A[25]);
    c3_6 = _mm_add_ss(c3_6, _mm_mul_ss(a3_6, b3));
    _mm_store_ss(&C[(l_n*84)+83], c3_6);
#else
    C[(l_n*84)+0] += A[19] * B[(l_n*84)+3];
    C[(l_n*84)+3] += A[20] * B[(l_n*84)+3];
    C[(l_n*84)+9] += A[21] * B[(l_n*84)+3];
    C[(l_n*84)+19] += A[22] * B[(l_n*84)+3];
    C[(l_n*84)+34] += A[23] * B[(l_n*84)+3];
    C[(l_n*84)+55] += A[24] * B[(l_n*84)+3];
    C[(l_n*84)+83] += A[25] * B[(l_n*84)+3];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b4 = _mm_broadcast_ss(&B[(l_n*84)+4]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b4 = _mm_load_ss(&B[(l_n*84)+4]);    b4 = _mm_shuffle_ps(b4, b4, 0x00);
#endif
    __m128 c4_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a4_0 = _mm_load_ss(&A[26]);
    c4_0 = _mm_add_ss(c4_0, _mm_mul_ss(a4_0, b4));
    _mm_store_ss(&C[(l_n*84)+4], c4_0);
    __m128 c4_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a4_1 = _mm_load_ss(&A[27]);
    c4_1 = _mm_add_ss(c4_1, _mm_mul_ss(a4_1, b4));
    _mm_store_ss(&C[(l_n*84)+14], c4_1);
    __m128 c4_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a4_2 = _mm_load_ss(&A[28]);
    c4_2 = _mm_add_ss(c4_2, _mm_mul_ss(a4_2, b4));
    _mm_store_ss(&C[(l_n*84)+29], c4_2);
    __m128 c4_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a4_3 = _mm_load_ss(&A[29]);
    c4_3 = _mm_add_ss(c4_3, _mm_mul_ss(a4_3, b4));
    _mm_store_ss(&C[(l_n*84)+50], c4_3);
    __m128 c4_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a4_4 = _mm_load_ss(&A[30]);
    c4_4 = _mm_add_ss(c4_4, _mm_mul_ss(a4_4, b4));
    _mm_store_ss(&C[(l_n*84)+78], c4_4);
#else
    C[(l_n*84)+4] += A[26] * B[(l_n*84)+4];
    C[(l_n*84)+14] += A[27] * B[(l_n*84)+4];
    C[(l_n*84)+29] += A[28] * B[(l_n*84)+4];
    C[(l_n*84)+50] += A[29] * B[(l_n*84)+4];
    C[(l_n*84)+78] += A[30] * B[(l_n*84)+4];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b5 = _mm_broadcast_ss(&B[(l_n*84)+5]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b5 = _mm_load_ss(&B[(l_n*84)+5]);    b5 = _mm_shuffle_ps(b5, b5, 0x00);
#endif
    __m128 c5_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a5_0 = _mm_load_ss(&A[31]);
    c5_0 = _mm_add_ss(c5_0, _mm_mul_ss(a5_0, b5));
    _mm_store_ss(&C[(l_n*84)+5], c5_0);
    __m128 c5_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a5_1 = _mm_load_ss(&A[32]);
    c5_1 = _mm_add_ss(c5_1, _mm_mul_ss(a5_1, b5));
    _mm_store_ss(&C[(l_n*84)+15], c5_1);
    __m128 c5_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a5_2 = _mm_load_ss(&A[33]);
    c5_2 = _mm_add_ss(c5_2, _mm_mul_ss(a5_2, b5));
    _mm_store_ss(&C[(l_n*84)+30], c5_2);
    __m128 c5_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a5_3 = _mm_load_ss(&A[34]);
    c5_3 = _mm_add_ss(c5_3, _mm_mul_ss(a5_3, b5));
    _mm_store_ss(&C[(l_n*84)+51], c5_3);
    __m128 c5_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a5_4 = _mm_load_ss(&A[35]);
    c5_4 = _mm_add_ss(c5_4, _mm_mul_ss(a5_4, b5));
    _mm_store_ss(&C[(l_n*84)+79], c5_4);
#else
    C[(l_n*84)+5] += A[31] * B[(l_n*84)+5];
    C[(l_n*84)+15] += A[32] * B[(l_n*84)+5];
    C[(l_n*84)+30] += A[33] * B[(l_n*84)+5];
    C[(l_n*84)+51] += A[34] * B[(l_n*84)+5];
    C[(l_n*84)+79] += A[35] * B[(l_n*84)+5];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b6 = _mm_broadcast_ss(&B[(l_n*84)+6]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b6 = _mm_load_ss(&B[(l_n*84)+6]);    b6 = _mm_shuffle_ps(b6, b6, 0x00);
#endif
    __m128 c6_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a6_0 = _mm_load_ss(&A[36]);
    c6_0 = _mm_add_ss(c6_0, _mm_mul_ss(a6_0, b6));
    _mm_store_ss(&C[(l_n*84)+6], c6_0);
    __m128 c6_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a6_1 = _mm_load_ss(&A[37]);
    c6_1 = _mm_add_ss(c6_1, _mm_mul_ss(a6_1, b6));
    _mm_store_ss(&C[(l_n*84)+16], c6_1);
    __m128 c6_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a6_2 = _mm_load_ss(&A[38]);
    c6_2 = _mm_add_ss(c6_2, _mm_mul_ss(a6_2, b6));
    _mm_store_ss(&C[(l_n*84)+31], c6_2);
    __m128 c6_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a6_3 = _mm_load_ss(&A[39]);
    c6_3 = _mm_add_ss(c6_3, _mm_mul_ss(a6_3, b6));
    _mm_store_ss(&C[(l_n*84)+52], c6_3);
    __m128 c6_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a6_4 = _mm_load_ss(&A[40]);
    c6_4 = _mm_add_ss(c6_4, _mm_mul_ss(a6_4, b6));
    _mm_store_ss(&C[(l_n*84)+80], c6_4);
#else
    C[(l_n*84)+6] += A[36] * B[(l_n*84)+6];
    C[(l_n*84)+16] += A[37] * B[(l_n*84)+6];
    C[(l_n*84)+31] += A[38] * B[(l_n*84)+6];
    C[(l_n*84)+52] += A[39] * B[(l_n*84)+6];
    C[(l_n*84)+80] += A[40] * B[(l_n*84)+6];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b7 = _mm_broadcast_ss(&B[(l_n*84)+7]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b7 = _mm_load_ss(&B[(l_n*84)+7]);    b7 = _mm_shuffle_ps(b7, b7, 0x00);
#endif
    __m128 c7_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a7_0 = _mm_load_ss(&A[41]);
    c7_0 = _mm_add_ss(c7_0, _mm_mul_ss(a7_0, b7));
    _mm_store_ss(&C[(l_n*84)+1], c7_0);
    __m128 c7_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a7_1 = _mm_load_ss(&A[42]);
    c7_1 = _mm_add_ss(c7_1, _mm_mul_ss(a7_1, b7));
    _mm_store_ss(&C[(l_n*84)+7], c7_1);
    __m128 c7_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a7_2 = _mm_load_ss(&A[43]);
    c7_2 = _mm_add_ss(c7_2, _mm_mul_ss(a7_2, b7));
    _mm_store_ss(&C[(l_n*84)+17], c7_2);
    __m128 c7_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a7_3 = _mm_load_ss(&A[44]);
    c7_3 = _mm_add_ss(c7_3, _mm_mul_ss(a7_3, b7));
    _mm_store_ss(&C[(l_n*84)+32], c7_3);
    __m128 c7_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a7_4 = _mm_load_ss(&A[45]);
    c7_4 = _mm_add_ss(c7_4, _mm_mul_ss(a7_4, b7));
    _mm_store_ss(&C[(l_n*84)+53], c7_4);
    __m128 c7_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a7_5 = _mm_load_ss(&A[46]);
    c7_5 = _mm_add_ss(c7_5, _mm_mul_ss(a7_5, b7));
    _mm_store_ss(&C[(l_n*84)+81], c7_5);
#else
    C[(l_n*84)+1] += A[41] * B[(l_n*84)+7];
    C[(l_n*84)+7] += A[42] * B[(l_n*84)+7];
    C[(l_n*84)+17] += A[43] * B[(l_n*84)+7];
    C[(l_n*84)+32] += A[44] * B[(l_n*84)+7];
    C[(l_n*84)+53] += A[45] * B[(l_n*84)+7];
    C[(l_n*84)+81] += A[46] * B[(l_n*84)+7];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b8 = _mm_broadcast_ss(&B[(l_n*84)+8]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b8 = _mm_load_ss(&B[(l_n*84)+8]);    b8 = _mm_shuffle_ps(b8, b8, 0x00);
#endif
    __m128 c8_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a8_0 = _mm_load_ss(&A[47]);
    c8_0 = _mm_add_ss(c8_0, _mm_mul_ss(a8_0, b8));
    _mm_store_ss(&C[(l_n*84)+2], c8_0);
    __m128 c8_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a8_1 = _mm_load_ss(&A[48]);
    c8_1 = _mm_add_ss(c8_1, _mm_mul_ss(a8_1, b8));
    _mm_store_ss(&C[(l_n*84)+8], c8_1);
    __m128 c8_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a8_2 = _mm_load_ss(&A[49]);
    c8_2 = _mm_add_ss(c8_2, _mm_mul_ss(a8_2, b8));
    _mm_store_ss(&C[(l_n*84)+18], c8_2);
    __m128 c8_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a8_3 = _mm_load_ss(&A[50]);
    c8_3 = _mm_add_ss(c8_3, _mm_mul_ss(a8_3, b8));
    _mm_store_ss(&C[(l_n*84)+33], c8_3);
    __m128 c8_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a8_4 = _mm_load_ss(&A[51]);
    c8_4 = _mm_add_ss(c8_4, _mm_mul_ss(a8_4, b8));
    _mm_store_ss(&C[(l_n*84)+54], c8_4);
    __m128 c8_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a8_5 = _mm_load_ss(&A[52]);
    c8_5 = _mm_add_ss(c8_5, _mm_mul_ss(a8_5, b8));
    _mm_store_ss(&C[(l_n*84)+82], c8_5);
#else
    C[(l_n*84)+2] += A[47] * B[(l_n*84)+8];
    C[(l_n*84)+8] += A[48] * B[(l_n*84)+8];
    C[(l_n*84)+18] += A[49] * B[(l_n*84)+8];
    C[(l_n*84)+33] += A[50] * B[(l_n*84)+8];
    C[(l_n*84)+54] += A[51] * B[(l_n*84)+8];
    C[(l_n*84)+82] += A[52] * B[(l_n*84)+8];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b9 = _mm_broadcast_ss(&B[(l_n*84)+9]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b9 = _mm_load_ss(&B[(l_n*84)+9]);    b9 = _mm_shuffle_ps(b9, b9, 0x00);
#endif
    __m128 c9_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a9_0 = _mm_load_ss(&A[53]);
    c9_0 = _mm_add_ss(c9_0, _mm_mul_ss(a9_0, b9));
    _mm_store_ss(&C[(l_n*84)+0], c9_0);
    __m128 c9_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a9_1 = _mm_load_ss(&A[54]);
    c9_1 = _mm_add_ss(c9_1, _mm_mul_ss(a9_1, b9));
    _mm_store_ss(&C[(l_n*84)+3], c9_1);
    __m128 c9_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a9_2 = _mm_load_ss(&A[55]);
    c9_2 = _mm_add_ss(c9_2, _mm_mul_ss(a9_2, b9));
    _mm_store_ss(&C[(l_n*84)+9], c9_2);
    __m128 c9_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a9_3 = _mm_load_ss(&A[56]);
    c9_3 = _mm_add_ss(c9_3, _mm_mul_ss(a9_3, b9));
    _mm_store_ss(&C[(l_n*84)+19], c9_3);
    __m128 c9_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a9_4 = _mm_load_ss(&A[57]);
    c9_4 = _mm_add_ss(c9_4, _mm_mul_ss(a9_4, b9));
    _mm_store_ss(&C[(l_n*84)+34], c9_4);
    __m128 c9_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a9_5 = _mm_load_ss(&A[58]);
    c9_5 = _mm_add_ss(c9_5, _mm_mul_ss(a9_5, b9));
    _mm_store_ss(&C[(l_n*84)+55], c9_5);
    __m128 c9_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a9_6 = _mm_load_ss(&A[59]);
    c9_6 = _mm_add_ss(c9_6, _mm_mul_ss(a9_6, b9));
    _mm_store_ss(&C[(l_n*84)+83], c9_6);
#else
    C[(l_n*84)+0] += A[53] * B[(l_n*84)+9];
    C[(l_n*84)+3] += A[54] * B[(l_n*84)+9];
    C[(l_n*84)+9] += A[55] * B[(l_n*84)+9];
    C[(l_n*84)+19] += A[56] * B[(l_n*84)+9];
    C[(l_n*84)+34] += A[57] * B[(l_n*84)+9];
    C[(l_n*84)+55] += A[58] * B[(l_n*84)+9];
    C[(l_n*84)+83] += A[59] * B[(l_n*84)+9];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b10 = _mm_broadcast_ss(&B[(l_n*84)+10]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b10 = _mm_load_ss(&B[(l_n*84)+10]);    b10 = _mm_shuffle_ps(b10, b10, 0x00);
#endif
    __m128 c10_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a10_0 = _mm_load_ss(&A[60]);
    c10_0 = _mm_add_ss(c10_0, _mm_mul_ss(a10_0, b10));
    _mm_store_ss(&C[(l_n*84)+10], c10_0);
    __m128 c10_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a10_1 = _mm_load_ss(&A[61]);
    c10_1 = _mm_add_ss(c10_1, _mm_mul_ss(a10_1, b10));
    _mm_store_ss(&C[(l_n*84)+25], c10_1);
    __m128 c10_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a10_2 = _mm_load_ss(&A[62]);
    c10_2 = _mm_add_ss(c10_2, _mm_mul_ss(a10_2, b10));
    _mm_store_ss(&C[(l_n*84)+46], c10_2);
    __m128 c10_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a10_3 = _mm_load_ss(&A[63]);
    c10_3 = _mm_add_ss(c10_3, _mm_mul_ss(a10_3, b10));
    _mm_store_ss(&C[(l_n*84)+74], c10_3);
#else
    C[(l_n*84)+10] += A[60] * B[(l_n*84)+10];
    C[(l_n*84)+25] += A[61] * B[(l_n*84)+10];
    C[(l_n*84)+46] += A[62] * B[(l_n*84)+10];
    C[(l_n*84)+74] += A[63] * B[(l_n*84)+10];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b11 = _mm_broadcast_ss(&B[(l_n*84)+11]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b11 = _mm_load_ss(&B[(l_n*84)+11]);    b11 = _mm_shuffle_ps(b11, b11, 0x00);
#endif
    __m128 c11_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a11_0 = _mm_load_ss(&A[64]);
    c11_0 = _mm_add_ss(c11_0, _mm_mul_ss(a11_0, b11));
    _mm_store_ss(&C[(l_n*84)+11], c11_0);
    __m128 c11_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a11_1 = _mm_load_ss(&A[65]);
    c11_1 = _mm_add_ss(c11_1, _mm_mul_ss(a11_1, b11));
    _mm_store_ss(&C[(l_n*84)+26], c11_1);
    __m128 c11_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a11_2 = _mm_load_ss(&A[66]);
    c11_2 = _mm_add_ss(c11_2, _mm_mul_ss(a11_2, b11));
    _mm_store_ss(&C[(l_n*84)+47], c11_2);
    __m128 c11_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a11_3 = _mm_load_ss(&A[67]);
    c11_3 = _mm_add_ss(c11_3, _mm_mul_ss(a11_3, b11));
    _mm_store_ss(&C[(l_n*84)+75], c11_3);
#else
    C[(l_n*84)+11] += A[64] * B[(l_n*84)+11];
    C[(l_n*84)+26] += A[65] * B[(l_n*84)+11];
    C[(l_n*84)+47] += A[66] * B[(l_n*84)+11];
    C[(l_n*84)+75] += A[67] * B[(l_n*84)+11];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b12 = _mm_broadcast_ss(&B[(l_n*84)+12]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b12 = _mm_load_ss(&B[(l_n*84)+12]);    b12 = _mm_shuffle_ps(b12, b12, 0x00);
#endif
    __m128 c12_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a12_0 = _mm_load_ss(&A[68]);
    c12_0 = _mm_add_ss(c12_0, _mm_mul_ss(a12_0, b12));
    _mm_store_ss(&C[(l_n*84)+12], c12_0);
    __m128 c12_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a12_1 = _mm_load_ss(&A[69]);
    c12_1 = _mm_add_ss(c12_1, _mm_mul_ss(a12_1, b12));
    _mm_store_ss(&C[(l_n*84)+27], c12_1);
    __m128 c12_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a12_2 = _mm_load_ss(&A[70]);
    c12_2 = _mm_add_ss(c12_2, _mm_mul_ss(a12_2, b12));
    _mm_store_ss(&C[(l_n*84)+48], c12_2);
    __m128 c12_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a12_3 = _mm_load_ss(&A[71]);
    c12_3 = _mm_add_ss(c12_3, _mm_mul_ss(a12_3, b12));
    _mm_store_ss(&C[(l_n*84)+76], c12_3);
#else
    C[(l_n*84)+12] += A[68] * B[(l_n*84)+12];
    C[(l_n*84)+27] += A[69] * B[(l_n*84)+12];
    C[(l_n*84)+48] += A[70] * B[(l_n*84)+12];
    C[(l_n*84)+76] += A[71] * B[(l_n*84)+12];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b13 = _mm_broadcast_ss(&B[(l_n*84)+13]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b13 = _mm_load_ss(&B[(l_n*84)+13]);    b13 = _mm_shuffle_ps(b13, b13, 0x00);
#endif
    __m128 c13_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a13_0 = _mm_load_ss(&A[72]);
    c13_0 = _mm_add_ss(c13_0, _mm_mul_ss(a13_0, b13));
    _mm_store_ss(&C[(l_n*84)+13], c13_0);
    __m128 c13_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a13_1 = _mm_load_ss(&A[73]);
    c13_1 = _mm_add_ss(c13_1, _mm_mul_ss(a13_1, b13));
    _mm_store_ss(&C[(l_n*84)+28], c13_1);
    __m128 c13_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a13_2 = _mm_load_ss(&A[74]);
    c13_2 = _mm_add_ss(c13_2, _mm_mul_ss(a13_2, b13));
    _mm_store_ss(&C[(l_n*84)+49], c13_2);
    __m128 c13_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a13_3 = _mm_load_ss(&A[75]);
    c13_3 = _mm_add_ss(c13_3, _mm_mul_ss(a13_3, b13));
    _mm_store_ss(&C[(l_n*84)+77], c13_3);
#else
    C[(l_n*84)+13] += A[72] * B[(l_n*84)+13];
    C[(l_n*84)+28] += A[73] * B[(l_n*84)+13];
    C[(l_n*84)+49] += A[74] * B[(l_n*84)+13];
    C[(l_n*84)+77] += A[75] * B[(l_n*84)+13];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b14 = _mm_broadcast_ss(&B[(l_n*84)+14]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b14 = _mm_load_ss(&B[(l_n*84)+14]);    b14 = _mm_shuffle_ps(b14, b14, 0x00);
#endif
    __m128 c14_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a14_0 = _mm_load_ss(&A[76]);
    c14_0 = _mm_add_ss(c14_0, _mm_mul_ss(a14_0, b14));
    _mm_store_ss(&C[(l_n*84)+4], c14_0);
    __m128 c14_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a14_1 = _mm_load_ss(&A[77]);
    c14_1 = _mm_add_ss(c14_1, _mm_mul_ss(a14_1, b14));
    _mm_store_ss(&C[(l_n*84)+14], c14_1);
    __m128 c14_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a14_2 = _mm_load_ss(&A[78]);
    c14_2 = _mm_add_ss(c14_2, _mm_mul_ss(a14_2, b14));
    _mm_store_ss(&C[(l_n*84)+29], c14_2);
    __m128 c14_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a14_3 = _mm_load_ss(&A[79]);
    c14_3 = _mm_add_ss(c14_3, _mm_mul_ss(a14_3, b14));
    _mm_store_ss(&C[(l_n*84)+50], c14_3);
    __m128 c14_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a14_4 = _mm_load_ss(&A[80]);
    c14_4 = _mm_add_ss(c14_4, _mm_mul_ss(a14_4, b14));
    _mm_store_ss(&C[(l_n*84)+78], c14_4);
#else
    C[(l_n*84)+4] += A[76] * B[(l_n*84)+14];
    C[(l_n*84)+14] += A[77] * B[(l_n*84)+14];
    C[(l_n*84)+29] += A[78] * B[(l_n*84)+14];
    C[(l_n*84)+50] += A[79] * B[(l_n*84)+14];
    C[(l_n*84)+78] += A[80] * B[(l_n*84)+14];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b15 = _mm_broadcast_ss(&B[(l_n*84)+15]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b15 = _mm_load_ss(&B[(l_n*84)+15]);    b15 = _mm_shuffle_ps(b15, b15, 0x00);
#endif
    __m128 c15_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a15_0 = _mm_load_ss(&A[81]);
    c15_0 = _mm_add_ss(c15_0, _mm_mul_ss(a15_0, b15));
    _mm_store_ss(&C[(l_n*84)+5], c15_0);
    __m128 c15_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a15_1 = _mm_load_ss(&A[82]);
    c15_1 = _mm_add_ss(c15_1, _mm_mul_ss(a15_1, b15));
    _mm_store_ss(&C[(l_n*84)+15], c15_1);
    __m128 c15_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a15_2 = _mm_load_ss(&A[83]);
    c15_2 = _mm_add_ss(c15_2, _mm_mul_ss(a15_2, b15));
    _mm_store_ss(&C[(l_n*84)+30], c15_2);
    __m128 c15_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a15_3 = _mm_load_ss(&A[84]);
    c15_3 = _mm_add_ss(c15_3, _mm_mul_ss(a15_3, b15));
    _mm_store_ss(&C[(l_n*84)+51], c15_3);
    __m128 c15_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a15_4 = _mm_load_ss(&A[85]);
    c15_4 = _mm_add_ss(c15_4, _mm_mul_ss(a15_4, b15));
    _mm_store_ss(&C[(l_n*84)+79], c15_4);
#else
    C[(l_n*84)+5] += A[81] * B[(l_n*84)+15];
    C[(l_n*84)+15] += A[82] * B[(l_n*84)+15];
    C[(l_n*84)+30] += A[83] * B[(l_n*84)+15];
    C[(l_n*84)+51] += A[84] * B[(l_n*84)+15];
    C[(l_n*84)+79] += A[85] * B[(l_n*84)+15];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b16 = _mm_broadcast_ss(&B[(l_n*84)+16]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b16 = _mm_load_ss(&B[(l_n*84)+16]);    b16 = _mm_shuffle_ps(b16, b16, 0x00);
#endif
    __m128 c16_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a16_0 = _mm_load_ss(&A[86]);
    c16_0 = _mm_add_ss(c16_0, _mm_mul_ss(a16_0, b16));
    _mm_store_ss(&C[(l_n*84)+6], c16_0);
    __m128 c16_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a16_1 = _mm_load_ss(&A[87]);
    c16_1 = _mm_add_ss(c16_1, _mm_mul_ss(a16_1, b16));
    _mm_store_ss(&C[(l_n*84)+16], c16_1);
    __m128 c16_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a16_2 = _mm_load_ss(&A[88]);
    c16_2 = _mm_add_ss(c16_2, _mm_mul_ss(a16_2, b16));
    _mm_store_ss(&C[(l_n*84)+31], c16_2);
    __m128 c16_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a16_3 = _mm_load_ss(&A[89]);
    c16_3 = _mm_add_ss(c16_3, _mm_mul_ss(a16_3, b16));
    _mm_store_ss(&C[(l_n*84)+52], c16_3);
    __m128 c16_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a16_4 = _mm_load_ss(&A[90]);
    c16_4 = _mm_add_ss(c16_4, _mm_mul_ss(a16_4, b16));
    _mm_store_ss(&C[(l_n*84)+80], c16_4);
#else
    C[(l_n*84)+6] += A[86] * B[(l_n*84)+16];
    C[(l_n*84)+16] += A[87] * B[(l_n*84)+16];
    C[(l_n*84)+31] += A[88] * B[(l_n*84)+16];
    C[(l_n*84)+52] += A[89] * B[(l_n*84)+16];
    C[(l_n*84)+80] += A[90] * B[(l_n*84)+16];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b17 = _mm_broadcast_ss(&B[(l_n*84)+17]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b17 = _mm_load_ss(&B[(l_n*84)+17]);    b17 = _mm_shuffle_ps(b17, b17, 0x00);
#endif
    __m128 c17_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a17_0 = _mm_load_ss(&A[91]);
    c17_0 = _mm_add_ss(c17_0, _mm_mul_ss(a17_0, b17));
    _mm_store_ss(&C[(l_n*84)+1], c17_0);
    __m128 c17_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a17_1 = _mm_load_ss(&A[92]);
    c17_1 = _mm_add_ss(c17_1, _mm_mul_ss(a17_1, b17));
    _mm_store_ss(&C[(l_n*84)+7], c17_1);
    __m128 c17_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a17_2 = _mm_load_ss(&A[93]);
    c17_2 = _mm_add_ss(c17_2, _mm_mul_ss(a17_2, b17));
    _mm_store_ss(&C[(l_n*84)+17], c17_2);
    __m128 c17_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a17_3 = _mm_load_ss(&A[94]);
    c17_3 = _mm_add_ss(c17_3, _mm_mul_ss(a17_3, b17));
    _mm_store_ss(&C[(l_n*84)+32], c17_3);
    __m128 c17_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a17_4 = _mm_load_ss(&A[95]);
    c17_4 = _mm_add_ss(c17_4, _mm_mul_ss(a17_4, b17));
    _mm_store_ss(&C[(l_n*84)+53], c17_4);
    __m128 c17_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a17_5 = _mm_load_ss(&A[96]);
    c17_5 = _mm_add_ss(c17_5, _mm_mul_ss(a17_5, b17));
    _mm_store_ss(&C[(l_n*84)+81], c17_5);
#else
    C[(l_n*84)+1] += A[91] * B[(l_n*84)+17];
    C[(l_n*84)+7] += A[92] * B[(l_n*84)+17];
    C[(l_n*84)+17] += A[93] * B[(l_n*84)+17];
    C[(l_n*84)+32] += A[94] * B[(l_n*84)+17];
    C[(l_n*84)+53] += A[95] * B[(l_n*84)+17];
    C[(l_n*84)+81] += A[96] * B[(l_n*84)+17];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b18 = _mm_broadcast_ss(&B[(l_n*84)+18]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b18 = _mm_load_ss(&B[(l_n*84)+18]);    b18 = _mm_shuffle_ps(b18, b18, 0x00);
#endif
    __m128 c18_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a18_0 = _mm_load_ss(&A[97]);
    c18_0 = _mm_add_ss(c18_0, _mm_mul_ss(a18_0, b18));
    _mm_store_ss(&C[(l_n*84)+2], c18_0);
    __m128 c18_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a18_1 = _mm_load_ss(&A[98]);
    c18_1 = _mm_add_ss(c18_1, _mm_mul_ss(a18_1, b18));
    _mm_store_ss(&C[(l_n*84)+8], c18_1);
    __m128 c18_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a18_2 = _mm_load_ss(&A[99]);
    c18_2 = _mm_add_ss(c18_2, _mm_mul_ss(a18_2, b18));
    _mm_store_ss(&C[(l_n*84)+18], c18_2);
    __m128 c18_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a18_3 = _mm_load_ss(&A[100]);
    c18_3 = _mm_add_ss(c18_3, _mm_mul_ss(a18_3, b18));
    _mm_store_ss(&C[(l_n*84)+33], c18_3);
    __m128 c18_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a18_4 = _mm_load_ss(&A[101]);
    c18_4 = _mm_add_ss(c18_4, _mm_mul_ss(a18_4, b18));
    _mm_store_ss(&C[(l_n*84)+54], c18_4);
    __m128 c18_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a18_5 = _mm_load_ss(&A[102]);
    c18_5 = _mm_add_ss(c18_5, _mm_mul_ss(a18_5, b18));
    _mm_store_ss(&C[(l_n*84)+82], c18_5);
#else
    C[(l_n*84)+2] += A[97] * B[(l_n*84)+18];
    C[(l_n*84)+8] += A[98] * B[(l_n*84)+18];
    C[(l_n*84)+18] += A[99] * B[(l_n*84)+18];
    C[(l_n*84)+33] += A[100] * B[(l_n*84)+18];
    C[(l_n*84)+54] += A[101] * B[(l_n*84)+18];
    C[(l_n*84)+82] += A[102] * B[(l_n*84)+18];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b19 = _mm_broadcast_ss(&B[(l_n*84)+19]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b19 = _mm_load_ss(&B[(l_n*84)+19]);    b19 = _mm_shuffle_ps(b19, b19, 0x00);
#endif
    __m128 c19_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a19_0 = _mm_load_ss(&A[103]);
    c19_0 = _mm_add_ss(c19_0, _mm_mul_ss(a19_0, b19));
    _mm_store_ss(&C[(l_n*84)+0], c19_0);
    __m128 c19_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a19_1 = _mm_load_ss(&A[104]);
    c19_1 = _mm_add_ss(c19_1, _mm_mul_ss(a19_1, b19));
    _mm_store_ss(&C[(l_n*84)+3], c19_1);
    __m128 c19_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a19_2 = _mm_load_ss(&A[105]);
    c19_2 = _mm_add_ss(c19_2, _mm_mul_ss(a19_2, b19));
    _mm_store_ss(&C[(l_n*84)+9], c19_2);
    __m128 c19_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a19_3 = _mm_load_ss(&A[106]);
    c19_3 = _mm_add_ss(c19_3, _mm_mul_ss(a19_3, b19));
    _mm_store_ss(&C[(l_n*84)+19], c19_3);
    __m128 c19_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a19_4 = _mm_load_ss(&A[107]);
    c19_4 = _mm_add_ss(c19_4, _mm_mul_ss(a19_4, b19));
    _mm_store_ss(&C[(l_n*84)+34], c19_4);
    __m128 c19_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a19_5 = _mm_load_ss(&A[108]);
    c19_5 = _mm_add_ss(c19_5, _mm_mul_ss(a19_5, b19));
    _mm_store_ss(&C[(l_n*84)+55], c19_5);
    __m128 c19_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a19_6 = _mm_load_ss(&A[109]);
    c19_6 = _mm_add_ss(c19_6, _mm_mul_ss(a19_6, b19));
    _mm_store_ss(&C[(l_n*84)+83], c19_6);
#else
    C[(l_n*84)+0] += A[103] * B[(l_n*84)+19];
    C[(l_n*84)+3] += A[104] * B[(l_n*84)+19];
    C[(l_n*84)+9] += A[105] * B[(l_n*84)+19];
    C[(l_n*84)+19] += A[106] * B[(l_n*84)+19];
    C[(l_n*84)+34] += A[107] * B[(l_n*84)+19];
    C[(l_n*84)+55] += A[108] * B[(l_n*84)+19];
    C[(l_n*84)+83] += A[109] * B[(l_n*84)+19];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b20 = _mm_broadcast_ss(&B[(l_n*84)+20]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b20 = _mm_load_ss(&B[(l_n*84)+20]);    b20 = _mm_shuffle_ps(b20, b20, 0x00);
#endif
    __m128 c20_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a20_0 = _mm_load_ss(&A[110]);
    c20_0 = _mm_add_ss(c20_0, _mm_mul_ss(a20_0, b20));
    _mm_store_ss(&C[(l_n*84)+20], c20_0);
    __m128 c20_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a20_1 = _mm_load_ss(&A[111]);
    c20_1 = _mm_add_ss(c20_1, _mm_mul_ss(a20_1, b20));
    _mm_store_ss(&C[(l_n*84)+41], c20_1);
    __m128 c20_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a20_2 = _mm_load_ss(&A[112]);
    c20_2 = _mm_add_ss(c20_2, _mm_mul_ss(a20_2, b20));
    _mm_store_ss(&C[(l_n*84)+69], c20_2);
#else
    C[(l_n*84)+20] += A[110] * B[(l_n*84)+20];
    C[(l_n*84)+41] += A[111] * B[(l_n*84)+20];
    C[(l_n*84)+69] += A[112] * B[(l_n*84)+20];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b21 = _mm_broadcast_ss(&B[(l_n*84)+21]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b21 = _mm_load_ss(&B[(l_n*84)+21]);    b21 = _mm_shuffle_ps(b21, b21, 0x00);
#endif
    __m128 c21_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a21_0 = _mm_load_ss(&A[113]);
    c21_0 = _mm_add_ss(c21_0, _mm_mul_ss(a21_0, b21));
    _mm_store_ss(&C[(l_n*84)+21], c21_0);
    __m128 c21_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a21_1 = _mm_load_ss(&A[114]);
    c21_1 = _mm_add_ss(c21_1, _mm_mul_ss(a21_1, b21));
    _mm_store_ss(&C[(l_n*84)+42], c21_1);
    __m128 c21_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a21_2 = _mm_load_ss(&A[115]);
    c21_2 = _mm_add_ss(c21_2, _mm_mul_ss(a21_2, b21));
    _mm_store_ss(&C[(l_n*84)+70], c21_2);
#else
    C[(l_n*84)+21] += A[113] * B[(l_n*84)+21];
    C[(l_n*84)+42] += A[114] * B[(l_n*84)+21];
    C[(l_n*84)+70] += A[115] * B[(l_n*84)+21];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b22 = _mm_broadcast_ss(&B[(l_n*84)+22]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b22 = _mm_load_ss(&B[(l_n*84)+22]);    b22 = _mm_shuffle_ps(b22, b22, 0x00);
#endif
    __m128 c22_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a22_0 = _mm_load_ss(&A[116]);
    c22_0 = _mm_add_ss(c22_0, _mm_mul_ss(a22_0, b22));
    _mm_store_ss(&C[(l_n*84)+22], c22_0);
    __m128 c22_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a22_1 = _mm_load_ss(&A[117]);
    c22_1 = _mm_add_ss(c22_1, _mm_mul_ss(a22_1, b22));
    _mm_store_ss(&C[(l_n*84)+43], c22_1);
    __m128 c22_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a22_2 = _mm_load_ss(&A[118]);
    c22_2 = _mm_add_ss(c22_2, _mm_mul_ss(a22_2, b22));
    _mm_store_ss(&C[(l_n*84)+71], c22_2);
#else
    C[(l_n*84)+22] += A[116] * B[(l_n*84)+22];
    C[(l_n*84)+43] += A[117] * B[(l_n*84)+22];
    C[(l_n*84)+71] += A[118] * B[(l_n*84)+22];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b23 = _mm_broadcast_ss(&B[(l_n*84)+23]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b23 = _mm_load_ss(&B[(l_n*84)+23]);    b23 = _mm_shuffle_ps(b23, b23, 0x00);
#endif
    __m128 c23_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a23_0 = _mm_load_ss(&A[119]);
    c23_0 = _mm_add_ss(c23_0, _mm_mul_ss(a23_0, b23));
    _mm_store_ss(&C[(l_n*84)+23], c23_0);
    __m128 c23_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a23_1 = _mm_load_ss(&A[120]);
    c23_1 = _mm_add_ss(c23_1, _mm_mul_ss(a23_1, b23));
    _mm_store_ss(&C[(l_n*84)+44], c23_1);
    __m128 c23_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a23_2 = _mm_load_ss(&A[121]);
    c23_2 = _mm_add_ss(c23_2, _mm_mul_ss(a23_2, b23));
    _mm_store_ss(&C[(l_n*84)+72], c23_2);
#else
    C[(l_n*84)+23] += A[119] * B[(l_n*84)+23];
    C[(l_n*84)+44] += A[120] * B[(l_n*84)+23];
    C[(l_n*84)+72] += A[121] * B[(l_n*84)+23];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b24 = _mm_broadcast_ss(&B[(l_n*84)+24]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b24 = _mm_load_ss(&B[(l_n*84)+24]);    b24 = _mm_shuffle_ps(b24, b24, 0x00);
#endif
    __m128 c24_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a24_0 = _mm_load_ss(&A[122]);
    c24_0 = _mm_add_ss(c24_0, _mm_mul_ss(a24_0, b24));
    _mm_store_ss(&C[(l_n*84)+24], c24_0);
    __m128 c24_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a24_1 = _mm_load_ss(&A[123]);
    c24_1 = _mm_add_ss(c24_1, _mm_mul_ss(a24_1, b24));
    _mm_store_ss(&C[(l_n*84)+45], c24_1);
    __m128 c24_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a24_2 = _mm_load_ss(&A[124]);
    c24_2 = _mm_add_ss(c24_2, _mm_mul_ss(a24_2, b24));
    _mm_store_ss(&C[(l_n*84)+73], c24_2);
#else
    C[(l_n*84)+24] += A[122] * B[(l_n*84)+24];
    C[(l_n*84)+45] += A[123] * B[(l_n*84)+24];
    C[(l_n*84)+73] += A[124] * B[(l_n*84)+24];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b25 = _mm_broadcast_ss(&B[(l_n*84)+25]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b25 = _mm_load_ss(&B[(l_n*84)+25]);    b25 = _mm_shuffle_ps(b25, b25, 0x00);
#endif
    __m128 c25_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a25_0 = _mm_load_ss(&A[125]);
    c25_0 = _mm_add_ss(c25_0, _mm_mul_ss(a25_0, b25));
    _mm_store_ss(&C[(l_n*84)+10], c25_0);
    __m128 c25_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a25_1 = _mm_load_ss(&A[126]);
    c25_1 = _mm_add_ss(c25_1, _mm_mul_ss(a25_1, b25));
    _mm_store_ss(&C[(l_n*84)+25], c25_1);
    __m128 c25_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a25_2 = _mm_load_ss(&A[127]);
    c25_2 = _mm_add_ss(c25_2, _mm_mul_ss(a25_2, b25));
    _mm_store_ss(&C[(l_n*84)+46], c25_2);
    __m128 c25_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a25_3 = _mm_load_ss(&A[128]);
    c25_3 = _mm_add_ss(c25_3, _mm_mul_ss(a25_3, b25));
    _mm_store_ss(&C[(l_n*84)+74], c25_3);
#else
    C[(l_n*84)+10] += A[125] * B[(l_n*84)+25];
    C[(l_n*84)+25] += A[126] * B[(l_n*84)+25];
    C[(l_n*84)+46] += A[127] * B[(l_n*84)+25];
    C[(l_n*84)+74] += A[128] * B[(l_n*84)+25];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b26 = _mm_broadcast_ss(&B[(l_n*84)+26]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b26 = _mm_load_ss(&B[(l_n*84)+26]);    b26 = _mm_shuffle_ps(b26, b26, 0x00);
#endif
    __m128 c26_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a26_0 = _mm_load_ss(&A[129]);
    c26_0 = _mm_add_ss(c26_0, _mm_mul_ss(a26_0, b26));
    _mm_store_ss(&C[(l_n*84)+11], c26_0);
    __m128 c26_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a26_1 = _mm_load_ss(&A[130]);
    c26_1 = _mm_add_ss(c26_1, _mm_mul_ss(a26_1, b26));
    _mm_store_ss(&C[(l_n*84)+26], c26_1);
    __m128 c26_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a26_2 = _mm_load_ss(&A[131]);
    c26_2 = _mm_add_ss(c26_2, _mm_mul_ss(a26_2, b26));
    _mm_store_ss(&C[(l_n*84)+47], c26_2);
    __m128 c26_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a26_3 = _mm_load_ss(&A[132]);
    c26_3 = _mm_add_ss(c26_3, _mm_mul_ss(a26_3, b26));
    _mm_store_ss(&C[(l_n*84)+75], c26_3);
#else
    C[(l_n*84)+11] += A[129] * B[(l_n*84)+26];
    C[(l_n*84)+26] += A[130] * B[(l_n*84)+26];
    C[(l_n*84)+47] += A[131] * B[(l_n*84)+26];
    C[(l_n*84)+75] += A[132] * B[(l_n*84)+26];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b27 = _mm_broadcast_ss(&B[(l_n*84)+27]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b27 = _mm_load_ss(&B[(l_n*84)+27]);    b27 = _mm_shuffle_ps(b27, b27, 0x00);
#endif
    __m128 c27_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a27_0 = _mm_load_ss(&A[133]);
    c27_0 = _mm_add_ss(c27_0, _mm_mul_ss(a27_0, b27));
    _mm_store_ss(&C[(l_n*84)+12], c27_0);
    __m128 c27_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a27_1 = _mm_load_ss(&A[134]);
    c27_1 = _mm_add_ss(c27_1, _mm_mul_ss(a27_1, b27));
    _mm_store_ss(&C[(l_n*84)+27], c27_1);
    __m128 c27_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a27_2 = _mm_load_ss(&A[135]);
    c27_2 = _mm_add_ss(c27_2, _mm_mul_ss(a27_2, b27));
    _mm_store_ss(&C[(l_n*84)+48], c27_2);
    __m128 c27_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a27_3 = _mm_load_ss(&A[136]);
    c27_3 = _mm_add_ss(c27_3, _mm_mul_ss(a27_3, b27));
    _mm_store_ss(&C[(l_n*84)+76], c27_3);
#else
    C[(l_n*84)+12] += A[133] * B[(l_n*84)+27];
    C[(l_n*84)+27] += A[134] * B[(l_n*84)+27];
    C[(l_n*84)+48] += A[135] * B[(l_n*84)+27];
    C[(l_n*84)+76] += A[136] * B[(l_n*84)+27];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b28 = _mm_broadcast_ss(&B[(l_n*84)+28]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b28 = _mm_load_ss(&B[(l_n*84)+28]);    b28 = _mm_shuffle_ps(b28, b28, 0x00);
#endif
    __m128 c28_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a28_0 = _mm_load_ss(&A[137]);
    c28_0 = _mm_add_ss(c28_0, _mm_mul_ss(a28_0, b28));
    _mm_store_ss(&C[(l_n*84)+13], c28_0);
    __m128 c28_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a28_1 = _mm_load_ss(&A[138]);
    c28_1 = _mm_add_ss(c28_1, _mm_mul_ss(a28_1, b28));
    _mm_store_ss(&C[(l_n*84)+28], c28_1);
    __m128 c28_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a28_2 = _mm_load_ss(&A[139]);
    c28_2 = _mm_add_ss(c28_2, _mm_mul_ss(a28_2, b28));
    _mm_store_ss(&C[(l_n*84)+49], c28_2);
    __m128 c28_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a28_3 = _mm_load_ss(&A[140]);
    c28_3 = _mm_add_ss(c28_3, _mm_mul_ss(a28_3, b28));
    _mm_store_ss(&C[(l_n*84)+77], c28_3);
#else
    C[(l_n*84)+13] += A[137] * B[(l_n*84)+28];
    C[(l_n*84)+28] += A[138] * B[(l_n*84)+28];
    C[(l_n*84)+49] += A[139] * B[(l_n*84)+28];
    C[(l_n*84)+77] += A[140] * B[(l_n*84)+28];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b29 = _mm_broadcast_ss(&B[(l_n*84)+29]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b29 = _mm_load_ss(&B[(l_n*84)+29]);    b29 = _mm_shuffle_ps(b29, b29, 0x00);
#endif
    __m128 c29_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a29_0 = _mm_load_ss(&A[141]);
    c29_0 = _mm_add_ss(c29_0, _mm_mul_ss(a29_0, b29));
    _mm_store_ss(&C[(l_n*84)+4], c29_0);
    __m128 c29_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a29_1 = _mm_load_ss(&A[142]);
    c29_1 = _mm_add_ss(c29_1, _mm_mul_ss(a29_1, b29));
    _mm_store_ss(&C[(l_n*84)+14], c29_1);
    __m128 c29_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a29_2 = _mm_load_ss(&A[143]);
    c29_2 = _mm_add_ss(c29_2, _mm_mul_ss(a29_2, b29));
    _mm_store_ss(&C[(l_n*84)+29], c29_2);
    __m128 c29_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a29_3 = _mm_load_ss(&A[144]);
    c29_3 = _mm_add_ss(c29_3, _mm_mul_ss(a29_3, b29));
    _mm_store_ss(&C[(l_n*84)+50], c29_3);
    __m128 c29_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a29_4 = _mm_load_ss(&A[145]);
    c29_4 = _mm_add_ss(c29_4, _mm_mul_ss(a29_4, b29));
    _mm_store_ss(&C[(l_n*84)+78], c29_4);
#else
    C[(l_n*84)+4] += A[141] * B[(l_n*84)+29];
    C[(l_n*84)+14] += A[142] * B[(l_n*84)+29];
    C[(l_n*84)+29] += A[143] * B[(l_n*84)+29];
    C[(l_n*84)+50] += A[144] * B[(l_n*84)+29];
    C[(l_n*84)+78] += A[145] * B[(l_n*84)+29];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b30 = _mm_broadcast_ss(&B[(l_n*84)+30]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b30 = _mm_load_ss(&B[(l_n*84)+30]);    b30 = _mm_shuffle_ps(b30, b30, 0x00);
#endif
    __m128 c30_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a30_0 = _mm_load_ss(&A[146]);
    c30_0 = _mm_add_ss(c30_0, _mm_mul_ss(a30_0, b30));
    _mm_store_ss(&C[(l_n*84)+5], c30_0);
    __m128 c30_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a30_1 = _mm_load_ss(&A[147]);
    c30_1 = _mm_add_ss(c30_1, _mm_mul_ss(a30_1, b30));
    _mm_store_ss(&C[(l_n*84)+15], c30_1);
    __m128 c30_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a30_2 = _mm_load_ss(&A[148]);
    c30_2 = _mm_add_ss(c30_2, _mm_mul_ss(a30_2, b30));
    _mm_store_ss(&C[(l_n*84)+30], c30_2);
    __m128 c30_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a30_3 = _mm_load_ss(&A[149]);
    c30_3 = _mm_add_ss(c30_3, _mm_mul_ss(a30_3, b30));
    _mm_store_ss(&C[(l_n*84)+51], c30_3);
    __m128 c30_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a30_4 = _mm_load_ss(&A[150]);
    c30_4 = _mm_add_ss(c30_4, _mm_mul_ss(a30_4, b30));
    _mm_store_ss(&C[(l_n*84)+79], c30_4);
#else
    C[(l_n*84)+5] += A[146] * B[(l_n*84)+30];
    C[(l_n*84)+15] += A[147] * B[(l_n*84)+30];
    C[(l_n*84)+30] += A[148] * B[(l_n*84)+30];
    C[(l_n*84)+51] += A[149] * B[(l_n*84)+30];
    C[(l_n*84)+79] += A[150] * B[(l_n*84)+30];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b31 = _mm_broadcast_ss(&B[(l_n*84)+31]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b31 = _mm_load_ss(&B[(l_n*84)+31]);    b31 = _mm_shuffle_ps(b31, b31, 0x00);
#endif
    __m128 c31_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a31_0 = _mm_load_ss(&A[151]);
    c31_0 = _mm_add_ss(c31_0, _mm_mul_ss(a31_0, b31));
    _mm_store_ss(&C[(l_n*84)+6], c31_0);
    __m128 c31_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a31_1 = _mm_load_ss(&A[152]);
    c31_1 = _mm_add_ss(c31_1, _mm_mul_ss(a31_1, b31));
    _mm_store_ss(&C[(l_n*84)+16], c31_1);
    __m128 c31_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a31_2 = _mm_load_ss(&A[153]);
    c31_2 = _mm_add_ss(c31_2, _mm_mul_ss(a31_2, b31));
    _mm_store_ss(&C[(l_n*84)+31], c31_2);
    __m128 c31_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a31_3 = _mm_load_ss(&A[154]);
    c31_3 = _mm_add_ss(c31_3, _mm_mul_ss(a31_3, b31));
    _mm_store_ss(&C[(l_n*84)+52], c31_3);
    __m128 c31_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a31_4 = _mm_load_ss(&A[155]);
    c31_4 = _mm_add_ss(c31_4, _mm_mul_ss(a31_4, b31));
    _mm_store_ss(&C[(l_n*84)+80], c31_4);
#else
    C[(l_n*84)+6] += A[151] * B[(l_n*84)+31];
    C[(l_n*84)+16] += A[152] * B[(l_n*84)+31];
    C[(l_n*84)+31] += A[153] * B[(l_n*84)+31];
    C[(l_n*84)+52] += A[154] * B[(l_n*84)+31];
    C[(l_n*84)+80] += A[155] * B[(l_n*84)+31];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b32 = _mm_broadcast_ss(&B[(l_n*84)+32]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b32 = _mm_load_ss(&B[(l_n*84)+32]);    b32 = _mm_shuffle_ps(b32, b32, 0x00);
#endif
    __m128 c32_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a32_0 = _mm_load_ss(&A[156]);
    c32_0 = _mm_add_ss(c32_0, _mm_mul_ss(a32_0, b32));
    _mm_store_ss(&C[(l_n*84)+1], c32_0);
    __m128 c32_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a32_1 = _mm_load_ss(&A[157]);
    c32_1 = _mm_add_ss(c32_1, _mm_mul_ss(a32_1, b32));
    _mm_store_ss(&C[(l_n*84)+7], c32_1);
    __m128 c32_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a32_2 = _mm_load_ss(&A[158]);
    c32_2 = _mm_add_ss(c32_2, _mm_mul_ss(a32_2, b32));
    _mm_store_ss(&C[(l_n*84)+17], c32_2);
    __m128 c32_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a32_3 = _mm_load_ss(&A[159]);
    c32_3 = _mm_add_ss(c32_3, _mm_mul_ss(a32_3, b32));
    _mm_store_ss(&C[(l_n*84)+32], c32_3);
    __m128 c32_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a32_4 = _mm_load_ss(&A[160]);
    c32_4 = _mm_add_ss(c32_4, _mm_mul_ss(a32_4, b32));
    _mm_store_ss(&C[(l_n*84)+53], c32_4);
    __m128 c32_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a32_5 = _mm_load_ss(&A[161]);
    c32_5 = _mm_add_ss(c32_5, _mm_mul_ss(a32_5, b32));
    _mm_store_ss(&C[(l_n*84)+81], c32_5);
#else
    C[(l_n*84)+1] += A[156] * B[(l_n*84)+32];
    C[(l_n*84)+7] += A[157] * B[(l_n*84)+32];
    C[(l_n*84)+17] += A[158] * B[(l_n*84)+32];
    C[(l_n*84)+32] += A[159] * B[(l_n*84)+32];
    C[(l_n*84)+53] += A[160] * B[(l_n*84)+32];
    C[(l_n*84)+81] += A[161] * B[(l_n*84)+32];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b33 = _mm_broadcast_ss(&B[(l_n*84)+33]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b33 = _mm_load_ss(&B[(l_n*84)+33]);    b33 = _mm_shuffle_ps(b33, b33, 0x00);
#endif
    __m128 c33_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a33_0 = _mm_load_ss(&A[162]);
    c33_0 = _mm_add_ss(c33_0, _mm_mul_ss(a33_0, b33));
    _mm_store_ss(&C[(l_n*84)+2], c33_0);
    __m128 c33_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a33_1 = _mm_load_ss(&A[163]);
    c33_1 = _mm_add_ss(c33_1, _mm_mul_ss(a33_1, b33));
    _mm_store_ss(&C[(l_n*84)+8], c33_1);
    __m128 c33_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a33_2 = _mm_load_ss(&A[164]);
    c33_2 = _mm_add_ss(c33_2, _mm_mul_ss(a33_2, b33));
    _mm_store_ss(&C[(l_n*84)+18], c33_2);
    __m128 c33_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a33_3 = _mm_load_ss(&A[165]);
    c33_3 = _mm_add_ss(c33_3, _mm_mul_ss(a33_3, b33));
    _mm_store_ss(&C[(l_n*84)+33], c33_3);
    __m128 c33_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a33_4 = _mm_load_ss(&A[166]);
    c33_4 = _mm_add_ss(c33_4, _mm_mul_ss(a33_4, b33));
    _mm_store_ss(&C[(l_n*84)+54], c33_4);
    __m128 c33_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a33_5 = _mm_load_ss(&A[167]);
    c33_5 = _mm_add_ss(c33_5, _mm_mul_ss(a33_5, b33));
    _mm_store_ss(&C[(l_n*84)+82], c33_5);
#else
    C[(l_n*84)+2] += A[162] * B[(l_n*84)+33];
    C[(l_n*84)+8] += A[163] * B[(l_n*84)+33];
    C[(l_n*84)+18] += A[164] * B[(l_n*84)+33];
    C[(l_n*84)+33] += A[165] * B[(l_n*84)+33];
    C[(l_n*84)+54] += A[166] * B[(l_n*84)+33];
    C[(l_n*84)+82] += A[167] * B[(l_n*84)+33];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b34 = _mm_broadcast_ss(&B[(l_n*84)+34]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b34 = _mm_load_ss(&B[(l_n*84)+34]);    b34 = _mm_shuffle_ps(b34, b34, 0x00);
#endif
    __m128 c34_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a34_0 = _mm_load_ss(&A[168]);
    c34_0 = _mm_add_ss(c34_0, _mm_mul_ss(a34_0, b34));
    _mm_store_ss(&C[(l_n*84)+0], c34_0);
    __m128 c34_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a34_1 = _mm_load_ss(&A[169]);
    c34_1 = _mm_add_ss(c34_1, _mm_mul_ss(a34_1, b34));
    _mm_store_ss(&C[(l_n*84)+3], c34_1);
    __m128 c34_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a34_2 = _mm_load_ss(&A[170]);
    c34_2 = _mm_add_ss(c34_2, _mm_mul_ss(a34_2, b34));
    _mm_store_ss(&C[(l_n*84)+9], c34_2);
    __m128 c34_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a34_3 = _mm_load_ss(&A[171]);
    c34_3 = _mm_add_ss(c34_3, _mm_mul_ss(a34_3, b34));
    _mm_store_ss(&C[(l_n*84)+19], c34_3);
    __m128 c34_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a34_4 = _mm_load_ss(&A[172]);
    c34_4 = _mm_add_ss(c34_4, _mm_mul_ss(a34_4, b34));
    _mm_store_ss(&C[(l_n*84)+34], c34_4);
    __m128 c34_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a34_5 = _mm_load_ss(&A[173]);
    c34_5 = _mm_add_ss(c34_5, _mm_mul_ss(a34_5, b34));
    _mm_store_ss(&C[(l_n*84)+55], c34_5);
    __m128 c34_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a34_6 = _mm_load_ss(&A[174]);
    c34_6 = _mm_add_ss(c34_6, _mm_mul_ss(a34_6, b34));
    _mm_store_ss(&C[(l_n*84)+83], c34_6);
#else
    C[(l_n*84)+0] += A[168] * B[(l_n*84)+34];
    C[(l_n*84)+3] += A[169] * B[(l_n*84)+34];
    C[(l_n*84)+9] += A[170] * B[(l_n*84)+34];
    C[(l_n*84)+19] += A[171] * B[(l_n*84)+34];
    C[(l_n*84)+34] += A[172] * B[(l_n*84)+34];
    C[(l_n*84)+55] += A[173] * B[(l_n*84)+34];
    C[(l_n*84)+83] += A[174] * B[(l_n*84)+34];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b35 = _mm_broadcast_ss(&B[(l_n*84)+35]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b35 = _mm_load_ss(&B[(l_n*84)+35]);    b35 = _mm_shuffle_ps(b35, b35, 0x00);
#endif
    __m128 c35_0 = _mm_load_ss(&C[(l_n*84)+35]);
    __m128 a35_0 = _mm_load_ss(&A[175]);
    c35_0 = _mm_add_ss(c35_0, _mm_mul_ss(a35_0, b35));
    _mm_store_ss(&C[(l_n*84)+35], c35_0);
    __m128 c35_1 = _mm_load_ss(&C[(l_n*84)+63]);
    __m128 a35_1 = _mm_load_ss(&A[176]);
    c35_1 = _mm_add_ss(c35_1, _mm_mul_ss(a35_1, b35));
    _mm_store_ss(&C[(l_n*84)+63], c35_1);
#else
    C[(l_n*84)+35] += A[175] * B[(l_n*84)+35];
    C[(l_n*84)+63] += A[176] * B[(l_n*84)+35];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b36 = _mm_broadcast_ss(&B[(l_n*84)+36]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b36 = _mm_load_ss(&B[(l_n*84)+36]);    b36 = _mm_shuffle_ps(b36, b36, 0x00);
#endif
    __m128 c36_0 = _mm_load_ss(&C[(l_n*84)+36]);
    __m128 a36_0 = _mm_load_ss(&A[177]);
    c36_0 = _mm_add_ss(c36_0, _mm_mul_ss(a36_0, b36));
    _mm_store_ss(&C[(l_n*84)+36], c36_0);
    __m128 c36_1 = _mm_load_ss(&C[(l_n*84)+64]);
    __m128 a36_1 = _mm_load_ss(&A[178]);
    c36_1 = _mm_add_ss(c36_1, _mm_mul_ss(a36_1, b36));
    _mm_store_ss(&C[(l_n*84)+64], c36_1);
#else
    C[(l_n*84)+36] += A[177] * B[(l_n*84)+36];
    C[(l_n*84)+64] += A[178] * B[(l_n*84)+36];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b37 = _mm_broadcast_ss(&B[(l_n*84)+37]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b37 = _mm_load_ss(&B[(l_n*84)+37]);    b37 = _mm_shuffle_ps(b37, b37, 0x00);
#endif
    __m128 c37_0 = _mm_load_ss(&C[(l_n*84)+37]);
    __m128 a37_0 = _mm_load_ss(&A[179]);
    c37_0 = _mm_add_ss(c37_0, _mm_mul_ss(a37_0, b37));
    _mm_store_ss(&C[(l_n*84)+37], c37_0);
    __m128 c37_1 = _mm_load_ss(&C[(l_n*84)+65]);
    __m128 a37_1 = _mm_load_ss(&A[180]);
    c37_1 = _mm_add_ss(c37_1, _mm_mul_ss(a37_1, b37));
    _mm_store_ss(&C[(l_n*84)+65], c37_1);
#else
    C[(l_n*84)+37] += A[179] * B[(l_n*84)+37];
    C[(l_n*84)+65] += A[180] * B[(l_n*84)+37];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b38 = _mm_broadcast_ss(&B[(l_n*84)+38]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b38 = _mm_load_ss(&B[(l_n*84)+38]);    b38 = _mm_shuffle_ps(b38, b38, 0x00);
#endif
    __m128 c38_0 = _mm_load_ss(&C[(l_n*84)+38]);
    __m128 a38_0 = _mm_load_ss(&A[181]);
    c38_0 = _mm_add_ss(c38_0, _mm_mul_ss(a38_0, b38));
    _mm_store_ss(&C[(l_n*84)+38], c38_0);
    __m128 c38_1 = _mm_load_ss(&C[(l_n*84)+66]);
    __m128 a38_1 = _mm_load_ss(&A[182]);
    c38_1 = _mm_add_ss(c38_1, _mm_mul_ss(a38_1, b38));
    _mm_store_ss(&C[(l_n*84)+66], c38_1);
#else
    C[(l_n*84)+38] += A[181] * B[(l_n*84)+38];
    C[(l_n*84)+66] += A[182] * B[(l_n*84)+38];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b39 = _mm_broadcast_ss(&B[(l_n*84)+39]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b39 = _mm_load_ss(&B[(l_n*84)+39]);    b39 = _mm_shuffle_ps(b39, b39, 0x00);
#endif
    __m128 c39_0 = _mm_load_ss(&C[(l_n*84)+39]);
    __m128 a39_0 = _mm_load_ss(&A[183]);
    c39_0 = _mm_add_ss(c39_0, _mm_mul_ss(a39_0, b39));
    _mm_store_ss(&C[(l_n*84)+39], c39_0);
    __m128 c39_1 = _mm_load_ss(&C[(l_n*84)+67]);
    __m128 a39_1 = _mm_load_ss(&A[184]);
    c39_1 = _mm_add_ss(c39_1, _mm_mul_ss(a39_1, b39));
    _mm_store_ss(&C[(l_n*84)+67], c39_1);
#else
    C[(l_n*84)+39] += A[183] * B[(l_n*84)+39];
    C[(l_n*84)+67] += A[184] * B[(l_n*84)+39];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b40 = _mm_broadcast_ss(&B[(l_n*84)+40]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b40 = _mm_load_ss(&B[(l_n*84)+40]);    b40 = _mm_shuffle_ps(b40, b40, 0x00);
#endif
    __m128 c40_0 = _mm_load_ss(&C[(l_n*84)+40]);
    __m128 a40_0 = _mm_load_ss(&A[185]);
    c40_0 = _mm_add_ss(c40_0, _mm_mul_ss(a40_0, b40));
    _mm_store_ss(&C[(l_n*84)+40], c40_0);
    __m128 c40_1 = _mm_load_ss(&C[(l_n*84)+68]);
    __m128 a40_1 = _mm_load_ss(&A[186]);
    c40_1 = _mm_add_ss(c40_1, _mm_mul_ss(a40_1, b40));
    _mm_store_ss(&C[(l_n*84)+68], c40_1);
#else
    C[(l_n*84)+40] += A[185] * B[(l_n*84)+40];
    C[(l_n*84)+68] += A[186] * B[(l_n*84)+40];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b41 = _mm_broadcast_ss(&B[(l_n*84)+41]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b41 = _mm_load_ss(&B[(l_n*84)+41]);    b41 = _mm_shuffle_ps(b41, b41, 0x00);
#endif
    __m128 c41_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a41_0 = _mm_load_ss(&A[187]);
    c41_0 = _mm_add_ss(c41_0, _mm_mul_ss(a41_0, b41));
    _mm_store_ss(&C[(l_n*84)+20], c41_0);
    __m128 c41_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a41_1 = _mm_load_ss(&A[188]);
    c41_1 = _mm_add_ss(c41_1, _mm_mul_ss(a41_1, b41));
    _mm_store_ss(&C[(l_n*84)+41], c41_1);
    __m128 c41_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a41_2 = _mm_load_ss(&A[189]);
    c41_2 = _mm_add_ss(c41_2, _mm_mul_ss(a41_2, b41));
    _mm_store_ss(&C[(l_n*84)+69], c41_2);
#else
    C[(l_n*84)+20] += A[187] * B[(l_n*84)+41];
    C[(l_n*84)+41] += A[188] * B[(l_n*84)+41];
    C[(l_n*84)+69] += A[189] * B[(l_n*84)+41];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b42 = _mm_broadcast_ss(&B[(l_n*84)+42]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b42 = _mm_load_ss(&B[(l_n*84)+42]);    b42 = _mm_shuffle_ps(b42, b42, 0x00);
#endif
    __m128 c42_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a42_0 = _mm_load_ss(&A[190]);
    c42_0 = _mm_add_ss(c42_0, _mm_mul_ss(a42_0, b42));
    _mm_store_ss(&C[(l_n*84)+21], c42_0);
    __m128 c42_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a42_1 = _mm_load_ss(&A[191]);
    c42_1 = _mm_add_ss(c42_1, _mm_mul_ss(a42_1, b42));
    _mm_store_ss(&C[(l_n*84)+42], c42_1);
    __m128 c42_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a42_2 = _mm_load_ss(&A[192]);
    c42_2 = _mm_add_ss(c42_2, _mm_mul_ss(a42_2, b42));
    _mm_store_ss(&C[(l_n*84)+70], c42_2);
#else
    C[(l_n*84)+21] += A[190] * B[(l_n*84)+42];
    C[(l_n*84)+42] += A[191] * B[(l_n*84)+42];
    C[(l_n*84)+70] += A[192] * B[(l_n*84)+42];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b43 = _mm_broadcast_ss(&B[(l_n*84)+43]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b43 = _mm_load_ss(&B[(l_n*84)+43]);    b43 = _mm_shuffle_ps(b43, b43, 0x00);
#endif
    __m128 c43_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a43_0 = _mm_load_ss(&A[193]);
    c43_0 = _mm_add_ss(c43_0, _mm_mul_ss(a43_0, b43));
    _mm_store_ss(&C[(l_n*84)+22], c43_0);
    __m128 c43_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a43_1 = _mm_load_ss(&A[194]);
    c43_1 = _mm_add_ss(c43_1, _mm_mul_ss(a43_1, b43));
    _mm_store_ss(&C[(l_n*84)+43], c43_1);
    __m128 c43_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a43_2 = _mm_load_ss(&A[195]);
    c43_2 = _mm_add_ss(c43_2, _mm_mul_ss(a43_2, b43));
    _mm_store_ss(&C[(l_n*84)+71], c43_2);
#else
    C[(l_n*84)+22] += A[193] * B[(l_n*84)+43];
    C[(l_n*84)+43] += A[194] * B[(l_n*84)+43];
    C[(l_n*84)+71] += A[195] * B[(l_n*84)+43];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b44 = _mm_broadcast_ss(&B[(l_n*84)+44]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b44 = _mm_load_ss(&B[(l_n*84)+44]);    b44 = _mm_shuffle_ps(b44, b44, 0x00);
#endif
    __m128 c44_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a44_0 = _mm_load_ss(&A[196]);
    c44_0 = _mm_add_ss(c44_0, _mm_mul_ss(a44_0, b44));
    _mm_store_ss(&C[(l_n*84)+23], c44_0);
    __m128 c44_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a44_1 = _mm_load_ss(&A[197]);
    c44_1 = _mm_add_ss(c44_1, _mm_mul_ss(a44_1, b44));
    _mm_store_ss(&C[(l_n*84)+44], c44_1);
    __m128 c44_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a44_2 = _mm_load_ss(&A[198]);
    c44_2 = _mm_add_ss(c44_2, _mm_mul_ss(a44_2, b44));
    _mm_store_ss(&C[(l_n*84)+72], c44_2);
#else
    C[(l_n*84)+23] += A[196] * B[(l_n*84)+44];
    C[(l_n*84)+44] += A[197] * B[(l_n*84)+44];
    C[(l_n*84)+72] += A[198] * B[(l_n*84)+44];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b45 = _mm_broadcast_ss(&B[(l_n*84)+45]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b45 = _mm_load_ss(&B[(l_n*84)+45]);    b45 = _mm_shuffle_ps(b45, b45, 0x00);
#endif
    __m128 c45_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a45_0 = _mm_load_ss(&A[199]);
    c45_0 = _mm_add_ss(c45_0, _mm_mul_ss(a45_0, b45));
    _mm_store_ss(&C[(l_n*84)+24], c45_0);
    __m128 c45_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a45_1 = _mm_load_ss(&A[200]);
    c45_1 = _mm_add_ss(c45_1, _mm_mul_ss(a45_1, b45));
    _mm_store_ss(&C[(l_n*84)+45], c45_1);
    __m128 c45_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a45_2 = _mm_load_ss(&A[201]);
    c45_2 = _mm_add_ss(c45_2, _mm_mul_ss(a45_2, b45));
    _mm_store_ss(&C[(l_n*84)+73], c45_2);
#else
    C[(l_n*84)+24] += A[199] * B[(l_n*84)+45];
    C[(l_n*84)+45] += A[200] * B[(l_n*84)+45];
    C[(l_n*84)+73] += A[201] * B[(l_n*84)+45];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b46 = _mm_broadcast_ss(&B[(l_n*84)+46]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b46 = _mm_load_ss(&B[(l_n*84)+46]);    b46 = _mm_shuffle_ps(b46, b46, 0x00);
#endif
    __m128 c46_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a46_0 = _mm_load_ss(&A[202]);
    c46_0 = _mm_add_ss(c46_0, _mm_mul_ss(a46_0, b46));
    _mm_store_ss(&C[(l_n*84)+10], c46_0);
    __m128 c46_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a46_1 = _mm_load_ss(&A[203]);
    c46_1 = _mm_add_ss(c46_1, _mm_mul_ss(a46_1, b46));
    _mm_store_ss(&C[(l_n*84)+25], c46_1);
    __m128 c46_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a46_2 = _mm_load_ss(&A[204]);
    c46_2 = _mm_add_ss(c46_2, _mm_mul_ss(a46_2, b46));
    _mm_store_ss(&C[(l_n*84)+46], c46_2);
    __m128 c46_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a46_3 = _mm_load_ss(&A[205]);
    c46_3 = _mm_add_ss(c46_3, _mm_mul_ss(a46_3, b46));
    _mm_store_ss(&C[(l_n*84)+74], c46_3);
#else
    C[(l_n*84)+10] += A[202] * B[(l_n*84)+46];
    C[(l_n*84)+25] += A[203] * B[(l_n*84)+46];
    C[(l_n*84)+46] += A[204] * B[(l_n*84)+46];
    C[(l_n*84)+74] += A[205] * B[(l_n*84)+46];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b47 = _mm_broadcast_ss(&B[(l_n*84)+47]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b47 = _mm_load_ss(&B[(l_n*84)+47]);    b47 = _mm_shuffle_ps(b47, b47, 0x00);
#endif
    __m128 c47_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a47_0 = _mm_load_ss(&A[206]);
    c47_0 = _mm_add_ss(c47_0, _mm_mul_ss(a47_0, b47));
    _mm_store_ss(&C[(l_n*84)+11], c47_0);
    __m128 c47_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a47_1 = _mm_load_ss(&A[207]);
    c47_1 = _mm_add_ss(c47_1, _mm_mul_ss(a47_1, b47));
    _mm_store_ss(&C[(l_n*84)+26], c47_1);
    __m128 c47_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a47_2 = _mm_load_ss(&A[208]);
    c47_2 = _mm_add_ss(c47_2, _mm_mul_ss(a47_2, b47));
    _mm_store_ss(&C[(l_n*84)+47], c47_2);
    __m128 c47_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a47_3 = _mm_load_ss(&A[209]);
    c47_3 = _mm_add_ss(c47_3, _mm_mul_ss(a47_3, b47));
    _mm_store_ss(&C[(l_n*84)+75], c47_3);
#else
    C[(l_n*84)+11] += A[206] * B[(l_n*84)+47];
    C[(l_n*84)+26] += A[207] * B[(l_n*84)+47];
    C[(l_n*84)+47] += A[208] * B[(l_n*84)+47];
    C[(l_n*84)+75] += A[209] * B[(l_n*84)+47];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b48 = _mm_broadcast_ss(&B[(l_n*84)+48]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b48 = _mm_load_ss(&B[(l_n*84)+48]);    b48 = _mm_shuffle_ps(b48, b48, 0x00);
#endif
    __m128 c48_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a48_0 = _mm_load_ss(&A[210]);
    c48_0 = _mm_add_ss(c48_0, _mm_mul_ss(a48_0, b48));
    _mm_store_ss(&C[(l_n*84)+12], c48_0);
    __m128 c48_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a48_1 = _mm_load_ss(&A[211]);
    c48_1 = _mm_add_ss(c48_1, _mm_mul_ss(a48_1, b48));
    _mm_store_ss(&C[(l_n*84)+27], c48_1);
    __m128 c48_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a48_2 = _mm_load_ss(&A[212]);
    c48_2 = _mm_add_ss(c48_2, _mm_mul_ss(a48_2, b48));
    _mm_store_ss(&C[(l_n*84)+48], c48_2);
    __m128 c48_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a48_3 = _mm_load_ss(&A[213]);
    c48_3 = _mm_add_ss(c48_3, _mm_mul_ss(a48_3, b48));
    _mm_store_ss(&C[(l_n*84)+76], c48_3);
#else
    C[(l_n*84)+12] += A[210] * B[(l_n*84)+48];
    C[(l_n*84)+27] += A[211] * B[(l_n*84)+48];
    C[(l_n*84)+48] += A[212] * B[(l_n*84)+48];
    C[(l_n*84)+76] += A[213] * B[(l_n*84)+48];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b49 = _mm_broadcast_ss(&B[(l_n*84)+49]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b49 = _mm_load_ss(&B[(l_n*84)+49]);    b49 = _mm_shuffle_ps(b49, b49, 0x00);
#endif
    __m128 c49_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a49_0 = _mm_load_ss(&A[214]);
    c49_0 = _mm_add_ss(c49_0, _mm_mul_ss(a49_0, b49));
    _mm_store_ss(&C[(l_n*84)+13], c49_0);
    __m128 c49_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a49_1 = _mm_load_ss(&A[215]);
    c49_1 = _mm_add_ss(c49_1, _mm_mul_ss(a49_1, b49));
    _mm_store_ss(&C[(l_n*84)+28], c49_1);
    __m128 c49_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a49_2 = _mm_load_ss(&A[216]);
    c49_2 = _mm_add_ss(c49_2, _mm_mul_ss(a49_2, b49));
    _mm_store_ss(&C[(l_n*84)+49], c49_2);
    __m128 c49_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a49_3 = _mm_load_ss(&A[217]);
    c49_3 = _mm_add_ss(c49_3, _mm_mul_ss(a49_3, b49));
    _mm_store_ss(&C[(l_n*84)+77], c49_3);
#else
    C[(l_n*84)+13] += A[214] * B[(l_n*84)+49];
    C[(l_n*84)+28] += A[215] * B[(l_n*84)+49];
    C[(l_n*84)+49] += A[216] * B[(l_n*84)+49];
    C[(l_n*84)+77] += A[217] * B[(l_n*84)+49];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b50 = _mm_broadcast_ss(&B[(l_n*84)+50]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b50 = _mm_load_ss(&B[(l_n*84)+50]);    b50 = _mm_shuffle_ps(b50, b50, 0x00);
#endif
    __m128 c50_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a50_0 = _mm_load_ss(&A[218]);
    c50_0 = _mm_add_ss(c50_0, _mm_mul_ss(a50_0, b50));
    _mm_store_ss(&C[(l_n*84)+4], c50_0);
    __m128 c50_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a50_1 = _mm_load_ss(&A[219]);
    c50_1 = _mm_add_ss(c50_1, _mm_mul_ss(a50_1, b50));
    _mm_store_ss(&C[(l_n*84)+14], c50_1);
    __m128 c50_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a50_2 = _mm_load_ss(&A[220]);
    c50_2 = _mm_add_ss(c50_2, _mm_mul_ss(a50_2, b50));
    _mm_store_ss(&C[(l_n*84)+29], c50_2);
    __m128 c50_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a50_3 = _mm_load_ss(&A[221]);
    c50_3 = _mm_add_ss(c50_3, _mm_mul_ss(a50_3, b50));
    _mm_store_ss(&C[(l_n*84)+50], c50_3);
    __m128 c50_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a50_4 = _mm_load_ss(&A[222]);
    c50_4 = _mm_add_ss(c50_4, _mm_mul_ss(a50_4, b50));
    _mm_store_ss(&C[(l_n*84)+78], c50_4);
#else
    C[(l_n*84)+4] += A[218] * B[(l_n*84)+50];
    C[(l_n*84)+14] += A[219] * B[(l_n*84)+50];
    C[(l_n*84)+29] += A[220] * B[(l_n*84)+50];
    C[(l_n*84)+50] += A[221] * B[(l_n*84)+50];
    C[(l_n*84)+78] += A[222] * B[(l_n*84)+50];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b51 = _mm_broadcast_ss(&B[(l_n*84)+51]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b51 = _mm_load_ss(&B[(l_n*84)+51]);    b51 = _mm_shuffle_ps(b51, b51, 0x00);
#endif
    __m128 c51_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a51_0 = _mm_load_ss(&A[223]);
    c51_0 = _mm_add_ss(c51_0, _mm_mul_ss(a51_0, b51));
    _mm_store_ss(&C[(l_n*84)+5], c51_0);
    __m128 c51_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a51_1 = _mm_load_ss(&A[224]);
    c51_1 = _mm_add_ss(c51_1, _mm_mul_ss(a51_1, b51));
    _mm_store_ss(&C[(l_n*84)+15], c51_1);
    __m128 c51_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a51_2 = _mm_load_ss(&A[225]);
    c51_2 = _mm_add_ss(c51_2, _mm_mul_ss(a51_2, b51));
    _mm_store_ss(&C[(l_n*84)+30], c51_2);
    __m128 c51_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a51_3 = _mm_load_ss(&A[226]);
    c51_3 = _mm_add_ss(c51_3, _mm_mul_ss(a51_3, b51));
    _mm_store_ss(&C[(l_n*84)+51], c51_3);
    __m128 c51_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a51_4 = _mm_load_ss(&A[227]);
    c51_4 = _mm_add_ss(c51_4, _mm_mul_ss(a51_4, b51));
    _mm_store_ss(&C[(l_n*84)+79], c51_4);
#else
    C[(l_n*84)+5] += A[223] * B[(l_n*84)+51];
    C[(l_n*84)+15] += A[224] * B[(l_n*84)+51];
    C[(l_n*84)+30] += A[225] * B[(l_n*84)+51];
    C[(l_n*84)+51] += A[226] * B[(l_n*84)+51];
    C[(l_n*84)+79] += A[227] * B[(l_n*84)+51];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b52 = _mm_broadcast_ss(&B[(l_n*84)+52]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b52 = _mm_load_ss(&B[(l_n*84)+52]);    b52 = _mm_shuffle_ps(b52, b52, 0x00);
#endif
    __m128 c52_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a52_0 = _mm_load_ss(&A[228]);
    c52_0 = _mm_add_ss(c52_0, _mm_mul_ss(a52_0, b52));
    _mm_store_ss(&C[(l_n*84)+6], c52_0);
    __m128 c52_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a52_1 = _mm_load_ss(&A[229]);
    c52_1 = _mm_add_ss(c52_1, _mm_mul_ss(a52_1, b52));
    _mm_store_ss(&C[(l_n*84)+16], c52_1);
    __m128 c52_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a52_2 = _mm_load_ss(&A[230]);
    c52_2 = _mm_add_ss(c52_2, _mm_mul_ss(a52_2, b52));
    _mm_store_ss(&C[(l_n*84)+31], c52_2);
    __m128 c52_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a52_3 = _mm_load_ss(&A[231]);
    c52_3 = _mm_add_ss(c52_3, _mm_mul_ss(a52_3, b52));
    _mm_store_ss(&C[(l_n*84)+52], c52_3);
    __m128 c52_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a52_4 = _mm_load_ss(&A[232]);
    c52_4 = _mm_add_ss(c52_4, _mm_mul_ss(a52_4, b52));
    _mm_store_ss(&C[(l_n*84)+80], c52_4);
#else
    C[(l_n*84)+6] += A[228] * B[(l_n*84)+52];
    C[(l_n*84)+16] += A[229] * B[(l_n*84)+52];
    C[(l_n*84)+31] += A[230] * B[(l_n*84)+52];
    C[(l_n*84)+52] += A[231] * B[(l_n*84)+52];
    C[(l_n*84)+80] += A[232] * B[(l_n*84)+52];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b53 = _mm_broadcast_ss(&B[(l_n*84)+53]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b53 = _mm_load_ss(&B[(l_n*84)+53]);    b53 = _mm_shuffle_ps(b53, b53, 0x00);
#endif
    __m128 c53_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a53_0 = _mm_load_ss(&A[233]);
    c53_0 = _mm_add_ss(c53_0, _mm_mul_ss(a53_0, b53));
    _mm_store_ss(&C[(l_n*84)+1], c53_0);
    __m128 c53_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a53_1 = _mm_load_ss(&A[234]);
    c53_1 = _mm_add_ss(c53_1, _mm_mul_ss(a53_1, b53));
    _mm_store_ss(&C[(l_n*84)+7], c53_1);
    __m128 c53_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a53_2 = _mm_load_ss(&A[235]);
    c53_2 = _mm_add_ss(c53_2, _mm_mul_ss(a53_2, b53));
    _mm_store_ss(&C[(l_n*84)+17], c53_2);
    __m128 c53_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a53_3 = _mm_load_ss(&A[236]);
    c53_3 = _mm_add_ss(c53_3, _mm_mul_ss(a53_3, b53));
    _mm_store_ss(&C[(l_n*84)+32], c53_3);
    __m128 c53_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a53_4 = _mm_load_ss(&A[237]);
    c53_4 = _mm_add_ss(c53_4, _mm_mul_ss(a53_4, b53));
    _mm_store_ss(&C[(l_n*84)+53], c53_4);
    __m128 c53_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a53_5 = _mm_load_ss(&A[238]);
    c53_5 = _mm_add_ss(c53_5, _mm_mul_ss(a53_5, b53));
    _mm_store_ss(&C[(l_n*84)+81], c53_5);
#else
    C[(l_n*84)+1] += A[233] * B[(l_n*84)+53];
    C[(l_n*84)+7] += A[234] * B[(l_n*84)+53];
    C[(l_n*84)+17] += A[235] * B[(l_n*84)+53];
    C[(l_n*84)+32] += A[236] * B[(l_n*84)+53];
    C[(l_n*84)+53] += A[237] * B[(l_n*84)+53];
    C[(l_n*84)+81] += A[238] * B[(l_n*84)+53];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b54 = _mm_broadcast_ss(&B[(l_n*84)+54]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b54 = _mm_load_ss(&B[(l_n*84)+54]);    b54 = _mm_shuffle_ps(b54, b54, 0x00);
#endif
    __m128 c54_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a54_0 = _mm_load_ss(&A[239]);
    c54_0 = _mm_add_ss(c54_0, _mm_mul_ss(a54_0, b54));
    _mm_store_ss(&C[(l_n*84)+2], c54_0);
    __m128 c54_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a54_1 = _mm_load_ss(&A[240]);
    c54_1 = _mm_add_ss(c54_1, _mm_mul_ss(a54_1, b54));
    _mm_store_ss(&C[(l_n*84)+8], c54_1);
    __m128 c54_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a54_2 = _mm_load_ss(&A[241]);
    c54_2 = _mm_add_ss(c54_2, _mm_mul_ss(a54_2, b54));
    _mm_store_ss(&C[(l_n*84)+18], c54_2);
    __m128 c54_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a54_3 = _mm_load_ss(&A[242]);
    c54_3 = _mm_add_ss(c54_3, _mm_mul_ss(a54_3, b54));
    _mm_store_ss(&C[(l_n*84)+33], c54_3);
    __m128 c54_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a54_4 = _mm_load_ss(&A[243]);
    c54_4 = _mm_add_ss(c54_4, _mm_mul_ss(a54_4, b54));
    _mm_store_ss(&C[(l_n*84)+54], c54_4);
    __m128 c54_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a54_5 = _mm_load_ss(&A[244]);
    c54_5 = _mm_add_ss(c54_5, _mm_mul_ss(a54_5, b54));
    _mm_store_ss(&C[(l_n*84)+82], c54_5);
#else
    C[(l_n*84)+2] += A[239] * B[(l_n*84)+54];
    C[(l_n*84)+8] += A[240] * B[(l_n*84)+54];
    C[(l_n*84)+18] += A[241] * B[(l_n*84)+54];
    C[(l_n*84)+33] += A[242] * B[(l_n*84)+54];
    C[(l_n*84)+54] += A[243] * B[(l_n*84)+54];
    C[(l_n*84)+82] += A[244] * B[(l_n*84)+54];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b55 = _mm_broadcast_ss(&B[(l_n*84)+55]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b55 = _mm_load_ss(&B[(l_n*84)+55]);    b55 = _mm_shuffle_ps(b55, b55, 0x00);
#endif
    __m128 c55_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a55_0 = _mm_load_ss(&A[245]);
    c55_0 = _mm_add_ss(c55_0, _mm_mul_ss(a55_0, b55));
    _mm_store_ss(&C[(l_n*84)+0], c55_0);
    __m128 c55_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a55_1 = _mm_load_ss(&A[246]);
    c55_1 = _mm_add_ss(c55_1, _mm_mul_ss(a55_1, b55));
    _mm_store_ss(&C[(l_n*84)+3], c55_1);
    __m128 c55_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a55_2 = _mm_load_ss(&A[247]);
    c55_2 = _mm_add_ss(c55_2, _mm_mul_ss(a55_2, b55));
    _mm_store_ss(&C[(l_n*84)+9], c55_2);
    __m128 c55_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a55_3 = _mm_load_ss(&A[248]);
    c55_3 = _mm_add_ss(c55_3, _mm_mul_ss(a55_3, b55));
    _mm_store_ss(&C[(l_n*84)+19], c55_3);
    __m128 c55_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a55_4 = _mm_load_ss(&A[249]);
    c55_4 = _mm_add_ss(c55_4, _mm_mul_ss(a55_4, b55));
    _mm_store_ss(&C[(l_n*84)+34], c55_4);
    __m128 c55_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a55_5 = _mm_load_ss(&A[250]);
    c55_5 = _mm_add_ss(c55_5, _mm_mul_ss(a55_5, b55));
    _mm_store_ss(&C[(l_n*84)+55], c55_5);
    __m128 c55_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a55_6 = _mm_load_ss(&A[251]);
    c55_6 = _mm_add_ss(c55_6, _mm_mul_ss(a55_6, b55));
    _mm_store_ss(&C[(l_n*84)+83], c55_6);
#else
    C[(l_n*84)+0] += A[245] * B[(l_n*84)+55];
    C[(l_n*84)+3] += A[246] * B[(l_n*84)+55];
    C[(l_n*84)+9] += A[247] * B[(l_n*84)+55];
    C[(l_n*84)+19] += A[248] * B[(l_n*84)+55];
    C[(l_n*84)+34] += A[249] * B[(l_n*84)+55];
    C[(l_n*84)+55] += A[250] * B[(l_n*84)+55];
    C[(l_n*84)+83] += A[251] * B[(l_n*84)+55];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b56 = _mm_broadcast_ss(&B[(l_n*84)+56]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b56 = _mm_load_ss(&B[(l_n*84)+56]);    b56 = _mm_shuffle_ps(b56, b56, 0x00);
#endif
    __m128 c56_0 = _mm_load_ss(&C[(l_n*84)+56]);
    __m128 a56_0 = _mm_load_ss(&A[252]);
    c56_0 = _mm_add_ss(c56_0, _mm_mul_ss(a56_0, b56));
    _mm_store_ss(&C[(l_n*84)+56], c56_0);
#else
    C[(l_n*84)+56] += A[252] * B[(l_n*84)+56];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b57 = _mm_broadcast_ss(&B[(l_n*84)+57]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b57 = _mm_load_ss(&B[(l_n*84)+57]);    b57 = _mm_shuffle_ps(b57, b57, 0x00);
#endif
    __m128 c57_0 = _mm_load_ss(&C[(l_n*84)+57]);
    __m128 a57_0 = _mm_load_ss(&A[253]);
    c57_0 = _mm_add_ss(c57_0, _mm_mul_ss(a57_0, b57));
    _mm_store_ss(&C[(l_n*84)+57], c57_0);
#else
    C[(l_n*84)+57] += A[253] * B[(l_n*84)+57];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b58 = _mm_broadcast_ss(&B[(l_n*84)+58]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b58 = _mm_load_ss(&B[(l_n*84)+58]);    b58 = _mm_shuffle_ps(b58, b58, 0x00);
#endif
    __m128 c58_0 = _mm_load_ss(&C[(l_n*84)+58]);
    __m128 a58_0 = _mm_load_ss(&A[254]);
    c58_0 = _mm_add_ss(c58_0, _mm_mul_ss(a58_0, b58));
    _mm_store_ss(&C[(l_n*84)+58], c58_0);
#else
    C[(l_n*84)+58] += A[254] * B[(l_n*84)+58];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b59 = _mm_broadcast_ss(&B[(l_n*84)+59]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b59 = _mm_load_ss(&B[(l_n*84)+59]);    b59 = _mm_shuffle_ps(b59, b59, 0x00);
#endif
    __m128 c59_0 = _mm_load_ss(&C[(l_n*84)+59]);
    __m128 a59_0 = _mm_load_ss(&A[255]);
    c59_0 = _mm_add_ss(c59_0, _mm_mul_ss(a59_0, b59));
    _mm_store_ss(&C[(l_n*84)+59], c59_0);
#else
    C[(l_n*84)+59] += A[255] * B[(l_n*84)+59];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b60 = _mm_broadcast_ss(&B[(l_n*84)+60]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b60 = _mm_load_ss(&B[(l_n*84)+60]);    b60 = _mm_shuffle_ps(b60, b60, 0x00);
#endif
    __m128 c60_0 = _mm_load_ss(&C[(l_n*84)+60]);
    __m128 a60_0 = _mm_load_ss(&A[256]);
    c60_0 = _mm_add_ss(c60_0, _mm_mul_ss(a60_0, b60));
    _mm_store_ss(&C[(l_n*84)+60], c60_0);
#else
    C[(l_n*84)+60] += A[256] * B[(l_n*84)+60];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b61 = _mm_broadcast_ss(&B[(l_n*84)+61]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b61 = _mm_load_ss(&B[(l_n*84)+61]);    b61 = _mm_shuffle_ps(b61, b61, 0x00);
#endif
    __m128 c61_0 = _mm_load_ss(&C[(l_n*84)+61]);
    __m128 a61_0 = _mm_load_ss(&A[257]);
    c61_0 = _mm_add_ss(c61_0, _mm_mul_ss(a61_0, b61));
    _mm_store_ss(&C[(l_n*84)+61], c61_0);
#else
    C[(l_n*84)+61] += A[257] * B[(l_n*84)+61];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b62 = _mm_broadcast_ss(&B[(l_n*84)+62]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b62 = _mm_load_ss(&B[(l_n*84)+62]);    b62 = _mm_shuffle_ps(b62, b62, 0x00);
#endif
    __m128 c62_0 = _mm_load_ss(&C[(l_n*84)+62]);
    __m128 a62_0 = _mm_load_ss(&A[258]);
    c62_0 = _mm_add_ss(c62_0, _mm_mul_ss(a62_0, b62));
    _mm_store_ss(&C[(l_n*84)+62], c62_0);
#else
    C[(l_n*84)+62] += A[258] * B[(l_n*84)+62];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b63 = _mm_broadcast_ss(&B[(l_n*84)+63]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b63 = _mm_load_ss(&B[(l_n*84)+63]);    b63 = _mm_shuffle_ps(b63, b63, 0x00);
#endif
    __m128 c63_0 = _mm_load_ss(&C[(l_n*84)+35]);
    __m128 a63_0 = _mm_load_ss(&A[259]);
    c63_0 = _mm_add_ss(c63_0, _mm_mul_ss(a63_0, b63));
    _mm_store_ss(&C[(l_n*84)+35], c63_0);
    __m128 c63_1 = _mm_load_ss(&C[(l_n*84)+63]);
    __m128 a63_1 = _mm_load_ss(&A[260]);
    c63_1 = _mm_add_ss(c63_1, _mm_mul_ss(a63_1, b63));
    _mm_store_ss(&C[(l_n*84)+63], c63_1);
#else
    C[(l_n*84)+35] += A[259] * B[(l_n*84)+63];
    C[(l_n*84)+63] += A[260] * B[(l_n*84)+63];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b64 = _mm_broadcast_ss(&B[(l_n*84)+64]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b64 = _mm_load_ss(&B[(l_n*84)+64]);    b64 = _mm_shuffle_ps(b64, b64, 0x00);
#endif
    __m128 c64_0 = _mm_load_ss(&C[(l_n*84)+36]);
    __m128 a64_0 = _mm_load_ss(&A[261]);
    c64_0 = _mm_add_ss(c64_0, _mm_mul_ss(a64_0, b64));
    _mm_store_ss(&C[(l_n*84)+36], c64_0);
    __m128 c64_1 = _mm_load_ss(&C[(l_n*84)+64]);
    __m128 a64_1 = _mm_load_ss(&A[262]);
    c64_1 = _mm_add_ss(c64_1, _mm_mul_ss(a64_1, b64));
    _mm_store_ss(&C[(l_n*84)+64], c64_1);
#else
    C[(l_n*84)+36] += A[261] * B[(l_n*84)+64];
    C[(l_n*84)+64] += A[262] * B[(l_n*84)+64];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b65 = _mm_broadcast_ss(&B[(l_n*84)+65]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b65 = _mm_load_ss(&B[(l_n*84)+65]);    b65 = _mm_shuffle_ps(b65, b65, 0x00);
#endif
    __m128 c65_0 = _mm_load_ss(&C[(l_n*84)+37]);
    __m128 a65_0 = _mm_load_ss(&A[263]);
    c65_0 = _mm_add_ss(c65_0, _mm_mul_ss(a65_0, b65));
    _mm_store_ss(&C[(l_n*84)+37], c65_0);
    __m128 c65_1 = _mm_load_ss(&C[(l_n*84)+65]);
    __m128 a65_1 = _mm_load_ss(&A[264]);
    c65_1 = _mm_add_ss(c65_1, _mm_mul_ss(a65_1, b65));
    _mm_store_ss(&C[(l_n*84)+65], c65_1);
#else
    C[(l_n*84)+37] += A[263] * B[(l_n*84)+65];
    C[(l_n*84)+65] += A[264] * B[(l_n*84)+65];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b66 = _mm_broadcast_ss(&B[(l_n*84)+66]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b66 = _mm_load_ss(&B[(l_n*84)+66]);    b66 = _mm_shuffle_ps(b66, b66, 0x00);
#endif
    __m128 c66_0 = _mm_load_ss(&C[(l_n*84)+38]);
    __m128 a66_0 = _mm_load_ss(&A[265]);
    c66_0 = _mm_add_ss(c66_0, _mm_mul_ss(a66_0, b66));
    _mm_store_ss(&C[(l_n*84)+38], c66_0);
    __m128 c66_1 = _mm_load_ss(&C[(l_n*84)+66]);
    __m128 a66_1 = _mm_load_ss(&A[266]);
    c66_1 = _mm_add_ss(c66_1, _mm_mul_ss(a66_1, b66));
    _mm_store_ss(&C[(l_n*84)+66], c66_1);
#else
    C[(l_n*84)+38] += A[265] * B[(l_n*84)+66];
    C[(l_n*84)+66] += A[266] * B[(l_n*84)+66];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b67 = _mm_broadcast_ss(&B[(l_n*84)+67]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b67 = _mm_load_ss(&B[(l_n*84)+67]);    b67 = _mm_shuffle_ps(b67, b67, 0x00);
#endif
    __m128 c67_0 = _mm_load_ss(&C[(l_n*84)+39]);
    __m128 a67_0 = _mm_load_ss(&A[267]);
    c67_0 = _mm_add_ss(c67_0, _mm_mul_ss(a67_0, b67));
    _mm_store_ss(&C[(l_n*84)+39], c67_0);
    __m128 c67_1 = _mm_load_ss(&C[(l_n*84)+67]);
    __m128 a67_1 = _mm_load_ss(&A[268]);
    c67_1 = _mm_add_ss(c67_1, _mm_mul_ss(a67_1, b67));
    _mm_store_ss(&C[(l_n*84)+67], c67_1);
#else
    C[(l_n*84)+39] += A[267] * B[(l_n*84)+67];
    C[(l_n*84)+67] += A[268] * B[(l_n*84)+67];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b68 = _mm_broadcast_ss(&B[(l_n*84)+68]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b68 = _mm_load_ss(&B[(l_n*84)+68]);    b68 = _mm_shuffle_ps(b68, b68, 0x00);
#endif
    __m128 c68_0 = _mm_load_ss(&C[(l_n*84)+40]);
    __m128 a68_0 = _mm_load_ss(&A[269]);
    c68_0 = _mm_add_ss(c68_0, _mm_mul_ss(a68_0, b68));
    _mm_store_ss(&C[(l_n*84)+40], c68_0);
    __m128 c68_1 = _mm_load_ss(&C[(l_n*84)+68]);
    __m128 a68_1 = _mm_load_ss(&A[270]);
    c68_1 = _mm_add_ss(c68_1, _mm_mul_ss(a68_1, b68));
    _mm_store_ss(&C[(l_n*84)+68], c68_1);
#else
    C[(l_n*84)+40] += A[269] * B[(l_n*84)+68];
    C[(l_n*84)+68] += A[270] * B[(l_n*84)+68];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b69 = _mm_broadcast_ss(&B[(l_n*84)+69]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b69 = _mm_load_ss(&B[(l_n*84)+69]);    b69 = _mm_shuffle_ps(b69, b69, 0x00);
#endif
    __m128 c69_0 = _mm_load_ss(&C[(l_n*84)+20]);
    __m128 a69_0 = _mm_load_ss(&A[271]);
    c69_0 = _mm_add_ss(c69_0, _mm_mul_ss(a69_0, b69));
    _mm_store_ss(&C[(l_n*84)+20], c69_0);
    __m128 c69_1 = _mm_load_ss(&C[(l_n*84)+41]);
    __m128 a69_1 = _mm_load_ss(&A[272]);
    c69_1 = _mm_add_ss(c69_1, _mm_mul_ss(a69_1, b69));
    _mm_store_ss(&C[(l_n*84)+41], c69_1);
    __m128 c69_2 = _mm_load_ss(&C[(l_n*84)+69]);
    __m128 a69_2 = _mm_load_ss(&A[273]);
    c69_2 = _mm_add_ss(c69_2, _mm_mul_ss(a69_2, b69));
    _mm_store_ss(&C[(l_n*84)+69], c69_2);
#else
    C[(l_n*84)+20] += A[271] * B[(l_n*84)+69];
    C[(l_n*84)+41] += A[272] * B[(l_n*84)+69];
    C[(l_n*84)+69] += A[273] * B[(l_n*84)+69];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b70 = _mm_broadcast_ss(&B[(l_n*84)+70]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b70 = _mm_load_ss(&B[(l_n*84)+70]);    b70 = _mm_shuffle_ps(b70, b70, 0x00);
#endif
    __m128 c70_0 = _mm_load_ss(&C[(l_n*84)+21]);
    __m128 a70_0 = _mm_load_ss(&A[274]);
    c70_0 = _mm_add_ss(c70_0, _mm_mul_ss(a70_0, b70));
    _mm_store_ss(&C[(l_n*84)+21], c70_0);
    __m128 c70_1 = _mm_load_ss(&C[(l_n*84)+42]);
    __m128 a70_1 = _mm_load_ss(&A[275]);
    c70_1 = _mm_add_ss(c70_1, _mm_mul_ss(a70_1, b70));
    _mm_store_ss(&C[(l_n*84)+42], c70_1);
    __m128 c70_2 = _mm_load_ss(&C[(l_n*84)+70]);
    __m128 a70_2 = _mm_load_ss(&A[276]);
    c70_2 = _mm_add_ss(c70_2, _mm_mul_ss(a70_2, b70));
    _mm_store_ss(&C[(l_n*84)+70], c70_2);
#else
    C[(l_n*84)+21] += A[274] * B[(l_n*84)+70];
    C[(l_n*84)+42] += A[275] * B[(l_n*84)+70];
    C[(l_n*84)+70] += A[276] * B[(l_n*84)+70];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b71 = _mm_broadcast_ss(&B[(l_n*84)+71]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b71 = _mm_load_ss(&B[(l_n*84)+71]);    b71 = _mm_shuffle_ps(b71, b71, 0x00);
#endif
    __m128 c71_0 = _mm_load_ss(&C[(l_n*84)+22]);
    __m128 a71_0 = _mm_load_ss(&A[277]);
    c71_0 = _mm_add_ss(c71_0, _mm_mul_ss(a71_0, b71));
    _mm_store_ss(&C[(l_n*84)+22], c71_0);
    __m128 c71_1 = _mm_load_ss(&C[(l_n*84)+43]);
    __m128 a71_1 = _mm_load_ss(&A[278]);
    c71_1 = _mm_add_ss(c71_1, _mm_mul_ss(a71_1, b71));
    _mm_store_ss(&C[(l_n*84)+43], c71_1);
    __m128 c71_2 = _mm_load_ss(&C[(l_n*84)+71]);
    __m128 a71_2 = _mm_load_ss(&A[279]);
    c71_2 = _mm_add_ss(c71_2, _mm_mul_ss(a71_2, b71));
    _mm_store_ss(&C[(l_n*84)+71], c71_2);
#else
    C[(l_n*84)+22] += A[277] * B[(l_n*84)+71];
    C[(l_n*84)+43] += A[278] * B[(l_n*84)+71];
    C[(l_n*84)+71] += A[279] * B[(l_n*84)+71];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b72 = _mm_broadcast_ss(&B[(l_n*84)+72]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b72 = _mm_load_ss(&B[(l_n*84)+72]);    b72 = _mm_shuffle_ps(b72, b72, 0x00);
#endif
    __m128 c72_0 = _mm_load_ss(&C[(l_n*84)+23]);
    __m128 a72_0 = _mm_load_ss(&A[280]);
    c72_0 = _mm_add_ss(c72_0, _mm_mul_ss(a72_0, b72));
    _mm_store_ss(&C[(l_n*84)+23], c72_0);
    __m128 c72_1 = _mm_load_ss(&C[(l_n*84)+44]);
    __m128 a72_1 = _mm_load_ss(&A[281]);
    c72_1 = _mm_add_ss(c72_1, _mm_mul_ss(a72_1, b72));
    _mm_store_ss(&C[(l_n*84)+44], c72_1);
    __m128 c72_2 = _mm_load_ss(&C[(l_n*84)+72]);
    __m128 a72_2 = _mm_load_ss(&A[282]);
    c72_2 = _mm_add_ss(c72_2, _mm_mul_ss(a72_2, b72));
    _mm_store_ss(&C[(l_n*84)+72], c72_2);
#else
    C[(l_n*84)+23] += A[280] * B[(l_n*84)+72];
    C[(l_n*84)+44] += A[281] * B[(l_n*84)+72];
    C[(l_n*84)+72] += A[282] * B[(l_n*84)+72];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b73 = _mm_broadcast_ss(&B[(l_n*84)+73]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b73 = _mm_load_ss(&B[(l_n*84)+73]);    b73 = _mm_shuffle_ps(b73, b73, 0x00);
#endif
    __m128 c73_0 = _mm_load_ss(&C[(l_n*84)+24]);
    __m128 a73_0 = _mm_load_ss(&A[283]);
    c73_0 = _mm_add_ss(c73_0, _mm_mul_ss(a73_0, b73));
    _mm_store_ss(&C[(l_n*84)+24], c73_0);
    __m128 c73_1 = _mm_load_ss(&C[(l_n*84)+45]);
    __m128 a73_1 = _mm_load_ss(&A[284]);
    c73_1 = _mm_add_ss(c73_1, _mm_mul_ss(a73_1, b73));
    _mm_store_ss(&C[(l_n*84)+45], c73_1);
    __m128 c73_2 = _mm_load_ss(&C[(l_n*84)+73]);
    __m128 a73_2 = _mm_load_ss(&A[285]);
    c73_2 = _mm_add_ss(c73_2, _mm_mul_ss(a73_2, b73));
    _mm_store_ss(&C[(l_n*84)+73], c73_2);
#else
    C[(l_n*84)+24] += A[283] * B[(l_n*84)+73];
    C[(l_n*84)+45] += A[284] * B[(l_n*84)+73];
    C[(l_n*84)+73] += A[285] * B[(l_n*84)+73];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b74 = _mm_broadcast_ss(&B[(l_n*84)+74]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b74 = _mm_load_ss(&B[(l_n*84)+74]);    b74 = _mm_shuffle_ps(b74, b74, 0x00);
#endif
    __m128 c74_0 = _mm_load_ss(&C[(l_n*84)+10]);
    __m128 a74_0 = _mm_load_ss(&A[286]);
    c74_0 = _mm_add_ss(c74_0, _mm_mul_ss(a74_0, b74));
    _mm_store_ss(&C[(l_n*84)+10], c74_0);
    __m128 c74_1 = _mm_load_ss(&C[(l_n*84)+25]);
    __m128 a74_1 = _mm_load_ss(&A[287]);
    c74_1 = _mm_add_ss(c74_1, _mm_mul_ss(a74_1, b74));
    _mm_store_ss(&C[(l_n*84)+25], c74_1);
    __m128 c74_2 = _mm_load_ss(&C[(l_n*84)+46]);
    __m128 a74_2 = _mm_load_ss(&A[288]);
    c74_2 = _mm_add_ss(c74_2, _mm_mul_ss(a74_2, b74));
    _mm_store_ss(&C[(l_n*84)+46], c74_2);
    __m128 c74_3 = _mm_load_ss(&C[(l_n*84)+74]);
    __m128 a74_3 = _mm_load_ss(&A[289]);
    c74_3 = _mm_add_ss(c74_3, _mm_mul_ss(a74_3, b74));
    _mm_store_ss(&C[(l_n*84)+74], c74_3);
#else
    C[(l_n*84)+10] += A[286] * B[(l_n*84)+74];
    C[(l_n*84)+25] += A[287] * B[(l_n*84)+74];
    C[(l_n*84)+46] += A[288] * B[(l_n*84)+74];
    C[(l_n*84)+74] += A[289] * B[(l_n*84)+74];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b75 = _mm_broadcast_ss(&B[(l_n*84)+75]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b75 = _mm_load_ss(&B[(l_n*84)+75]);    b75 = _mm_shuffle_ps(b75, b75, 0x00);
#endif
    __m128 c75_0 = _mm_load_ss(&C[(l_n*84)+11]);
    __m128 a75_0 = _mm_load_ss(&A[290]);
    c75_0 = _mm_add_ss(c75_0, _mm_mul_ss(a75_0, b75));
    _mm_store_ss(&C[(l_n*84)+11], c75_0);
    __m128 c75_1 = _mm_load_ss(&C[(l_n*84)+26]);
    __m128 a75_1 = _mm_load_ss(&A[291]);
    c75_1 = _mm_add_ss(c75_1, _mm_mul_ss(a75_1, b75));
    _mm_store_ss(&C[(l_n*84)+26], c75_1);
    __m128 c75_2 = _mm_load_ss(&C[(l_n*84)+47]);
    __m128 a75_2 = _mm_load_ss(&A[292]);
    c75_2 = _mm_add_ss(c75_2, _mm_mul_ss(a75_2, b75));
    _mm_store_ss(&C[(l_n*84)+47], c75_2);
    __m128 c75_3 = _mm_load_ss(&C[(l_n*84)+75]);
    __m128 a75_3 = _mm_load_ss(&A[293]);
    c75_3 = _mm_add_ss(c75_3, _mm_mul_ss(a75_3, b75));
    _mm_store_ss(&C[(l_n*84)+75], c75_3);
#else
    C[(l_n*84)+11] += A[290] * B[(l_n*84)+75];
    C[(l_n*84)+26] += A[291] * B[(l_n*84)+75];
    C[(l_n*84)+47] += A[292] * B[(l_n*84)+75];
    C[(l_n*84)+75] += A[293] * B[(l_n*84)+75];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b76 = _mm_broadcast_ss(&B[(l_n*84)+76]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b76 = _mm_load_ss(&B[(l_n*84)+76]);    b76 = _mm_shuffle_ps(b76, b76, 0x00);
#endif
    __m128 c76_0 = _mm_load_ss(&C[(l_n*84)+12]);
    __m128 a76_0 = _mm_load_ss(&A[294]);
    c76_0 = _mm_add_ss(c76_0, _mm_mul_ss(a76_0, b76));
    _mm_store_ss(&C[(l_n*84)+12], c76_0);
    __m128 c76_1 = _mm_load_ss(&C[(l_n*84)+27]);
    __m128 a76_1 = _mm_load_ss(&A[295]);
    c76_1 = _mm_add_ss(c76_1, _mm_mul_ss(a76_1, b76));
    _mm_store_ss(&C[(l_n*84)+27], c76_1);
    __m128 c76_2 = _mm_load_ss(&C[(l_n*84)+48]);
    __m128 a76_2 = _mm_load_ss(&A[296]);
    c76_2 = _mm_add_ss(c76_2, _mm_mul_ss(a76_2, b76));
    _mm_store_ss(&C[(l_n*84)+48], c76_2);
    __m128 c76_3 = _mm_load_ss(&C[(l_n*84)+76]);
    __m128 a76_3 = _mm_load_ss(&A[297]);
    c76_3 = _mm_add_ss(c76_3, _mm_mul_ss(a76_3, b76));
    _mm_store_ss(&C[(l_n*84)+76], c76_3);
#else
    C[(l_n*84)+12] += A[294] * B[(l_n*84)+76];
    C[(l_n*84)+27] += A[295] * B[(l_n*84)+76];
    C[(l_n*84)+48] += A[296] * B[(l_n*84)+76];
    C[(l_n*84)+76] += A[297] * B[(l_n*84)+76];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b77 = _mm_broadcast_ss(&B[(l_n*84)+77]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b77 = _mm_load_ss(&B[(l_n*84)+77]);    b77 = _mm_shuffle_ps(b77, b77, 0x00);
#endif
    __m128 c77_0 = _mm_load_ss(&C[(l_n*84)+13]);
    __m128 a77_0 = _mm_load_ss(&A[298]);
    c77_0 = _mm_add_ss(c77_0, _mm_mul_ss(a77_0, b77));
    _mm_store_ss(&C[(l_n*84)+13], c77_0);
    __m128 c77_1 = _mm_load_ss(&C[(l_n*84)+28]);
    __m128 a77_1 = _mm_load_ss(&A[299]);
    c77_1 = _mm_add_ss(c77_1, _mm_mul_ss(a77_1, b77));
    _mm_store_ss(&C[(l_n*84)+28], c77_1);
    __m128 c77_2 = _mm_load_ss(&C[(l_n*84)+49]);
    __m128 a77_2 = _mm_load_ss(&A[300]);
    c77_2 = _mm_add_ss(c77_2, _mm_mul_ss(a77_2, b77));
    _mm_store_ss(&C[(l_n*84)+49], c77_2);
    __m128 c77_3 = _mm_load_ss(&C[(l_n*84)+77]);
    __m128 a77_3 = _mm_load_ss(&A[301]);
    c77_3 = _mm_add_ss(c77_3, _mm_mul_ss(a77_3, b77));
    _mm_store_ss(&C[(l_n*84)+77], c77_3);
#else
    C[(l_n*84)+13] += A[298] * B[(l_n*84)+77];
    C[(l_n*84)+28] += A[299] * B[(l_n*84)+77];
    C[(l_n*84)+49] += A[300] * B[(l_n*84)+77];
    C[(l_n*84)+77] += A[301] * B[(l_n*84)+77];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b78 = _mm_broadcast_ss(&B[(l_n*84)+78]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b78 = _mm_load_ss(&B[(l_n*84)+78]);    b78 = _mm_shuffle_ps(b78, b78, 0x00);
#endif
    __m128 c78_0 = _mm_load_ss(&C[(l_n*84)+4]);
    __m128 a78_0 = _mm_load_ss(&A[302]);
    c78_0 = _mm_add_ss(c78_0, _mm_mul_ss(a78_0, b78));
    _mm_store_ss(&C[(l_n*84)+4], c78_0);
    __m128 c78_1 = _mm_load_ss(&C[(l_n*84)+14]);
    __m128 a78_1 = _mm_load_ss(&A[303]);
    c78_1 = _mm_add_ss(c78_1, _mm_mul_ss(a78_1, b78));
    _mm_store_ss(&C[(l_n*84)+14], c78_1);
    __m128 c78_2 = _mm_load_ss(&C[(l_n*84)+29]);
    __m128 a78_2 = _mm_load_ss(&A[304]);
    c78_2 = _mm_add_ss(c78_2, _mm_mul_ss(a78_2, b78));
    _mm_store_ss(&C[(l_n*84)+29], c78_2);
    __m128 c78_3 = _mm_load_ss(&C[(l_n*84)+50]);
    __m128 a78_3 = _mm_load_ss(&A[305]);
    c78_3 = _mm_add_ss(c78_3, _mm_mul_ss(a78_3, b78));
    _mm_store_ss(&C[(l_n*84)+50], c78_3);
    __m128 c78_4 = _mm_load_ss(&C[(l_n*84)+78]);
    __m128 a78_4 = _mm_load_ss(&A[306]);
    c78_4 = _mm_add_ss(c78_4, _mm_mul_ss(a78_4, b78));
    _mm_store_ss(&C[(l_n*84)+78], c78_4);
#else
    C[(l_n*84)+4] += A[302] * B[(l_n*84)+78];
    C[(l_n*84)+14] += A[303] * B[(l_n*84)+78];
    C[(l_n*84)+29] += A[304] * B[(l_n*84)+78];
    C[(l_n*84)+50] += A[305] * B[(l_n*84)+78];
    C[(l_n*84)+78] += A[306] * B[(l_n*84)+78];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b79 = _mm_broadcast_ss(&B[(l_n*84)+79]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b79 = _mm_load_ss(&B[(l_n*84)+79]);    b79 = _mm_shuffle_ps(b79, b79, 0x00);
#endif
    __m128 c79_0 = _mm_load_ss(&C[(l_n*84)+5]);
    __m128 a79_0 = _mm_load_ss(&A[307]);
    c79_0 = _mm_add_ss(c79_0, _mm_mul_ss(a79_0, b79));
    _mm_store_ss(&C[(l_n*84)+5], c79_0);
    __m128 c79_1 = _mm_load_ss(&C[(l_n*84)+15]);
    __m128 a79_1 = _mm_load_ss(&A[308]);
    c79_1 = _mm_add_ss(c79_1, _mm_mul_ss(a79_1, b79));
    _mm_store_ss(&C[(l_n*84)+15], c79_1);
    __m128 c79_2 = _mm_load_ss(&C[(l_n*84)+30]);
    __m128 a79_2 = _mm_load_ss(&A[309]);
    c79_2 = _mm_add_ss(c79_2, _mm_mul_ss(a79_2, b79));
    _mm_store_ss(&C[(l_n*84)+30], c79_2);
    __m128 c79_3 = _mm_load_ss(&C[(l_n*84)+51]);
    __m128 a79_3 = _mm_load_ss(&A[310]);
    c79_3 = _mm_add_ss(c79_3, _mm_mul_ss(a79_3, b79));
    _mm_store_ss(&C[(l_n*84)+51], c79_3);
    __m128 c79_4 = _mm_load_ss(&C[(l_n*84)+79]);
    __m128 a79_4 = _mm_load_ss(&A[311]);
    c79_4 = _mm_add_ss(c79_4, _mm_mul_ss(a79_4, b79));
    _mm_store_ss(&C[(l_n*84)+79], c79_4);
#else
    C[(l_n*84)+5] += A[307] * B[(l_n*84)+79];
    C[(l_n*84)+15] += A[308] * B[(l_n*84)+79];
    C[(l_n*84)+30] += A[309] * B[(l_n*84)+79];
    C[(l_n*84)+51] += A[310] * B[(l_n*84)+79];
    C[(l_n*84)+79] += A[311] * B[(l_n*84)+79];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b80 = _mm_broadcast_ss(&B[(l_n*84)+80]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b80 = _mm_load_ss(&B[(l_n*84)+80]);    b80 = _mm_shuffle_ps(b80, b80, 0x00);
#endif
    __m128 c80_0 = _mm_load_ss(&C[(l_n*84)+6]);
    __m128 a80_0 = _mm_load_ss(&A[312]);
    c80_0 = _mm_add_ss(c80_0, _mm_mul_ss(a80_0, b80));
    _mm_store_ss(&C[(l_n*84)+6], c80_0);
    __m128 c80_1 = _mm_load_ss(&C[(l_n*84)+16]);
    __m128 a80_1 = _mm_load_ss(&A[313]);
    c80_1 = _mm_add_ss(c80_1, _mm_mul_ss(a80_1, b80));
    _mm_store_ss(&C[(l_n*84)+16], c80_1);
    __m128 c80_2 = _mm_load_ss(&C[(l_n*84)+31]);
    __m128 a80_2 = _mm_load_ss(&A[314]);
    c80_2 = _mm_add_ss(c80_2, _mm_mul_ss(a80_2, b80));
    _mm_store_ss(&C[(l_n*84)+31], c80_2);
    __m128 c80_3 = _mm_load_ss(&C[(l_n*84)+52]);
    __m128 a80_3 = _mm_load_ss(&A[315]);
    c80_3 = _mm_add_ss(c80_3, _mm_mul_ss(a80_3, b80));
    _mm_store_ss(&C[(l_n*84)+52], c80_3);
    __m128 c80_4 = _mm_load_ss(&C[(l_n*84)+80]);
    __m128 a80_4 = _mm_load_ss(&A[316]);
    c80_4 = _mm_add_ss(c80_4, _mm_mul_ss(a80_4, b80));
    _mm_store_ss(&C[(l_n*84)+80], c80_4);
#else
    C[(l_n*84)+6] += A[312] * B[(l_n*84)+80];
    C[(l_n*84)+16] += A[313] * B[(l_n*84)+80];
    C[(l_n*84)+31] += A[314] * B[(l_n*84)+80];
    C[(l_n*84)+52] += A[315] * B[(l_n*84)+80];
    C[(l_n*84)+80] += A[316] * B[(l_n*84)+80];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b81 = _mm_broadcast_ss(&B[(l_n*84)+81]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b81 = _mm_load_ss(&B[(l_n*84)+81]);    b81 = _mm_shuffle_ps(b81, b81, 0x00);
#endif
    __m128 c81_0 = _mm_load_ss(&C[(l_n*84)+1]);
    __m128 a81_0 = _mm_load_ss(&A[317]);
    c81_0 = _mm_add_ss(c81_0, _mm_mul_ss(a81_0, b81));
    _mm_store_ss(&C[(l_n*84)+1], c81_0);
    __m128 c81_1 = _mm_load_ss(&C[(l_n*84)+7]);
    __m128 a81_1 = _mm_load_ss(&A[318]);
    c81_1 = _mm_add_ss(c81_1, _mm_mul_ss(a81_1, b81));
    _mm_store_ss(&C[(l_n*84)+7], c81_1);
    __m128 c81_2 = _mm_load_ss(&C[(l_n*84)+17]);
    __m128 a81_2 = _mm_load_ss(&A[319]);
    c81_2 = _mm_add_ss(c81_2, _mm_mul_ss(a81_2, b81));
    _mm_store_ss(&C[(l_n*84)+17], c81_2);
    __m128 c81_3 = _mm_load_ss(&C[(l_n*84)+32]);
    __m128 a81_3 = _mm_load_ss(&A[320]);
    c81_3 = _mm_add_ss(c81_3, _mm_mul_ss(a81_3, b81));
    _mm_store_ss(&C[(l_n*84)+32], c81_3);
    __m128 c81_4 = _mm_load_ss(&C[(l_n*84)+53]);
    __m128 a81_4 = _mm_load_ss(&A[321]);
    c81_4 = _mm_add_ss(c81_4, _mm_mul_ss(a81_4, b81));
    _mm_store_ss(&C[(l_n*84)+53], c81_4);
    __m128 c81_5 = _mm_load_ss(&C[(l_n*84)+81]);
    __m128 a81_5 = _mm_load_ss(&A[322]);
    c81_5 = _mm_add_ss(c81_5, _mm_mul_ss(a81_5, b81));
    _mm_store_ss(&C[(l_n*84)+81], c81_5);
#else
    C[(l_n*84)+1] += A[317] * B[(l_n*84)+81];
    C[(l_n*84)+7] += A[318] * B[(l_n*84)+81];
    C[(l_n*84)+17] += A[319] * B[(l_n*84)+81];
    C[(l_n*84)+32] += A[320] * B[(l_n*84)+81];
    C[(l_n*84)+53] += A[321] * B[(l_n*84)+81];
    C[(l_n*84)+81] += A[322] * B[(l_n*84)+81];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b82 = _mm_broadcast_ss(&B[(l_n*84)+82]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b82 = _mm_load_ss(&B[(l_n*84)+82]);    b82 = _mm_shuffle_ps(b82, b82, 0x00);
#endif
    __m128 c82_0 = _mm_load_ss(&C[(l_n*84)+2]);
    __m128 a82_0 = _mm_load_ss(&A[323]);
    c82_0 = _mm_add_ss(c82_0, _mm_mul_ss(a82_0, b82));
    _mm_store_ss(&C[(l_n*84)+2], c82_0);
    __m128 c82_1 = _mm_load_ss(&C[(l_n*84)+8]);
    __m128 a82_1 = _mm_load_ss(&A[324]);
    c82_1 = _mm_add_ss(c82_1, _mm_mul_ss(a82_1, b82));
    _mm_store_ss(&C[(l_n*84)+8], c82_1);
    __m128 c82_2 = _mm_load_ss(&C[(l_n*84)+18]);
    __m128 a82_2 = _mm_load_ss(&A[325]);
    c82_2 = _mm_add_ss(c82_2, _mm_mul_ss(a82_2, b82));
    _mm_store_ss(&C[(l_n*84)+18], c82_2);
    __m128 c82_3 = _mm_load_ss(&C[(l_n*84)+33]);
    __m128 a82_3 = _mm_load_ss(&A[326]);
    c82_3 = _mm_add_ss(c82_3, _mm_mul_ss(a82_3, b82));
    _mm_store_ss(&C[(l_n*84)+33], c82_3);
    __m128 c82_4 = _mm_load_ss(&C[(l_n*84)+54]);
    __m128 a82_4 = _mm_load_ss(&A[327]);
    c82_4 = _mm_add_ss(c82_4, _mm_mul_ss(a82_4, b82));
    _mm_store_ss(&C[(l_n*84)+54], c82_4);
    __m128 c82_5 = _mm_load_ss(&C[(l_n*84)+82]);
    __m128 a82_5 = _mm_load_ss(&A[328]);
    c82_5 = _mm_add_ss(c82_5, _mm_mul_ss(a82_5, b82));
    _mm_store_ss(&C[(l_n*84)+82], c82_5);
#else
    C[(l_n*84)+2] += A[323] * B[(l_n*84)+82];
    C[(l_n*84)+8] += A[324] * B[(l_n*84)+82];
    C[(l_n*84)+18] += A[325] * B[(l_n*84)+82];
    C[(l_n*84)+33] += A[326] * B[(l_n*84)+82];
    C[(l_n*84)+54] += A[327] * B[(l_n*84)+82];
    C[(l_n*84)+82] += A[328] * B[(l_n*84)+82];
#endif

#if defined(__SSE3__) || defined(__AVX__)
#if defined(__SSE3__) && defined(__AVX__)
    __m128 b83 = _mm_broadcast_ss(&B[(l_n*84)+83]);
#endif
#if defined(__SSE3__) && !defined(__AVX__)
    __m128 b83 = _mm_load_ss(&B[(l_n*84)+83]);    b83 = _mm_shuffle_ps(b83, b83, 0x00);
#endif
    __m128 c83_0 = _mm_load_ss(&C[(l_n*84)+0]);
    __m128 a83_0 = _mm_load_ss(&A[329]);
    c83_0 = _mm_add_ss(c83_0, _mm_mul_ss(a83_0, b83));
    _mm_store_ss(&C[(l_n*84)+0], c83_0);
    __m128 c83_1 = _mm_load_ss(&C[(l_n*84)+3]);
    __m128 a83_1 = _mm_load_ss(&A[330]);
    c83_1 = _mm_add_ss(c83_1, _mm_mul_ss(a83_1, b83));
    _mm_store_ss(&C[(l_n*84)+3], c83_1);
    __m128 c83_2 = _mm_load_ss(&C[(l_n*84)+9]);
    __m128 a83_2 = _mm_load_ss(&A[331]);
    c83_2 = _mm_add_ss(c83_2, _mm_mul_ss(a83_2, b83));
    _mm_store_ss(&C[(l_n*84)+9], c83_2);
    __m128 c83_3 = _mm_load_ss(&C[(l_n*84)+19]);
    __m128 a83_3 = _mm_load_ss(&A[332]);
    c83_3 = _mm_add_ss(c83_3, _mm_mul_ss(a83_3, b83));
    _mm_store_ss(&C[(l_n*84)+19], c83_3);
    __m128 c83_4 = _mm_load_ss(&C[(l_n*84)+34]);
    __m128 a83_4 = _mm_load_ss(&A[333]);
    c83_4 = _mm_add_ss(c83_4, _mm_mul_ss(a83_4, b83));
    _mm_store_ss(&C[(l_n*84)+34], c83_4);
    __m128 c83_5 = _mm_load_ss(&C[(l_n*84)+55]);
    __m128 a83_5 = _mm_load_ss(&A[334]);
    c83_5 = _mm_add_ss(c83_5, _mm_mul_ss(a83_5, b83));
    _mm_store_ss(&C[(l_n*84)+55], c83_5);
    __m128 c83_6 = _mm_load_ss(&C[(l_n*84)+83]);
    __m128 a83_6 = _mm_load_ss(&A[335]);
    c83_6 = _mm_add_ss(c83_6, _mm_mul_ss(a83_6, b83));
    _mm_store_ss(&C[(l_n*84)+83], c83_6);
#else
    C[(l_n*84)+0] += A[329] * B[(l_n*84)+83];
    C[(l_n*84)+3] += A[330] * B[(l_n*84)+83];
    C[(l_n*84)+9] += A[331] * B[(l_n*84)+83];
    C[(l_n*84)+19] += A[332] * B[(l_n*84)+83];
    C[(l_n*84)+34] += A[333] * B[(l_n*84)+83];
    C[(l_n*84)+55] += A[334] * B[(l_n*84)+83];
    C[(l_n*84)+83] += A[335] * B[(l_n*84)+83];
#endif

  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 6048;
#endif
}

void ssparse_starMatrix_m120_n9_k9_ldA120_ldBna8_ldC120_beta1_pfsigonly(const float* A, const float* B, float* C, const float* A_prefetch, const float* B_prefetch, const float* C_prefetch) {
  unsigned int l_m = 0;

  #pragma simd vectorlength(8)
  #pragma vector aligned
  for ( l_m = 0; l_m < 120; l_m++) {
    C[0+l_m] += A[720+l_m] * B[0];
    C[0+l_m] += A[840+l_m] * B[1];
    C[0+l_m] += A[960+l_m] * B[2];
    C[120+l_m] += A[720+l_m] * B[3];
    C[120+l_m] += A[840+l_m] * B[4];
    C[120+l_m] += A[960+l_m] * B[5];
    C[240+l_m] += A[720+l_m] * B[6];
    C[240+l_m] += A[840+l_m] * B[7];
    C[240+l_m] += A[960+l_m] * B[8];
    C[360+l_m] += A[720+l_m] * B[9];
    C[360+l_m] += A[840+l_m] * B[10];
    C[480+l_m] += A[840+l_m] * B[11];
    C[480+l_m] += A[960+l_m] * B[12];
    C[600+l_m] += A[720+l_m] * B[13];
    C[600+l_m] += A[960+l_m] * B[14];
    C[720+l_m] += A[0+l_m] * B[15];
    C[720+l_m] += A[360+l_m] * B[16];
    C[720+l_m] += A[600+l_m] * B[17];
    C[840+l_m] += A[120+l_m] * B[18];
    C[840+l_m] += A[360+l_m] * B[19];
    C[840+l_m] += A[480+l_m] * B[20];
    C[960+l_m] += A[240+l_m] * B[21];
    C[960+l_m] += A[480+l_m] * B[22];
    C[960+l_m] += A[600+l_m] * B[23];
  }

#ifndef NDEBUG
#ifdef _OPENMP
#pragma omp atomic
#endif
libxsmm_num_total_flops += 5760;
#endif
}

#endif
